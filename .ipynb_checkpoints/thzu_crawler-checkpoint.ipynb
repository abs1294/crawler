{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as rq\n",
    "import re\n",
    "import datetime\n",
    "import pymysql\n",
    "from flask import Flask\n",
    "from flask import jsonify, request\n",
    "from fake_useragent import UserAgent\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from urllib.parse import urlencode\n",
    "from flask import Flask,request\n",
    "import threading\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import traceback\n",
    "import requests.packages.urllib3\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_settings = {\n",
    "    \"host\": \"127.0.0.1\",\n",
    "    \"port\": 3306,\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"As123459362\",\n",
    "    \"db\": \"pttcrawler\",\n",
    "    \"charset\": \"utf8\",\n",
    "    \"autocommit\":True\n",
    "}\n",
    "headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "    \"Accept-Language\": \"zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6\", \n",
    "    \"Host\": \"29th.cc\",  #目標網站 \n",
    "    \"Sec-Fetch-Dest\": \"document\", \n",
    "    \"Sec-Fetch-Mode\": \"navigate\", \n",
    "    \"Sec-Fetch-Site\": \"same-origin\", \n",
    "    \"Upgrade-Insecure-Requests\": \"?1\", \n",
    "     #使用者代理\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從免費代理網站取得代理ip\n",
    "def get_proxy_ip(isHttps):\n",
    "    r = rq.get(\"https://www.us-proxy.org/\",verify=False)\n",
    "    html_doc = r.text\n",
    "    soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "    trs = soup.select(\"table.table.table-striped.table-bordered tr\")\n",
    "    metas=[]\n",
    "    for tr in trs:\n",
    "        tds = tr.select(\"td\")\n",
    "        if len(tds) > 6:\n",
    "            ifScheme = tds[6].text\n",
    "            ip = tds[0].text\n",
    "            port = tds[1].text\n",
    "            anonymity = tds[4].text\n",
    "            proxy = \"%s:%s\"%(ip, port)\n",
    "            meta = {\n",
    "                'proxyIp': proxy,\n",
    "                'connect_times':0,\n",
    "                'successful_connect_times':0,\n",
    "                'qual_ratio':0,\n",
    "                'isDelete':False,\n",
    "              }\n",
    "            if ifScheme == isHttps:\n",
    "                if ifScheme == \"yes\":\n",
    "                    meta['ishttps'] = True\n",
    "                    meta['proxyIp'] = \"https://\" + meta['proxyIp']\n",
    "                elif ifScheme == \"no\":\n",
    "                    meta['ishttps'] = False\n",
    "                    meta['proxyIp'] = \"http://\" + meta['proxyIp']\n",
    "                metas.append(meta)\n",
    "                continue\n",
    "    return metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 proxy 可用性\n",
    "def test_proxy(proxy,timeout_sec,headers={}):\n",
    "    url = \"http://icanhazip.com/\"\n",
    "    print(proxy['proxyIp'])\n",
    "    try:\n",
    "        if proxy['ishttps'] == True:\n",
    "            r = rq.get(url, headers = headers,proxies={'http':proxy['proxyIp'],},verify=False, timeout=timeout_sec)\n",
    "        elif proxy['ishttps'] == False:\n",
    "            r = rq.get(url, headers = headers,proxies={'http':proxy['proxyIp'],},verify=False, timeout=timeout_sec)\n",
    "        print(r.status_code)\n",
    "        print(r.text.strip())\n",
    "        if r.status_code==200:\n",
    "            proxy['isValidate']=True\n",
    "    except Exception as ex:\n",
    "        proxy['isValidate']=False\n",
    "        print(ex)\n",
    "    print(\"---------------\")\n",
    "    return proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將取得的代理ip寫入資料庫，此函數也可以更新已寫入資料庫的資料\n",
    "def send_ip(proxies_pool_https, db_settings,typee=\"\",send_log = True):\n",
    "    time_start = datetime.datetime.now() #開始計時\n",
    "    data_combine=[]\n",
    "    wrong = 0\n",
    "    try:\n",
    "        # 建立Connection物件\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        # 建立Cursor物件\n",
    "        with conn.cursor() as cursor:\n",
    "          #資料表相關操作\n",
    "            get_max_log_id=\"select MAX(UID) FROM log\"\n",
    "            # % 操作符只能直接用於字串(‘123’)，列表([1,2,3])、元組\n",
    "            command = \"INSERT INTO proxy_ip(log_UID, proxyIp, connect_times, successful_connect_times, qual_ratio, isValidate, ishttps,isDelete, fromm)VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s) on DUPLICATE KEY UPDATE log_UID = values(log_UID),connect_times =values(connect_times),successful_connect_times=values(successful_connect_times),qual_ratio=values(qual_ratio),isValidate = values(isValidate),ishttps=values(ishttps),isDelete=values(isDelete)\"\n",
    "            command_log = \"INSERT INTO log(datetime, task, status, record_des, errmsg) VALUES(%s, %s, %s, %s, %s)\"\n",
    "            # 紀錄開始\n",
    "            if send_log:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"proxy_ip\", \"start\", typee, \"\"))\n",
    "            # 取得 log 的 UID\n",
    "            cursor.execute(get_max_log_id)\n",
    "            log_UID = str(cursor.fetchone()[0])\n",
    "            # 組合數據\n",
    "            if type(proxies_pool_https) == list:\n",
    "                for proxy in proxies_pool_https:\n",
    "                    data_combine.append((log_UID, proxy[\"proxyIp\"], proxy[\"connect_times\"], proxy[\"successful_connect_times\"], proxy[\"qual_ratio\"], proxy[\"isValidate\"], proxy[\"ishttps\"], proxy[\"isDelete\"], proxy[\"fromm\"]))  # 注意要用两个括号扩起来\n",
    "            else:\n",
    "                data_combine.append((log_UID, proxies_pool_https[\"proxyIp\"], proxies_pool_https[\"connect_times\"], proxies_pool_https[\"successful_connect_times\"], proxies_pool_https[\"qual_ratio\"], proxies_pool_https[\"isValidate\"], proxies_pool_https[\"ishttps\"], proxies_pool_https[\"isDelete\"], proxies_pool_https[\"fromm\"]))  # 注意要用两个括号扩起来\n",
    "            # 執行\n",
    "            try:\n",
    "                cursor.executemany(command, data_combine)\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "                wrong = wrong + 1\n",
    "                print(wrong)\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"proxy_ip\", \"wrong\" + str(wrong), \"\", str(err)))\n",
    "            # 紀錄結束\n",
    "            if wrong==0 and send_log:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"proxy_ip\", \"finish\", \"well_completed\", \"\"))\n",
    "            elif wrong>0 and send_log:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"proxy_ip\", \"finish\", str(wrong) + \" records are wrong\", \"\"))\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "    time_end = datetime.datetime.now()    #結束計時\n",
    "    time_c= time_end - time_start   #執行所花時間\n",
    "    print('time cost', time_c, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從資料庫取出ip\n",
    "def get_ip_from_db(isHttps,db_settings,qual_ratio=0,connect_times=4):\n",
    "    try:\n",
    "        # 建立Connection物件\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        # 建立Cursor物件\n",
    "        with conn.cursor() as cursor:\n",
    "          #資料表相關操作\n",
    "            if isHttps==\"yes\":\n",
    "                get_ip=f\"select * FROM proxy_ip where isHttps = 1 and isDelete is False and ( qual_ratio > {qual_ratio} or ( connect_times < {connect_times} ))\"\n",
    "            elif isHttps==\"no\":\n",
    "                get_ip=f\"select * FROM proxy_ip where isHttps = 0 and isDelete is False and ( qual_ratio > {qual_ratio} or ( connect_times < {connect_times} ))\"\n",
    "            cursor.execute(get_ip)\n",
    "            xx = cursor.fetchall()\n",
    "            des = cursor.description\n",
    "            data=[]\n",
    "            for i in xx:\n",
    "                d = {}\n",
    "                x=0\n",
    "                for j in des:\n",
    "                    d[j[0]]=i[x]\n",
    "                    x=x+1\n",
    "                data.append(d)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我要翻頁!!!!!!   by 更改url\n",
    "def thzucrawler(domain,forum_number,typee,pages=1, need_detail = False):\n",
    "    time_start = time.time()\n",
    "    now = datetime.date.today()\n",
    "    articles = []\n",
    "    s = rq.session()\n",
    "    proxies_this={}\n",
    "    proxies=get_proxy_ip(\"no\")\n",
    "    err = 0\n",
    "    headers = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "        \"Accept-Language\": \"zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6\", \n",
    "        \"Host\": \"web.pcc.gov.tw\",  #目標網站 \n",
    "        \"Sec-Fetch-Dest\": \"document\", \n",
    "        \"Sec-Fetch-Mode\": \"navigate\", \n",
    "        \"Sec-Fetch-Site\": \"same-origin\", \n",
    "        \"Upgrade-Insecure-Requests\": \"?1\", \n",
    "        \"User-Agent\":UserAgent().random\n",
    "         #使用者代理\n",
    "    } \n",
    "    \n",
    "    for i in range(1,10000):\n",
    "        print(i-err)\n",
    "        url = domain + 'forum-'+ str(forum_number)+'-'+str(i-err)+'.html'  \n",
    "        try:\n",
    "            response = rq.get(url,cookies={'over18': '1'},proxies = {'http':proxies_this},timeout=3)\n",
    "            html_doc = response.text\n",
    "            soup = BeautifulSoup(html_doc, 'lxml')\n",
    "            if i == 1:\n",
    "                pages_actual_text=soup.select('div.pg a')[-2].text\n",
    "                #總頁數\n",
    "                pages_actual= int(re.search('\\d.*',pages_actual_text).group(0))\n",
    "            page = soup.select(\"div#pgt div strong\")[0].text\n",
    "            for s in soup.select(\"div.mn div#threadlist div.bm_c form#moderate table tbody\"):\n",
    "                if (response.status_code == 200) and (re.search('normalthread.*',s['id'])):\n",
    "                    #requests.get() 的結果是 request.Response 物件, 我們可以先透過該物件的 statu_code 屬性取得 server 回覆的狀態碼\n",
    "                    #(例如 200 表示正常, 404 表示找不到網頁等), 若狀態碼為 200, 代表正常回應\n",
    "                    board=s.select(\"th em a\")[0].text\n",
    "                    name=s.select(\"th a.s.xst\")[0].text\n",
    "                    url_articles= domain + s.select(\"th a.s.xst\")[0]['href']\n",
    "                    reply = s.select(\"a.xi2\")[0].text\n",
    "                    look = s.select(\"td.num em\")[0].text\n",
    "                    nos_of_d=-1\n",
    "                    date=s.select(\"td span\")[0].text\n",
    "                    if re.search('前',date) or re.search('天',date):\n",
    "                        if re.search('小时',date) or re.search('秒',date) or re.search('分钟',date):\n",
    "                            date=now\n",
    "                        elif re.search('(.*)天',date).group(1)=='昨':\n",
    "                            date=now+datetime.timedelta(-1)\n",
    "                        elif re.search('(.*)天',date).group(1)=='前':\n",
    "                            date=now+datetime.timedelta(-2)\n",
    "                        else:\n",
    "                            delta=re.search('(.*)天',date).group(1)\n",
    "                            date=now+datetime.timedelta(int(delta))\n",
    "                    if need_detail:\n",
    "                        responsee = rq.get(url_articles,cookies={'over18': '1'})\n",
    "                        html_docc = responsee.text\n",
    "                        soupp = BeautifulSoup(html_docc, 'lxml')\n",
    "                        try:\n",
    "                            nos_of_d_text = soupp.select('dd p:nth-child(3)')[0].text\n",
    "                            nos_of_d = re.search(r'下载次数:(.*)',nos_of_d_text)\n",
    "                            nos_of_d = int(nos_of_d.group(1).strip())\n",
    "                        except Exception as exx:\n",
    "                            print(\"沒有下載次數\")\n",
    "                            nos_of_d = -1\n",
    "                            pass\n",
    "                    articles.append({'page': page,'typee':typee,'board': board,'title': name,'date': str(date),'url': url_articles,'nos_of_d':nos_of_d,'reply':reply,'look':look})\n",
    "            if i-err == pages or i-err == pages_actual:\n",
    "                break\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "            detail = ex.args[0] #取得詳細內容\n",
    "            cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "            lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "            fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "            lineNum = lastCallStack[1] #取得發生的行號\n",
    "            funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "            errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "            print(errMsg)\n",
    "            print(\"換一下 proxy\")\n",
    "            err = err + 1\n",
    "            proxies_this = proxies[err]['proxyIp']\n",
    "            if err == len(proxies):\n",
    "                err = 0\n",
    "                proxies=get_proxy_ip(\"no\")\n",
    "            continue\n",
    "    print(\"done\")\n",
    "    time_end = time.time()    #結束計時\n",
    "    time_c= time_end - time_start   #執行所花時間\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_db(db_settings, raw_datas):\n",
    "# import charts\n",
    "# 資料庫參數設定\n",
    "\n",
    "    time_start = time.time() #開始計時\n",
    "\n",
    "    try:\n",
    "        # 建立Connection物件\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        # 建立Cursor物件\n",
    "        with conn.cursor() as cursor:\n",
    "          #資料表相關操作\n",
    "            get_max_log_id=\"select MAX(UID) FROM log\"\n",
    "            # % 操作符只能直接用於字串(‘123’)，列表([1,2,3])、元組\n",
    "            command = \"INSERT INTO thzu_data(log_UID, page, title, board, date, url, nos_of_d, type, look ,reply)VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)  on DUPLICATE KEY UPDATE log_UID = values(log_UID), page = values(page), title = values(title), board = values(board), date = values(date), url = values(url), nos_of_d = values(nos_of_d), type = values(type), look = values(look), reply = values(reply)\"\n",
    "            command_log = \"INSERT INTO log(datetime, task, status, record_des, errmsg) VALUES(%s, %s, %s, %s, %s)\"\n",
    "            # 紀錄開始\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"thzucrawler\", \"start\", \"\", \"\"))\n",
    "            # 取得 log 的 UID\n",
    "            cursor.execute(get_max_log_id)\n",
    "            log_UID = str(cursor.fetchone()[0])\n",
    "            # 執行\n",
    "            for raw_data in raw_datas:\n",
    "                try:\n",
    "                    cursor.execute(command, (int(log_UID),int(raw_data[\"page\"]), raw_data[\"title\"], raw_data[\"board\"], raw_data[\"date\"], raw_data[\"url\"], raw_data[\"nos_of_d\"], raw_data[\"typee\"], int(raw_data[\"look\"]), int(raw_data[\"reply\"])))\n",
    "                except Exception as ex:\n",
    "                    error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "                    detail = ex.args[0] #取得詳細內容\n",
    "                    cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "                    lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "                    fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "                    lineNum = lastCallStack[1] #取得發生的行號\n",
    "                    funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "                    errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "                    print(errMsg)\n",
    "                    cursor.execute(command_log, (datetime.datetime.now(), \"thzucrawler\", \"wrong\", str(raw_data), str(ex) + \" + \" + errMsg))\n",
    "                    continue\n",
    "            # 紀錄結束\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"thzucrawler\", \"finish\", \"\", \"\"))\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "        detail = ex.args[0] #取得詳細內容\n",
    "        cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "        lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "        fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "        lineNum = lastCallStack[1] #取得發生的行號\n",
    "        funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "        errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "        print(errMsg)\n",
    "\n",
    "    time_end = time.time()    #結束計時\n",
    "    time_c= time_end - time_start   #執行所花時間\n",
    "    print('寫入資料庫，time cost', time_c, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m forum_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m220\u001b[39m\n\u001b[0;32m      4\u001b[0m pages \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m----> 5\u001b[0m raw_datas\u001b[38;5;241m=\u001b[39m\u001b[43mthzucrawler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m39\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m日韩情色(BT)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mpages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m send_db(db_settings, raw_datas)\n\u001b[0;32m      7\u001b[0m raw_datas\u001b[38;5;241m=\u001b[39mthzucrawler(domain,\u001b[38;5;241m181\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m亚洲無碼原創\u001b[39m\u001b[38;5;124m\"\u001b[39m,pages)\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mthzucrawler\u001b[1;34m(domain, forum_number, typee, pages, need_detail)\u001b[0m\n\u001b[0;32m      6\u001b[0m s \u001b[38;5;241m=\u001b[39m rq\u001b[38;5;241m.\u001b[39msession()\n\u001b[0;32m      7\u001b[0m proxies_this\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m----> 8\u001b[0m proxies\u001b[38;5;241m=\u001b[39m\u001b[43mget_proxy_ip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     10\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip, deflate, br\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m      \u001b[38;5;66;03m#使用者代理\u001b[39;00m\n\u001b[0;32m     21\u001b[0m } \n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mget_proxy_ip\u001b[1;34m(isHttps)\u001b[0m\n\u001b[0;32m      3\u001b[0m r \u001b[38;5;241m=\u001b[39m rq\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.us-proxy.org/\u001b[39m\u001b[38;5;124m\"\u001b[39m,verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m html_doc \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m----> 5\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml_doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m trs \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable.table.table-striped.table-bordered tr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m metas\u001b[38;5;241m=\u001b[39m[]\n",
      "File \u001b[1;32mD:\\Program Files\\anaconda3\\envs\\govpur\\lib\\site-packages\\bs4\\__init__.py:248\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    251\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features))\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "# http://99thz.cc/forum-220-5.html\n",
    "domain = 'http://99thz.cc/'\n",
    "forum_number = 220\n",
    "pages = 2\n",
    "raw_datas=thzucrawler(domain,39,\"日韩情色(BT)\",pages)\n",
    "send_db(db_settings, raw_datas)\n",
    "raw_datas=thzucrawler(domain,181,\"亚洲無碼原創\",pages)\n",
    "send_db(db_settings, raw_datas)\n",
    "#以下兩個版不常更新\n",
    "# raw_datas=thzucrawler(domain,220,\"亚洲有碼原創\",pages)\n",
    "# send_db(db_settings, raw_datas)\n",
    "# raw_datas=thzucrawler(domain,182,\"欧美無碼\",pages)\n",
    "# send_db(db_settings, raw_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_date = [('2021-07-15','2021-08-01'),('2021-08-15','2021-09-01'),('2021-07-01','2021-07-15'),('2021-08-01','2021-08-15'),('2021-09-01','2021-10-01'),('2021-10-01','2021-11-01'),('2021-11-01','2021-12-01'),('2022-02-01','2022-03-01'),('2022-01-01','2022-02-01')]\n",
    "print(len(total_date))\n",
    "def thzu_detail(date,db_settings):\n",
    "    time_start = datetime.datetime.now() #開始計時\n",
    "    conn = pymysql.connect(**db_settings)\n",
    "    # 建立Cursor物件\n",
    "    with conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "      #資料表相關操作\n",
    "        get_max_log_id=f\"SELECT * FROM pttcrawler.thzu_data where (nos_of_d = 0 or nos_of_d = -1) and (date >= '{date[0]}' and  date <= '{date[1]}')\"\n",
    "        print(get_max_log_id)\n",
    "        cursor.execute(get_max_log_id)\n",
    "        raw_datas = cursor.fetchall()            \n",
    "    print(f'一共抓出 {len(raw_datas)} 筆還沒有內頁資料')\n",
    "    proxies_this={}\n",
    "    proxies_all = get_ip_from_db(\"no\",db_settings,qual_ratio=0,connect_times=4)\n",
    "#     try:\n",
    "#         for i in raw_datas:\n",
    "#             print(\"-------------\")\n",
    "#             with rq.session() as s:\n",
    "#                 try:\n",
    "#                     r = s.get(i['url'],headers=headers)\n",
    "#                     html_doc = r.text\n",
    "#                     soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "#                     trs = soup.select(\"table tr em.xg1\")\n",
    "#                     if trs !=[]:\n",
    "#                         i['nos_of_d']=int(re.search(r\"下载次数: (.*)\\)\",trs[0].text).group(1))\n",
    "#                         continue\n",
    "#                     trs = soup.select(\"dd p\")\n",
    "#                     if trs !=[]:\n",
    "#                         i['nos_of_d']=int(re.search(r\"下载次数: (.*)\",trs[3].text).group(1))\n",
    "#                         continue\n",
    "#                 except requests.exceptions.RequestException as ex:\n",
    "#                     print(ex)\n",
    "#                     proxies_this={'http':proxies_all[0]['proxyIp']}\n",
    "#                     proxies_all.pop(0)\n",
    "#                 except Exception as ex:\n",
    "#                     print(ex)\n",
    "#                     print(i['url'])\n",
    "#     except:\n",
    "#         pass\n",
    "#     send_db(db_settings, raw_datas)\n",
    "#     time_end = datetime.datetime.now() #開始計時\n",
    "#     time_c= time_end - time_start   #執行所花時間\n",
    "#     print(f'{date}，花費 {time_c} 秒')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = rq.get(\"http://29th.cc/thread-2452804-1-1.html\",headers=headers)\n",
    "# html_doc = r.text\n",
    "# soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "# trs = soup.select(\"dd p\")\n",
    "# int(re.search(r\"下载次数: (.*)\",trs[3].text).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = []\n",
    "t1 = threading.Thread(target=thzu_detail, args=(total_date[0],db_settings))\n",
    "t_list.append(t1)\n",
    "\n",
    "t2 = threading.Thread(target=thzu_detail, args=(total_date[1],db_settings))\n",
    "t_list.append(t2)\n",
    "                      \n",
    "t3 = threading.Thread(target=thzu_detail, args=(total_date[2],db_settings))\n",
    "t_list.append(t3)\n",
    "                      \n",
    "t4 = threading.Thread(target=thzu_detail, args=(total_date[3],db_settings))\n",
    "t_list.append(t4)\n",
    "                      \n",
    "t5 = threading.Thread(target=thzu_detail, args=(total_date[4],db_settings))\n",
    "t_list.append(t5)\n",
    "                      \n",
    "t6 = threading.Thread(target=thzu_detail, args=(total_date[5],db_settings))\n",
    "t_list.append(t6)\n",
    "                      \n",
    "t7 = threading.Thread(target=thzu_detail, args=(total_date[6],db_settings))\n",
    "t_list.append(t7)\n",
    "                      \n",
    "t8 = threading.Thread(target=thzu_detail, args=(total_date[7],db_settings))\n",
    "t_list.append(t8)\n",
    "                      \n",
    "t9 = threading.Thread(target=thzu_detail, args=(total_date[8],db_settings))\n",
    "t_list.append(t9)\n",
    "\n",
    "# 開始工作\n",
    "for t in t_list:\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GOVPUR",
   "language": "python",
   "name": "govpur"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

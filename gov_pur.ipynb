{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a0fcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\cacert.pem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\\n%s\" %\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as rq\n",
    "import re\n",
    "import datetime\n",
    "import pymysql\n",
    "from flask import Flask,jsonify, request\n",
    "# from fake_useragent import UserAgent\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from urllib.parse import urlencode\n",
    "import threading\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import traceback\n",
    "import requests.packages.urllib3\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import base64\n",
    "os.environ['REQUESTS_CA_BUNDLE'] =  os.path.join(os.path.dirname(sys.argv[0]), 'cacert.pem')\n",
    "print(os.path.join(os.path.dirname(sys.argv[0]), 'cacert.pem'))\n",
    "import telnetlib\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "583f9a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "參數成功接收\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # 建立 ConfigParser\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('gov_pur_settings.ini')\n",
    "    config_db_settings = config['db_settings']    \n",
    "    db_settings = {\n",
    "        \"host\": config_db_settings['host'],\n",
    "        \"port\": config_db_settings.getint('port'),\n",
    "        \"user\": config_db_settings['user'],\n",
    "        \"password\": config_db_settings['password'],\n",
    "        \"db\": config_db_settings['db'],\n",
    "        \"charset\": config_db_settings['charset'],\n",
    "        \"autocommit\":config_db_settings.getboolean('autocommit')\n",
    "    }\n",
    "    \n",
    "    config_ip_time_use_interval = config['other_Settings'].getfloat('ip_time_use_interval')\n",
    "    # 限制取得內頁資料的時間 (若成功抓取則等待)\n",
    "    config_diff_seconds_data = config['other_Settings'].getfloat('diff_seconds_data')\n",
    "    # 限制每一次嘗試取得內頁資料的時間 (每次嘗試連接都等待)\n",
    "    config_diff_seconds_conn = config['other_Settings'].getfloat('diff_seconds_conn')\n",
    "    # 設定是否用本機ip爬取內頁資料 (TRUE = 用本機ip)\n",
    "    config_No_proxy = config['other_Settings'].getboolean('No_proxy')\n",
    "    # 爬取幾筆就存進資料庫？\n",
    "    config_block_crawl_no = config['other_Settings'].getint('block_crawl_no')\n",
    "    # 請輸入主機ip ( 若要從本機啟動，可輸入127.0.0.1 )\n",
    "    config_my_host = config['other_Settings']['my_host']\n",
    "    print('參數成功接收')\n",
    "except:\n",
    "    print('參數填寫錯誤')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47e4974",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "    \"Accept-Language\": \"zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6\", \n",
    "    \"Host\": \"web.pcc.gov.tw\",  #目標網站 \n",
    "    \"Connection\":\"close\",\n",
    "    \"Sec-Fetch-Dest\": \"document\", \n",
    "    \"Sec-Fetch-Mode\": \"navigate\", \n",
    "    \"Sec-Fetch-Site\": \"same-origin\", \n",
    "    \"Upgrade-Insecure-Requests\": \"?1\", \n",
    "    \"User-Agent\":'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36'\n",
    "     #使用者代理\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27a82e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lxml pyinstaller編不動，nuitka不行\n",
    "#parser = \"lxml\"\n",
    "parser = \"html.parser\"\n",
    "parser_detail = 'html5lib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a54a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#創建一個資料夾存放驗證用撲克牌圖片\n",
    "path = './temp_validate_img'\n",
    "if os.path.exists(path):\n",
    "    shutil.rmtree (path)\n",
    "    os.mkdir(path)\n",
    "else:\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f12c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://web.pcc.gov.tw/tps/tpam/main/tps/tpam/tpam_tender_detail.do?searchMode=common&scope=F&primaryKey=53759591'\n",
    "# urll='https://icanhazip.com/'\n",
    "# s = rq.session()\n",
    "# r = s.get(urll, headers = headers,verify=False,proxies={'https':'http://170.155.5.235:8080'}, timeout=(4,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be8c765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_ip(db_settings):\n",
    "    try:\n",
    "        # 建立Connection物件\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        # 建立Cursor物件\n",
    "        with conn.cursor() as cursor:\n",
    "          #資料表相關操作\n",
    "            # % 操作符只能直接用於字串(‘123’)，列表([1,2,3])、元組\n",
    "            command = \"DELETE FROM proxy_ip WHERE TIMESTAMPDIFF(HOUR, updated_time, now()) >= 23;\"\n",
    "            # 執行\n",
    "            try:\n",
    "                cursor.execute(command)\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"proxy_ip\", \"delete_wrong\", \"\", str(err)))\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a871f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ip(ip,port):\n",
    "    try:\n",
    "        telnetlib.Telnet(ip, port, timeout=1)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0f8fcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從免費代理網站取得代理ip\n",
    "def get_proxy_ip(isHttps,db_settings):\n",
    "    delete_ip(db_settings)\n",
    "    Now = datetime.datetime.now()\n",
    "    url = {\"usproxy\":\"https://www.us-proxy.org/\",\"freeproxylist_sslproxy\":\"https://www.sslproxies.org/\",\"freeproxylist_socksproxy\":\"https://www.socks-proxy.net/\"}\n",
    "    metas=[]\n",
    "    for i in url:\n",
    "        print(i)\n",
    "        r = rq.get(url[i],verify=False, timeout=(4,15))\n",
    "        html_doc = r.text\n",
    "        soup = BeautifulSoup(html_doc, parser)\n",
    "        trs = soup.select(\"table.table.table-striped.table-bordered tr\")  \n",
    "        #print(trs)\n",
    "        for tr in trs:\n",
    "            tds = tr.select(\"td\")\n",
    "            #print(tds)\n",
    "            #print(len(tds))\n",
    "            if len(tds) > 6:\n",
    "                ifScheme = tds[6].text.lower()\n",
    "                ip = tds[0].text\n",
    "                port = tds[1].text\n",
    "                # socks 代理會用到 version，sslproxy會抓錯，會抓成 anonymity\n",
    "                version = tds[4].text.lower()\n",
    "                proxy = \"%s:%s\"%(ip, port)\n",
    "                meta = {\n",
    "                    'proxyIp': proxy,\n",
    "                    'connect_times':1,\n",
    "                    'successful_connect_times':0,\n",
    "                    'qual_ratio':0,\n",
    "                    'isDelete':False,\n",
    "                    'fromm':i,\n",
    "                    'mine' : None,\n",
    "                    'isValidate':test_ip(ip, port),\n",
    "                    'updated_time':Now\n",
    "                  }\n",
    "                if ifScheme == isHttps:\n",
    "                    if \"sock\" in version:\n",
    "                        meta['ishttps'] = True\n",
    "                        meta['proxyIp'] = version + \"://\" + meta['proxyIp']\n",
    "                    elif ifScheme == \"yes\":\n",
    "                        meta['ishttps'] = True\n",
    "                        # https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#https-proxy-error-http-proxy\n",
    "                        meta['proxyIp'] = \"http://\" + meta['proxyIp']\n",
    "                    elif ifScheme == \"no\":\n",
    "                        meta['ishttps'] = False\n",
    "                        meta['proxyIp'] = \"http://\" + meta['proxyIp']\n",
    "                    \n",
    "                    #meta = test_proxy(meta,timeout_sec=3)\n",
    "                    metas.append(meta)\n",
    "                    continue                    \n",
    "    #------------------------------------\n",
    "    headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "    \"Accept-Language\": \"zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6\", \n",
    "    \"Host\": \"free-proxy.cz\",  #目標網站 \n",
    "    \"Sec-Fetch-Dest\": \"document\", \n",
    "    \"Sec-Fetch-Mode\": \"navigate\", \n",
    "    \"Sec-Fetch-Site\": \"same-origin\", \n",
    "    \"Upgrade-Insecure-Requests\": \"?1\", \n",
    "    \"User-Agent\":'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36'\n",
    "     #使用者代理\n",
    "}\n",
    "    \n",
    "    if isHttps ==\"yes\":\n",
    "        pp = 'https'\n",
    "    elif isHttps ==\"no\":\n",
    "        pp = 'http'\n",
    "    with rq.session() as s:\n",
    "        s.keep_alive = True\n",
    "        for j in range(1,4):\n",
    "            try:\n",
    "                r = s.get(f\"http://free-proxy.cz/zh/\",headers=headers,timeout=10)\n",
    "\n",
    "                headers['Content-Type']='application/x-www-form-urlencoded'\n",
    "                data=f\"country=all&protocol={pp}&anonymity=all&send=Filter+proxies\"\n",
    "                r = s.post(\"http://free-proxy.cz/en/?do=searchFilter-submit\",data=data,headers=headers,timeout=10)\n",
    "                #http://free-proxy.cz/en/proxylist/country/all/https/ping/all/2\n",
    "                if j >1:\n",
    "                    headers['Referer']=f'http://free-proxy.cz/en/proxylist/country/all/{pp}/ping/all'\n",
    "                    headers['Connection']=f'keep-alive'\n",
    "                    r = s.get(f\"http://free-proxy.cz/zh/proxylist/country/all/{pp}/ping/all/{j}\",headers=headers,timeout=10,verify=False)\n",
    "                print(\"free-proxy.cz抓取頁數： \"+str(j))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"free-proxy.cz 無法進入\")\n",
    "                break\n",
    "            print(r.url)\n",
    "            html_doc = r.text\n",
    "            soup = BeautifulSoup(html_doc, parser)\n",
    "            proxylist = soup.select('table#proxy_list tr')[1:]\n",
    "            print(len(proxylist))\n",
    "            for i in proxylist:\n",
    "                try:\n",
    "                    try:\n",
    "                        ip = base64.b64decode(re.search(r'Base64\\.decode\\(\\\"(.*)\\\"\\)',str(i.select('td')[0])).group(1)).decode(\"utf-8\")\n",
    "                    except AttributeError :\n",
    "                        continue\n",
    "                    port = i.select('td')[1].text\n",
    "                    Protocol  = i.select('td')[2].text.lower()\n",
    "                    place=i.select('td')[2].text.lower()\n",
    "                    meta = {\n",
    "                        'proxyIp': 'http://'+ str(ip) +':'+ str(port),\n",
    "                        'connect_times':1,\n",
    "                        'successful_connect_times':0,\n",
    "                        'qual_ratio':0,\n",
    "                        'isDelete':False,\n",
    "                        'fromm':'freeProxyCZ',\n",
    "                        'mine' : None,\n",
    "                        'isValidate':test_ip(ip, port),\n",
    "                        'updated_time':Now\n",
    "                      }\n",
    "                    if Protocol =='https' and isHttps ==\"yes\":\n",
    "                        meta['ishttps'] = True\n",
    "                        # 測了就抓不到下一頁了\n",
    "                        #meta = test_proxy(meta,timeout_sec=3)\n",
    "                        metas.append(meta)\n",
    "                    elif Protocol =='http' and isHttps ==\"no\":\n",
    "                        meta['ishttps'] = False\n",
    "                        #meta = test_proxy(meta,timeout_sec=3)\n",
    "                        metas.append(meta)\n",
    "                except:\n",
    "                    continue\n",
    "                if len(proxylist)<37:\n",
    "                    break\n",
    "    return metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48a66ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 proxy 可用性\n",
    "def test_proxy(proxy,timeout_sec,headers={}):\n",
    "    #print(proxy['proxyIp'])\n",
    "    with rq.session() as s:\n",
    "        try:\n",
    "            if proxy['ishttps'] == True:\n",
    "                url = \"https://icanhazip.com/\"\n",
    "                r = s.get(url, headers = headers,proxies={'https':proxy['proxyIp'],},verify=False, timeout=timeout_sec)\n",
    "            elif proxy['ishttps'] == False:\n",
    "                url = \"http://icanhazip.com/\"\n",
    "                r = s.get(url, headers = headers,proxies={'http':proxy['proxyIp'],},verify=False, timeout=timeout_sec)\n",
    "            print(r.status_code)\n",
    "            print(proxy['proxyIp'])\n",
    "            if r.status_code==200:\n",
    "                proxy['isValidate']=True\n",
    "                proxy['successful_connect_times']=1\n",
    "                proxy['qual_ratio']=1\n",
    "            else:\n",
    "                proxy['isValidate']=False\n",
    "        except Exception as ex:\n",
    "            proxy['isValidate']=False\n",
    "            #print(ex)\n",
    "        #print(\"---------------\")\n",
    "    return proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0eb66b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將取得的代理ip寫入資料庫，此函數也可以更新已寫入資料庫的資料\n",
    "def send_ip(proxies_pool_https, db_settings,typee=\"\",send_log = True,new = False):\n",
    "    time_start = datetime.datetime.now() #開始計時\n",
    "    data_combine=[]\n",
    "    wrong = 0\n",
    "    try:\n",
    "        # 建立Connection物件\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        # 建立Cursor物件\n",
    "        with conn.cursor() as cursor:\n",
    "          #資料表相關操作\n",
    "            get_max_log_id=\"select MAX(UID) FROM log\"\n",
    "            # % 操作符只能直接用於字串(‘123’)，列表([1,2,3])、元組\n",
    "            command = \"INSERT INTO proxy_ip(log_UID, proxyIp, connect_times, successful_connect_times, qual_ratio, isValidate, ishttps,isDelete, fromm, updated_time)VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) on DUPLICATE KEY UPDATE log_UID = values(log_UID),connect_times =connect_times+values(connect_times),successful_connect_times=successful_connect_times+values(successful_connect_times),qual_ratio=((successful_connect_times+values(successful_connect_times))/(connect_times+values(connect_times))),isValidate = values(isValidate),ishttps=values(ishttps),isDelete=values(isDelete),updated_time=values(updated_time)\"\n",
    "            if new:\n",
    "                command = \"INSERT INTO proxy_ip(log_UID, proxyIp, connect_times, successful_connect_times, qual_ratio, isValidate, ishttps,isDelete, fromm, updated_time)VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) on DUPLICATE KEY UPDATE log_UID = values(log_UID),connect_times = values(connect_times),successful_connect_times=values(successful_connect_times),qual_ratio=values(qual_ratio),isValidate = values(isValidate),ishttps=values(ishttps),isDelete=values(isDelete),updated_time=values(updated_time)\"\n",
    "            command_log = \"INSERT INTO log(datetime, task, status, record_des, errmsg) VALUES(%s, %s, %s, %s, %s)\"\n",
    "            # 紀錄開始 (暫時取消)\n",
    "#             if send_log:\n",
    "#                 cursor.execute(command_log, (datetime.datetime.now(), \"proxy_ip\", \"start\", typee, \"\"))\n",
    "            # 取得 log 的 UID\n",
    "            cursor.execute(get_max_log_id)\n",
    "            log_UID = str(cursor.fetchone()[0])\n",
    "            # 組合數據\n",
    "            if type(proxies_pool_https) == list:\n",
    "                for proxy in proxies_pool_https:\n",
    "                    data_combine.append((log_UID, proxy[\"proxyIp\"], proxy[\"connect_times\"], proxy[\"successful_connect_times\"], proxy[\"qual_ratio\"], proxy[\"isValidate\"], proxy[\"ishttps\"], proxy[\"isDelete\"], proxy[\"fromm\"], proxy[\"updated_time\"]))  # 注意要用两个括号扩起来\n",
    "            else:\n",
    "                data_combine.append((log_UID, proxies_pool_https[\"proxyIp\"], proxies_pool_https[\"connect_times\"], proxies_pool_https[\"successful_connect_times\"], proxies_pool_https[\"qual_ratio\"], proxies_pool_https[\"isValidate\"], proxies_pool_https[\"ishttps\"], proxies_pool_https[\"isDelete\"], proxies_pool_https[\"fromm\"]))  # 注意要用两个括号扩起来\n",
    "            # 執行\n",
    "            try:\n",
    "                cursor.executemany(command, data_combine)\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "                wrong = wrong + 1\n",
    "                print(wrong)\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"proxy_ip\", \"wrong\" + str(wrong), \"\", str(err)))\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(data_combine)\n",
    "    time_end = datetime.datetime.now()    #結束計時\n",
    "    time_c= time_end - time_start   #執行所花時間\n",
    "    print('proxy ip寫入資料庫，time cost', time_c, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b69540c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_yes=get_proxy_ip(\"yes\",db_settings)\n",
    "# send_ip(p_yes, db_settings ,\"only https\",new = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72fd9a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從資料庫取出ip，未來有自架proxy的話，可以把proxy放到資料庫內，並設定 mine = 1，這樣就會固定抓這個proxy來用，如果沒有自架proxy的話，就還需要多寫 proxy expire 的處置方式。\n",
    "def get_ip_from_db(isHttps,db_settings,qual_ratio=0,connect_times=1,new=True,ip_time_use_interval=config_ip_time_use_interval):\n",
    "    try:\n",
    "        # 建立Connection物件\n",
    "        data=[]\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        # 建立Cursor物件\n",
    "        with conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "          #資料表相關操作\n",
    "            if isHttps==\"yes\":\n",
    "                get_ip=f\"select * FROM proxy_ip where isHttps = 1 and isDelete is False and ((qual_ratio > {qual_ratio} or connect_times <= {connect_times}) or (mine = 1 and TIMESTAMPDIFF(second, updated_time, now())>{ip_time_use_interval}))\"\n",
    "            elif isHttps==\"no\":\n",
    "                get_ip=f\"select * FROM proxy_ip where isHttps = 0 and isDelete is False and ((qual_ratio > {qual_ratio} or connect_times <= {connect_times}) or (mine = 1 and TIMESTAMPDIFF(second, updated_time, now())>{ip_time_use_interval}))\"\n",
    "            cursor.execute(get_ip)\n",
    "            datas = cursor.fetchall()\n",
    "        if new:\n",
    "            for data in datas:\n",
    "                data['successful_connect_times']=0\n",
    "                data['connect_times']=0\n",
    "                data['qual_ratio']=0\n",
    "        return datas\n",
    "    except Exception as ex:\n",
    "        print(f'ip抓取失敗：{ex}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2650ec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#更新 mine proxy_ip 的可取用時間\n",
    "def update_mine_proxy_ip(proxy_ip,db_settings):\n",
    "    conn = pymysql.connect(**db_settings)\n",
    "    with conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "        update_proxy_ip = f\"update proxy_ip SET updated_time=now() where proxyIp = '{proxy_ip['proxyIp']}'\"\n",
    "        cursor.execute(update_proxy_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8861663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#撲克牌圖片驗證用算法\n",
    "def pHash(img):\n",
    "    # 感知哈希算法\n",
    "    # 缩放32*32\n",
    "    img = cv2.resize(img, (32, 32))   # , interpolation=cv2.INTER_CUBIC\n",
    " \n",
    "    # 转换为灰度图\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # 将灰度图转为浮点型，再进行dct变换\n",
    "    dct = cv2.dct(np.float32(gray))\n",
    "    # opencv实现的掩码操作\n",
    "    dct_roi = dct[0:8, 0:8]\n",
    " \n",
    "    hash = []\n",
    "    avreage = np.mean(dct_roi)\n",
    "    for i in range(dct_roi.shape[0]):\n",
    "        for j in range(dct_roi.shape[1]):\n",
    "            if dct_roi[i, j] > avreage:\n",
    "                hash.append(1)\n",
    "            else:\n",
    "                hash.append(0)\n",
    "    return hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81990dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#撲克牌圖片驗證用算法 - 比較\n",
    "def cmpHash(hash1, hash2):\n",
    "    # Hash值对比\n",
    "    # 算法中1和0顺序组合起来的即是图片的指纹hash。顺序不固定，但是比较的时候必须是相同的顺序。\n",
    "    # 对比两幅图的指纹，计算汉明距离，即两个64位的hash值有多少是不一样的，不同的位数越小，图片越相似\n",
    "    # 汉明距离：一组二进制数据变成另一组数据所需要的步骤，可以衡量两图的差异，汉明距离越小，则相似度越高。汉明距离为0，即两张图片完全一样\n",
    "    n = 0\n",
    "    # hash长度不同则返回-1代表传参出错\n",
    "    if len(hash1) != len(hash2):\n",
    "        return -1\n",
    "    # 遍历判断\n",
    "    for i in range(len(hash1)):\n",
    "        # 不相等则n计数+1，n最终为相似度\n",
    "        if hash1[i] != hash2[i]:\n",
    "            n = n + 1\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d5c1cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#進行撲克牌驗證\n",
    "def validate_ip(s,url,proxies):\n",
    "    response=\"\"\n",
    "    x=0\n",
    "    while x <= 5:\n",
    "        response = s.get(url, proxies=proxies)\n",
    "        html_doc = response.text\n",
    "        soup = BeautifulSoup(html_doc, parser)\n",
    "        img_url_d = soup.select('form#validateForm img')\n",
    "        img_url_head = \"http://web.pcc.gov.tw/tps/tpam/\"\n",
    "        img_data = []\n",
    "        img_data.append({'img_src':img_url_head +img_url_d[0]['src'],'id':'question'})\n",
    "        for j in img_url_d[2:]:\n",
    "            d = {'img_src':img_url_head +j['src'],'id':j['alt']}\n",
    "            img_data.append(d)\n",
    "        right_hash_com =[]\n",
    "        left_hash_com =[]\n",
    "        \n",
    "        path = './temp_validate_img'\n",
    "        for k in img_data:\n",
    "            response = s.get(k['img_src'])\n",
    "            with open(path +'/'+ k['id']+'.jpg', 'wb') as file:\n",
    "                file.write(response.content)\n",
    "                file.close() \n",
    "            if k ==img_data[0]:\n",
    "                img = cv2.imread(path +'/'+\"question.jpg\")\n",
    "                crop_img = img[0:96, 6:77]\n",
    "                ret,crop_img = cv2.threshold(crop_img,127,255,cv2.THRESH_BINARY)\n",
    "                right_imageVar = pHash(crop_img)\n",
    "                crop_img = img[0:96, 89:160]\n",
    "                ret,crop_img = cv2.threshold(crop_img,127,255,cv2.THRESH_BINARY)\n",
    "                left_imageVar = pHash(crop_img)\n",
    "            image = cv2.imread(path +'/'+k['id']+'.jpg')\n",
    "            ret,image = cv2.threshold(image,127,255,cv2.THRESH_BINARY)\n",
    "            right = cmpHash(pHash(image),right_imageVar)\n",
    "            left = cmpHash(pHash(image),left_imageVar)\n",
    "            right_hash_com.append(right)\n",
    "            left_hash_com.append(left)\n",
    "\n",
    "        right_idx = np.argmin(right_hash_com[1:])\n",
    "        left_idx = np.argmin(left_hash_com[1:])\n",
    "        data=\"choose=\" +img_data[right_idx+1]['id']+\"&choose=\"+img_data[left_idx+1]['id']+\"&id=\"+re.match(r'.*id=(.*)', url, flags=0).group(1)\n",
    "        response = s.post(url,headers = {'Content-Type':'application/x-www-form-urlencoded'}, proxies=proxies,data=data, timeout=(5,150))\n",
    "        if re.search(\"為預防惡意程式針對本系統進行大量查詢致影響系統服務品質\",response.text):\n",
    "            print(f\"再次檢核：{response.url}\")\n",
    "            url=response.url\n",
    "        elif not re.search(\"為預防惡意程式針對本系統進行大量查詢致影響系統服務品質\",response.text) and response.status_code == 200:\n",
    "            print(\"通過驗證\")\n",
    "            #print(response.text)\n",
    "            break\n",
    "        else:\n",
    "            #print(response.text)\n",
    "            x=x+1\n",
    "            print(f\"驗證失敗，{response.status_code}\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bd8ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#取出字典某 key 值對應 value，若找無此 key 值，則回傳空字串 \n",
    "def dicMemberCheck(key, dicObj):\n",
    "    if key in dicObj:\n",
    "        return dicObj[key]\n",
    "    else:\n",
    "        return \"\"\n",
    "# dicMemberCheck('標案案號', {'標案案號':123,'ji3':456})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7059c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# href = \"http://web.pcc.gov.tw/tps/main/pms/tps/atm/atmHistoryAction.do?method=review&searchMode=common&pkPmsMainHist=null&pkPmsMain=53735656&awardNoticeDateOrgn=111/03/16\"\n",
    "# r=rq.get(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4195902c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# href = \"https://web.pcc.gov.tw/tps/main/pms/tps/atm/atmNonAwardAction.do?searchMode=common&method=nonAwardContentForPublic&pkAtmMain=53705381\"\n",
    "# s = rq.session()\n",
    "# proxies = get_ip_from_db(\"yes\",db_settings,qual_ratio=0.3,mine_ip_use_interval=800)\n",
    "# refresh_proxies=[]\n",
    "# ran=None\n",
    "# (detail_data,detail_data_html,proxies,refresh_proxies,ran) = get_tenderDeclaration_detail(s,href,None,proxies,refresh_proxies,db_settings,diff_seconds_conn=0,headers={},typee=\"final\",No_proxy=True)\n",
    "\n",
    "# conn = pymysql.connect(**db_settings)\n",
    "# get_data=\"SELECT * FROM tender.gov_purchase where (bid_no='CIPAS1110208' and tenderType='final') \"\n",
    "# # 建立Cursor物件\n",
    "# with conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "#   #資料表相關操作\n",
    "#     cursor.execute(get_data)\n",
    "#     raw_datas_all_3 = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c271b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tenderType = 'final'\n",
    "# conn = pymysql.connect(**db_settings)\n",
    "# get_data=\"SELECT * FROM tender.gov_purchase where (detail_data_html ='' and detail_data like '%{}%') and tenderType='\"+tenderType+\"'\"\n",
    "# # 建立Cursor物件\n",
    "# with conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "#   #資料表相關操作\n",
    "#     cursor.execute(get_data)\n",
    "#     raw_datas_all = cursor.fetchall()\n",
    "# raw_datas = raw_datas_all[0:10]\n",
    "# get_only_detail(tenderType,raw_datas,0,0,headers, db_settings,No_proxy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd46f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx=json.loads(raw_datas_all[0]['detail_data'])\n",
    "# raw_datas_all[0]['detail_data']=json.dumps(xx,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "899bee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# href=\"https://web.pcc.gov.tw/tps/tpam/main/tps/tpam/tpam_tender_detail.do?searchMode=common&scope=F&primaryKey=53789621\"\n",
    "# href2=\"https://www.youtube.com/watch?v=uV2rUMcGBYI\"\n",
    "# proxies=get_ip_from_db(\"yes\",db_settings,0.3)\n",
    "# proxies=proxies[:30]\n",
    "# good=[]\n",
    "# for i in proxies:\n",
    "#         try:\n",
    "#         r = rq.get(href, headers = headers,verify=False,proxies={'https':i['proxyIp']}, timeout=(6,15))\n",
    "#         print(str(i['proxyIp'])+\" !!!!!!!!!!!!!\")\n",
    "#         good.append(i)\n",
    "#     except:\n",
    "#         print(str(i['proxyIp'])+\" .............\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48119055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抓招標、決標內頁資料\n",
    "def get_tenderDeclaration_detail(s,href,ran,proxies,refresh_proxies,db_settings,diff_seconds_conn=1.2,headers={},typee=\"\",No_proxy=False):\n",
    "    detail_data_html=\"\"\n",
    "    detail_data={}\n",
    "    global detail_waitt\n",
    "    detail_waitt = 0\n",
    "    #with requests.Session() as s:\n",
    "    headers[\"User-Agent\"] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36'\n",
    "    breakdown = \"no\"\n",
    "    time.sleep(diff_seconds_conn)\n",
    "    if No_proxy:\n",
    "        while detail_waitt:\n",
    "            localtime = time.localtime()\n",
    "            result = time.strftime(\"%Y%m%d_%H%M%S\", localtime)\n",
    "            print(typee + f\"暫停休息 {60*detail_waitt}秒 一下，有執行緒被鎖，目前時間：{result}\")\n",
    "            time.sleep(60*detail_waitt)\n",
    "        while True:\n",
    "            try:\n",
    "                r = s.get(href, headers = headers,verify=False, timeout=(4,15))\n",
    "                if re.match(r'.*validate.*',r.url, flags=0):\n",
    "                    print(f\"本機 IP 準備執行驗證\")\n",
    "                    r = validate_ip(s,r.url,proxies={})\n",
    "                if r.status_code<300 and (re.search(r'公告日',r.text) or re.search(r'標案案號',r.text)):\n",
    "                    detail_waitt = 0\n",
    "                    break\n",
    "                elif re.search(r'系統發生錯誤',r.text) or re.search(r'Error 500',r.text):\n",
    "                    detail_waitt = 0\n",
    "                    detail_data=\"系統發生錯誤\"\n",
    "                    breakdown = \"yes\"\n",
    "                    break\n",
    "            except rq.exceptions.RequestException as ex:\n",
    "                print(\"本機 ip 遭到封鎖\")\n",
    "                detail_waitt = detail_waitt + 1\n",
    "                localtime = time.localtime()\n",
    "                result = time.strftime(\"%Y%m%d_%H%M%S\", localtime)\n",
    "                print(typee + f\"暫停休息 {800*detail_waitt}秒 一下，本機 IP 被鎖，目前時間：{result}\")\n",
    "                print(ex)\n",
    "                time.sleep(800*detail_waitt)\n",
    "            except Exception as ex:\n",
    "                print(f'其他錯誤_1：{ex}')\n",
    "                break\n",
    "    else:\n",
    "        while True:\n",
    "            if not ran:\n",
    "                ran = np.random.randint(len(proxies))\n",
    "            try:\n",
    "                proxies[ran]['connect_times'] =  proxies[ran]['connect_times'] + 1\n",
    "                proxies[ran]['qual_ratio'] = proxies[ran]['successful_connect_times'] / proxies[ran]['connect_times']\n",
    "                #print(f\"{proxies[ran]['proxyIp']} 準備進入，{href}\")                 \n",
    "                r = s.get(href, headers = headers,verify=False,proxies={'https':proxies[ran]['proxyIp']}, timeout=(4,15))\n",
    "                #print(\"內頁取得點存活\")\n",
    "                if re.match(r'.*validate.*',r.url, flags=0):\n",
    "                    print(f\"{proxies[ran]['proxyIp']} 準備執行驗證\")\n",
    "                    r = validate_ip(s,r.url,proxies={'https':proxies[ran]['proxyIp']})\n",
    "                if re.search(r'公告日',r.text):\n",
    "                    proxies[ran]['successful_connect_times'] =  proxies[ran]['successful_connect_times'] + 1\n",
    "                    proxies[ran]['qual_ratio'] = proxies[ran]['successful_connect_times'] / proxies[ran]['connect_times']\n",
    "                    proxies[ran]['updated_time'] = datetime.datetime.now()\n",
    "                    #避免鎖 ip 的情況，所以限定每個 ip 爬取 50 筆資料\n",
    "                    if proxies[ran]['successful_connect_times']>=50:\n",
    "                        refresh_proxies.append(proxies.pop(ran))\n",
    "                        ran=None\n",
    "                        print(\"此 ip 已爬取 50 個\")\n",
    "                    break\n",
    "                elif re.search(r'系統發生錯誤',r.text) or re.search(r'Error 500',r.text):\n",
    "                    detail_data=\"系統發生錯誤\"\n",
    "                    breakdown = \"yes\"\n",
    "                    break\n",
    "                else:\n",
    "                    refresh_proxies.append(proxies.pop(ran))\n",
    "                    ran=None\n",
    "                    # 被chrome擋住 or validate_ip 執行太多次\n",
    "                    print(str(refresh_proxies[-1]['proxyIp']) + \"內頁資料錯誤，還剩餘 \" + str(len(proxies)) +\" 個proxy\")    \n",
    "                    if len(proxies) == 0:\n",
    "                        breakdown = \"yes\"\n",
    "                        print(\"proxy 不夠用了，重新獲取\")\n",
    "                        break\n",
    "            except rq.exceptions.RequestException as ex:\n",
    "                if proxies[ran]['mine']==1:\n",
    "                    update_mine_proxy_ip(proxies[ran],db_settings)\n",
    "                #print(ex)\n",
    "                error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "                detail = ex.args[0] #取得詳細內容\n",
    "                cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "                lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "                fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "                lineNum = lastCallStack[1] #取得發生的行號\n",
    "                funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "                errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "                #print(errMsg)\n",
    "                refresh_proxies.append(proxies.pop(ran))\n",
    "                print(str(refresh_proxies[-1]['proxyIp']) + \" 連接失敗，還剩餘 \" + str(len(proxies)) +\" 個proxy\")\n",
    "                ran=None\n",
    "                if len(proxies) == 0:\n",
    "                    breakdown = \"yes\"\n",
    "                    print(\"proxy 不夠用了，重新獲取\")\n",
    "                    break\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "                detail = ex.args[0] #取得詳細內容\n",
    "                cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "                lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "                fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "                lineNum = lastCallStack[1] #取得發生的行號\n",
    "                funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "                errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "                print(\"其他錯誤_2：\" + errMsg)\n",
    "                ran=None\n",
    "    if breakdown == \"yes\":\n",
    "        return detail_data,detail_data_html,proxies,refresh_proxies,None\n",
    "\n",
    "    html_doc = r.text\n",
    "    soup = BeautifulSoup(html_doc, parser_detail)\n",
    "    if typee==\"predict\":\n",
    "        #政府採購預告\n",
    "        detail_data_html = soup.select('div#printRange')[0]\n",
    "        for i in detail_data_html.select('table'):\n",
    "            for j in i.select('tr'):\n",
    "                try:\n",
    "                    col = j.select('th')[-1].text.strip()\n",
    "                except:\n",
    "                    col = j.select('td')[-2].text.strip()\n",
    "                val = j.select('td')[-1].text.strip()\n",
    "                detail_data[col]=val\n",
    "\n",
    "    elif typee ==\"final\":\n",
    "        detail_data_html = soup.select('div#printArea.main table')[0]\n",
    "        # 決標公告：無法決標、撤銷公告\n",
    "        if soup.select('div#printArea.main div#hidden_message_id'):\n",
    "            detail_data['really_final']=0\n",
    "            block_data = detail_data_html.select('tr')\n",
    "            if not block_data[0].text.strip():\n",
    "                del block_data[0]\n",
    "            if re.search(r'紅色字體表示此次更正公',block_data[0].text):\n",
    "                del block_data[0]\n",
    "            for j in block_data:\n",
    "                try:\n",
    "                    try:\n",
    "                        col = j.select('th')[-1].text.strip()\n",
    "                    except:\n",
    "                        col = j.select('td')[-2].text.strip()\n",
    "                    val = j.select('td')[-1].text.strip()\n",
    "                    detail_data[col]=val\n",
    "                except:\n",
    "                    #https://web.pcc.gov.tw/tps/main/pms/tps/atm/atmNonAwardAction.do?searchMode=common&method=nonAwardContentForPublic&pkAtmMain=53705535\n",
    "                    #https://web.pcc.gov.tw/tps/main/pms/tps/atm/atmNonAwardAction.do?searchMode=common&method=nonAwardContentForPublic&pkAtmMain=53705381\n",
    "                    print(\"其他錯誤_3\")\n",
    "                    print(r.url)\n",
    "                    print(j)\n",
    "                    pass\n",
    "                \n",
    "        else:\n",
    "            #決標公告：正常決標公告\n",
    "            detail_data['really_final']=1\n",
    "            for i in range(1,10):\n",
    "                block_data = detail_data_html.select('tr.award_table_tr_'+str(i))\n",
    "                if i == 1 and re.search(r'紅色字體表示此次更正公',block_data[0].text):\n",
    "                    del block_data[0]\n",
    "                if len(block_data) == 0 :\n",
    "                    continue\n",
    "                else:\n",
    "                    del block_data[0]\n",
    "\n",
    "                for j in block_data:\n",
    "                    try:\n",
    "                        try:\n",
    "                            col = j.select('th.T11b')[-1].text.strip()\n",
    "                        except:\n",
    "                            col = j.select('td.newstop')[-2].text.strip()\n",
    "                        val = j.select('td')[-1].text.strip()\n",
    "                        detail_data[col]=val\n",
    "                    except:\n",
    "                        print(\"其他錯誤_4\")\n",
    "                        print(r.url)\n",
    "                        print(j)\n",
    "                        continue                    \n",
    "                        \n",
    "    else:\n",
    "        if re.search(\"電子競價公告\",html_doc):\n",
    "            keys=[]\n",
    "            values=[]\n",
    "            detail_data_html = soup.select('div#printArea table')[0]\n",
    "            for i in detail_data_html.select('td.T11b'):\n",
    "                keys.append(i.text.strip())\n",
    "\n",
    "            for i in detail_data_html.select('td.newstop'):\n",
    "                values.append(i.text.strip())\n",
    "\n",
    "            detail_data = dict(zip(keys, values))\n",
    "        else:\n",
    "            #招標公告\n",
    "            detail_data_html = soup.select('div#print_area table.table_block.tender_table')[0]\n",
    "            for i in range(1,10):\n",
    "                block_data = detail_data_html.select('tr.tender_table_tr_'+str(i))\n",
    "                if len(block_data) == 0 :\n",
    "                    continue\n",
    "                for j in block_data:\n",
    "                    try:\n",
    "                        try:\n",
    "                            col = j.select('th')[-1].text.strip()\n",
    "                        except:\n",
    "                            col = j.select('td')[-2].text.strip()\n",
    "                        val = j.select('td')[-1].text.strip()\n",
    "                        detail_data[col]=val\n",
    "                    except:\n",
    "                        pass\n",
    "    return detail_data,detail_data_html,proxies,refresh_proxies,ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58beb67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抓公開徵求內頁資料\n",
    "def get_searchAppeal_detail(headers,detail_connect):\n",
    "    detail_data={}\n",
    "    detail_data_html=\"\"\n",
    "    headers['Content-Type']='application/x-www-form-urlencoded'\n",
    "    url = \"https://web.pcc.gov.tw/tps/tps/tp/main/tps/tp/tp.do?method=initialAppealViewVendor&pMenu=common\"\n",
    "    x=0\n",
    "    while True:\n",
    "        try:\n",
    "            x=x+1\n",
    "            r = rq.post(url,data = detail_connect, headers = headers)\n",
    "            if r.status_code == 200 and re.search(r'公告日',r.text):\n",
    "                x=0\n",
    "                break\n",
    "            else:\n",
    "                x=0\n",
    "        except Exception as ex:\n",
    "            time.sleep(1201*x)\n",
    "            print(ex)\n",
    "            print(f\"get_searchAppeal_detail出錯，等待 {1201*x} 秒\")\n",
    "\n",
    "    html_doc = r.text\n",
    "    soup = BeautifulSoup(html_doc, parser)\n",
    "    if re.search(r'無符合此筆公開徵求廠商條件資料',html_doc):\n",
    "        print(\"其他錯誤_5：無符合此筆公開徵求廠商條件資料，可能公告已撤銷\")\n",
    "        detail_data_html = soup.select('center table')[0]\n",
    "        detail_data=\"\"\n",
    "        return detail_data,detail_data_html\n",
    "    try:\n",
    "        detail_data_html = soup.select('div#printRange')[0]\n",
    "    except:\n",
    "        print(\"----------------\")\n",
    "        print(soup)\n",
    "\n",
    "    for i in detail_data_html.select('tr'):\n",
    "        try:\n",
    "            col = i.select('th')[-1].text.strip()\n",
    "        except:\n",
    "            col = i.select('td')[0].text.strip()\n",
    "        val = i.select('td')[-1].text.strip()\n",
    "        detail_data[col]=val\n",
    "    \n",
    "    return detail_data,detail_data_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc0e0896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抓公開閱覽內頁資料\n",
    "def get_publicRead_detail(detail_connect):\n",
    "    x=0\n",
    "    while True:\n",
    "        try:\n",
    "            x=x+1\n",
    "            r = rq.get(detail_connect, headers = headers)\n",
    "            if r.status_code == 200:\n",
    "                x=0\n",
    "                break\n",
    "        except Exception as ex:\n",
    "            time.sleep(1201*x)\n",
    "            print(ex)\n",
    "            print(f\"get_publicRead_detail出錯，等待 {1201*x} 秒\")\n",
    "    \n",
    "    html_doc = r.text\n",
    "    soup = BeautifulSoup(html_doc, parser)\n",
    "    detail_data_html_1 = soup.select('div#printRange')[0]\n",
    "    detail_data_html_2 = soup.select('td#page table table table table')[2]\n",
    "    detail_data={}\n",
    "    for i in detail_data_html_1.select('tr'):\n",
    "        col = i.select('th')[-1].text.strip()\n",
    "        val = i.select('td')[-1].text.strip()\n",
    "        detail_data[col]=val\n",
    "    return detail_data,str(detail_data_html_1) +str(detail_data_html_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddb14e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_date = (datetime.date.today()- datetime.timedelta(days=30)).strftime(\"%Y/%m/%d\")\n",
    "# current_date = str(int(current_date[0:4])-1911) + current_date[4:]\n",
    "# print(current_date)\n",
    "# s = rq.session()\n",
    "\n",
    "# url_first = \"https://web.pcc.gov.tw/tps/pss/tender.do?searchMode=common&searchType=advance\"\n",
    "# data={'method':'search','searchMethod':'true','searchTarget':'ATM','hid_1':1,'hid_2':1,'hid_3':1,'tenderStatus':'5,6,20,28,8,21,22,29,33,9,23','btnQuery':'查詢','awardAnnounceStartDate':current_date,'awardAnnounceEndDate':current_date}\n",
    "# r = s.post(url_first,data = data)\n",
    "\n",
    "# url = \"https://web.pcc.gov.tw/tps/pss/tender.do?searchMode=common&searchType=advance&searchTarget=ATM&method=search&isSpdt=&execLocationArea=&pageIndex=1\"\n",
    "# r = s.get(url)\n",
    "# soup = BeautifulSoup(r.text, parser)\n",
    "# data = soup.select(\"div#print_area table tr\")\n",
    "\n",
    "# for i in data[1:3]:\n",
    "#     #機關名稱\n",
    "#     proposer_name=i.select('td')[1].text.strip()\n",
    "#     print(proposer_name)\n",
    "#     #標案案號\n",
    "#     bid_no=re.match(r'(.*)\\r\\n\\t', i.select('td')[2].text.strip(), flags=0).group(1)\n",
    "#     print(bid_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a5036d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_detail(tenderType,raw_datas,diff_seconds_data,diff_seconds_conn,headers, db_settings,No_proxy=False):\n",
    "    global progress_final_detail\n",
    "    global progress_tenderDeclaration_detail\n",
    "    proxies = get_ip_from_db(\"yes\",db_settings,qual_ratio=0.3)\n",
    "    refresh_proxies=[]\n",
    "    longg = len(raw_datas)\n",
    "    loses=[]\n",
    "    x=0\n",
    "    ran=None\n",
    "    with rq.session() as s:\n",
    "        s.mount('https://', requests.adapters.HTTPAdapter(pool_connections=25, pool_maxsize=50))\n",
    "        s.mount('http://', requests.adapters.HTTPAdapter(pool_connections=20, pool_maxsize=40))\n",
    "        s.mount('https://', requests.adapters.HTTPAdapter(max_retries=3))\n",
    "        s.mount('http://', requests.adapters.HTTPAdapter(max_retries=3))\n",
    "        s.keep_alive = False\n",
    "        for raw_data in raw_datas:\n",
    "            time_start = datetime.datetime.now() #開始計時\n",
    "            print(\"--------------------\")\n",
    "            try:\n",
    "                while True:\n",
    "                    (detail_data,detail_data_html,proxies,refresh_proxies,ran) = get_tenderDeclaration_detail(s, raw_data['detail_connect'],ran,proxies,refresh_proxies,db_settings,diff_seconds_conn,headers,typee=raw_data['tenderType'],No_proxy=No_proxy) \n",
    "                    str_detail_data_html=str(detail_data_html)\n",
    "                    if len(detail_data)>0 and (re.search(raw_data['bid_no'],str_detail_data_html) or re.search(raw_data['proposer_name'],str_detail_data_html)):\n",
    "                        raw_data['detail_data'] = detail_data\n",
    "                        raw_data['detail_data_html'] = detail_data_html\n",
    "                        print(raw_data['detail_data']['機關代碼'])\n",
    "                        \n",
    "                        time_end = datetime.datetime.now()    #結束計時\n",
    "                        diff=time_end-time_start\n",
    "                        if diff.seconds<diff_seconds_data:\n",
    "                            time.sleep(diff_seconds_data-diff.total_seconds())\n",
    "                        time_end = datetime.datetime.now()    #結束計時\n",
    "                        time_c= time_end - time_start   #執行所花時間\n",
    "                        if tenderType=='tenderDeclaration':\n",
    "                            progress_tenderDeclaration_detail = f'progress_tenderDeclaration_detail 目前是第 {x+1} 項，抓取該細項內頁花費 {time_c} 秒，進度：{(x+1)*100/longg}%，總共有 {longg} 項'\n",
    "                            print(progress_tenderDeclaration_detail)\n",
    "                        elif tenderType=='final':\n",
    "                            progress_final_detail = f'progress_final_detail 目前是第 {x+1} 項，抓取該細項內頁花費 {time_c} 秒，進度：{(x+1)*100/longg}%，總共有 {longg} 項'\n",
    "                            print(progress_final_detail)\n",
    "                        loses=[]\n",
    "                        x=x+1\n",
    "                        break\n",
    "                    elif detail_data==\"系統發生錯誤\":\n",
    "                        raw_data['error_code'] = 1\n",
    "                        raw_data['detail_data'] = \"\"\n",
    "                        time_end = datetime.datetime.now()#結束計時\n",
    "                        diff=time_end-time_start\n",
    "                        if diff.seconds<diff_seconds_data:\n",
    "                            time.sleep(diff_seconds_data-diff.total_seconds())\n",
    "                        time_end = datetime.datetime.now()    #結束計時\n",
    "                        time_c= time_end - time_start   #執行所花時間\n",
    "                        if tenderType=='tenderDeclaration':\n",
    "                            progress_tenderDeclaration_detail = f'progress_tenderDeclaration_detail 目前是第 {x+1} 項，抓取該細項內頁花費 {time_c} 秒，系統發生錯誤，進度：{(x+1)*100/longg}%，總共有 {longg} 項'\n",
    "                            print(progress_tenderDeclaration_detail)\n",
    "                        elif tenderType=='final':\n",
    "                            progress_final_detail = f'progress_final_detail 目前是第 {x+1} 項，抓取該細項內頁花費 {time_c} 秒，系統發生錯誤：{raw_data}，進度：{(x+1)*100/longg}%，總共有 {longg} 項'\n",
    "                            print(progress_final_detail)\n",
    "                        loses=[]\n",
    "                        x=x+1\n",
    "                        break\n",
    "                    elif len(proxies)==0:\n",
    "                        send_ip(refresh_proxies, db_settings,send_log=False)\n",
    "                        \n",
    "                        loses.append(1)\n",
    "                        #print(f\"失敗總次數：{len(loses)}，危險！\")\n",
    "                        proxies = get_ip_from_db(\"yes\",db_settings,qual_ratio=0.3,mine_ip_use_interval=800)\n",
    "                        \n",
    "                        # 避免既有的proxy在qual_ratio的情況下突然失效，要讓這種proxy的qual_ratio降到不會被抓取的水平要很久，所以設置如果 7次都用光proxy就重爬proxy一次\n",
    "                        if len(proxies)==0 or len(loses)>7:\n",
    "                            print(f\"失敗總次數：{len(loses)}，強制重抓proxy！！！！！！！！！！\")\n",
    "                            loses=[]\n",
    "                            proxies = get_proxy_ip(\"yes\",db_settings)\n",
    "                            send_ip(proxies, db_settings,new = True)\n",
    "                    else:\n",
    "                        raw_data['error_code'] = 2\n",
    "                        loses=[]\n",
    "                        localtime = time.localtime()\n",
    "                        result = time.strftime(\"%Y%m%d_%H%M%S\", localtime)\n",
    "                        with open(str(raw_data['bid_no'])+'_'+result+'.txt', 'a+', encoding='utf-8') as file:\n",
    "                            file.write(str(raw_data))\n",
    "                            file.close()\n",
    "            except Exception as ex:\n",
    "                error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "                detail = ex.args[0] #取得詳細內容\n",
    "                cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "                lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "                fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "                lineNum = lastCallStack[1] #取得發生的行號\n",
    "                funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "                errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "                print(errMsg)\n",
    "                print(\"其他錯誤_6：\"+str(raw_data))\n",
    "                pass\n",
    "    send_ip(refresh_proxies, db_settings,send_log=False)\n",
    "\n",
    "    if x>=1:\n",
    "        try:\n",
    "            send_db(raw_datas[0:longg],\"detail_data_\"+tenderType, db_settings,Notchange=False)\n",
    "        except Exception as ex:\n",
    "            print(\"其他錯誤_7：招標決標寫入DB 失敗\")\n",
    "            localtime = time.localtime()\n",
    "            result = time.strftime(\"%Y%m%d_%H%M%S\", localtime)\n",
    "            with open('send_db_2_' + result+'.txt', 'a+', encoding='utf-8') as file:\n",
    "                file.write(str(raw_datas[0:longg]))\n",
    "                file.close() \n",
    "            with open('send_db_1_' + result+'.txt', 'a+', encoding='utf-8') as file:\n",
    "                file.write(str(ex))\n",
    "                file.close() \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "202d1c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#招標決標資料塞進DB\n",
    "def send_db(raw_datas,data_class, db_settings,Notchange=True):\n",
    "    errMsg=None\n",
    "    time_start = datetime.datetime.now() #開始計時\n",
    "    data_combine=[]\n",
    "    # 建立Connection物件\n",
    "    conn = pymysql.connect(**db_settings)\n",
    "    # 建立Cursor物件\n",
    "    with conn.cursor() as cursor:\n",
    "      #資料表相關操作\n",
    "        get_max_log_id=\"select MAX(UID) FROM log\"\n",
    "        # % 操作符只能直接用於字串(‘123’)，列表([1,2,3])、元組\n",
    "        if Notchange:\n",
    "            command = \"INSERT INTO gov_purchase(log_UID, proposer_name, bid_no, bid_name, times, typ, clas, date, enddate, budget, ischange, detail_connect, detail_data, detail_data_html, get_data_date, tenderType,error_code)VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) on DUPLICATE KEY UPDATE log_UID = values(log_UID),get_data_date=values(get_data_date),times =values(times),typ=values(typ),clas=values(clas),date = values(date),enddate=values(enddate),budget=values(budget),ischange=values(ischange),detail_connect=values(detail_connect)\"\n",
    "            db_class = \"api_process_list\"\n",
    "        else:\n",
    "            command = \"INSERT INTO gov_purchase(log_UID, proposer_name, bid_no, bid_name, times, typ, clas, date, enddate, budget, ischange, detail_connect, detail_data, detail_data_html, get_data_date, tenderType,error_code)VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) on DUPLICATE KEY UPDATE log_UID = values(log_UID),get_data_date=values(get_data_date),times =values(times),typ=values(typ),clas=values(clas),date = values(date),enddate=values(enddate),budget=values(budget),ischange=values(ischange),detail_connect=values(detail_connect),detail_data=values(detail_data),detail_data_html=values(detail_data_html),error_code=values(error_code)\"\n",
    "            db_class = \"api_process_detail\"\n",
    "        command_log = \"INSERT INTO log(datetime, task, status, record_des, errmsg) VALUES(%s, %s, %s, %s, %s)\"\n",
    "        # 紀錄開始 (暫時取消)\n",
    "#         cursor.execute(command_log, (datetime.datetime.now(), \"gov_purchase\", \"start\", data_class, \"\"))\n",
    "        # 取得 log 的 UID\n",
    "        cursor.execute(get_max_log_id)\n",
    "        log_UID = str(cursor.fetchone()[0])\n",
    "        # 空資料處理\n",
    "        if len(raw_datas)==0:\n",
    "            #cursor.execute(command_log, (datetime.datetime.now(), db_class, \"wrong\", data_class, \"無資料可爬取\"))\n",
    "            pass\n",
    "        elif type(raw_datas) == dict:\n",
    "            data_combine.append((int(log_UID), raw_datas[\"proposer_name\"], raw_datas[\"bid_no\"], raw_datas[\"bid_name\"], raw_datas[\"times\"], raw_datas[\"typ\"], raw_datas[\"clas\"], raw_datas[\"date\"], raw_datas[\"enddate\"], raw_datas[\"budget\"], raw_datas[\"ischange\"], raw_datas[\"detail_connect\"], json.dumps(raw_datas[\"detail_data\"]), str(raw_datas[\"detail_data_html\"]),raw_datas[\"get_data_date\"], raw_datas[\"tenderType\"],raw_datas[\"error_code\"]))\n",
    "        # 組合數據\n",
    "        else:\n",
    "            #抓取到的資料是以公告日期最新到最舊，這樣寫進資料庫時最新的資料會被舊的資料覆蓋住，所以加一個reversed，讓最舊的先進去\n",
    "            for raw_data in reversed(raw_datas):\n",
    "                data_combine.append((int(log_UID), raw_data[\"proposer_name\"], raw_data[\"bid_no\"], raw_data[\"bid_name\"], raw_data[\"times\"], raw_data[\"typ\"], raw_data[\"clas\"], raw_data[\"date\"], raw_data[\"enddate\"], raw_data[\"budget\"], raw_data[\"ischange\"], raw_data[\"detail_connect\"], json.dumps(raw_data[\"detail_data\"]), str(raw_data[\"detail_data_html\"]),raw_data[\"get_data_date\"], raw_data[\"tenderType\"],raw_data[\"error_code\"]))  # 注意要用两个括号扩起来\n",
    "\n",
    "            # 執行\n",
    "            try:\n",
    "                cursor.executemany(command, data_combine)\n",
    "                time_end = datetime.datetime.now()    #結束計時\n",
    "                time_c= time_end - time_start   #執行所花時間\n",
    "                print('招標決標資料塞進DB，time cost', time_c, 's')\n",
    "            except Exception as ex:\n",
    "                error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "                detail = ex.args[0] #取得詳細內容\n",
    "                cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "                lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "                fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "                lineNum = lastCallStack[1] #取得發生的行號\n",
    "                funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "                errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "                print(\"send_db\" + errMsg)\n",
    "                raise\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"fail\", \"wrong\", data_class, str(error_class)+ \"  \" + str(detail) + errMsg))\n",
    "        # 紀錄結束\n",
    "        if not errMsg:\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), db_class, \"success\", data_class, str(len(raw_datas)) + \" 筆資料已完成\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45abed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter   #引入Counter\n",
    "# a = ll\n",
    "# b = dict(Counter(a))\n",
    "# print ([key for key,value in b.items()if value > 1])  #只展示重複元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96ce7a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gov_serach_crawler(url_type,proxies,db_settings,headers={},cookies={},account=\"\",password=\"\",start_page=1,day_before=0):\n",
    "    global waitt\n",
    "    get_data_date = (datetime.date.today() - datetime.timedelta(days=day_before))\n",
    "    current_date = get_data_date.strftime(\"%Y/%m/%d\")\n",
    "    current_date = str(int(current_date[0:4])-1911) + current_date[4:]\n",
    "    # 取得資料日期\n",
    "    print(\"！！！！！爬取日期：\"+current_date)\n",
    "    s = rq.session()\n",
    "    s.keep_alive = True\n",
    "    headers[\"User-Agent\"] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36'\n",
    "    if url_type == \"tenderDeclaration\":\n",
    "        datarow_of_end = -1\n",
    "        count_per_page = 100\n",
    "        url = \"https://web.pcc.gov.tw/tps/pss/tender.do?searchMode=common&searchType=basic&method=search&isSpdt=&pageIndex=\"\n",
    "        url_first = \"https://web.pcc.gov.tw/tps/pss/tender.do?searchMode=common&searchType=basic\"\n",
    "        data={'method':'search','searchMethod':'true','hid_1':'1','tenderType':'tenderDeclaration','tenderWay':'1,2,3,4,5,6,7,10,12','tenderDateRadio':'on','isSpdt':'N','btnQuery':'查詢','tenderEndDate':current_date,'tenderStartDate':current_date,'tenderEndDateStr':current_date,'tenderStartDateStr':current_date}\n",
    "        while waitt:\n",
    "                print(url_type + f\"暫停休息 {60*waitt}秒 一下，有執行緒被鎖\")\n",
    "                time.sleep(60*waitt)\n",
    "        while True:\n",
    "            try:\n",
    "                r = s.post(url_first, headers = headers, cookies = cookies ,data = data)\n",
    "                if r.status_code<300:\n",
    "                    waitt = 0\n",
    "                    break\n",
    "            except Exception as ex: \n",
    "                waitt = waitt + 1\n",
    "                print(url_type + f\"暫停休息 {1200*waitt}秒 一下，本機 IP 被鎖\")\n",
    "                print(ex)\n",
    "                time.sleep(1200*waitt)\n",
    "        global progress_tenderDeclaration\n",
    "    elif url_type == \"searchAppeal\":  \n",
    "        datarow_of_end = -1\n",
    "        count_per_page = 10\n",
    "        url = \"https://web.pcc.gov.tw/tps/tps/tp/main/tps/tp/searchListAppealVendorCommon.do?__PageBase=0&__PageIndex=\"\n",
    "        url_first = \"https://web.pcc.gov.tw/tps/tps/tp/main/tps/tp/searchAppealVendor.do?pMenu=common\"\n",
    "        data={'method':'searchVendor','isVendor':'Y','startDateStr':current_date,'endDateStr':current_date}\n",
    "        while waitt:\n",
    "                print(url_type + f\"暫停休息 {60*waitt}秒 一下，有執行緒被鎖\")\n",
    "                time.sleep(60*waitt)\n",
    "        while True:\n",
    "            try:\n",
    "                r = s.post(url_first, headers = headers, cookies = cookies ,data = data)\n",
    "                if r.status_code<300:\n",
    "                    waitt = 0\n",
    "                    break\n",
    "            except Exception as ex: \n",
    "                waitt = waitt + 1\n",
    "                print(url_type + f\"暫停休息 {1200*waitt}秒 一下，本機 IP 被鎖\")\n",
    "                print(ex)\n",
    "                time.sleep(1200*waitt)\n",
    "        global progress_searchAppeal\n",
    "    elif url_type == \"publicRead\":\n",
    "        datarow_of_end = -2\n",
    "        count_per_page = 10\n",
    "        url = \"https://web.pcc.gov.tw/tps/tps/tp/main/pms/tps/tp/commonPublicReadFormListPageControl.do?__PageBase=0&__PageIndex=\"\n",
    "        url_first = \"https://web.pcc.gov.tw/tps/tps/tp/main/pms/tps/tp/QueryPublicReadData.do?pMenu=common\"\n",
    "        data={'method':'queryPublicReadData','isVendor':'Y','startDateStr':current_date,'endDateStr':current_date}\n",
    "        while waitt:\n",
    "                print(url_type + f\"暫停休息 {60*waitt}秒 一下，有執行緒被鎖\")\n",
    "                time.sleep(60*waitt)\n",
    "        while True:\n",
    "            try:\n",
    "                r = s.post(url_first, headers = headers, cookies = cookies ,data = data)\n",
    "                if r.status_code<300:\n",
    "                    waitt = 0\n",
    "                    break\n",
    "            except Exception as ex: \n",
    "                waitt = waitt + 1\n",
    "                print(url_type + f\"暫停休息 {1200*waitt}秒 一下，本機 IP 被鎖\")\n",
    "                print(ex)\n",
    "                time.sleep(1200*waitt)\n",
    "        global progress_publicRead\n",
    "    elif url_type == \"predict\":\n",
    "        datarow_of_end = -1\n",
    "        count_per_page = 10\n",
    "        login_url = \"https://web.pcc.gov.tw/pis/main/sso/login.jsp\"\n",
    "        login_data = {\"VTI-GROUP\":0,\"id\":account,\"password\":password}\n",
    "        #login_data = urlencode(login_data)\n",
    "        r = s.post(login_url, headers = headers, cookies = cookies ,data = login_data)\n",
    "        print(r.text)\n",
    "        url = \"\"\n",
    "        url_first = \"https://web.pcc.gov.tw/tps/pss/tender.do?searchMode=common&searchType=basic\"\n",
    "        data={'method':'searchPredict','searchMethod':'true','hid_1':1,'isSpdt':'N','btnQuery':'查詢','tenderType':'predict','tenderWay':'各式預定招標方式','tenderDateRadio':'on','tenderStartDateStr':current_date,'tenderEndDateStr':current_date,'tenderStartDate':current_date,'tenderEndDate':current_date}\n",
    "        while waitt:\n",
    "                print(url_type + f\"暫停休息 {60*waitt}秒 一下，有執行緒被鎖\")\n",
    "                time.sleep(60*waitt)\n",
    "        while True:\n",
    "            try:\n",
    "                r = s.post(url_first, headers = headers, cookies = cookies ,data = data)\n",
    "                if r.status_code<300:\n",
    "                    waitt = 0\n",
    "                    break\n",
    "            except Exception as ex: \n",
    "                waitt = waitt + 1\n",
    "                print(url_type + f\"暫停休息 {1200*waitt}秒 一下，本機 IP 被鎖\")\n",
    "                print(ex)\n",
    "                time.sleep(1200*waitt)\n",
    "        global progress_predict\n",
    "    elif url_type == \"final\":  \n",
    "        datarow_of_end = -1\n",
    "        count_per_page = 100\n",
    "        url = \"https://web.pcc.gov.tw/tps/pss/tender.do?searchMode=common&searchType=advance&searchTarget=ATM&method=search&isSpdt=&execLocationArea=&pageIndex=\"\n",
    "        url_first = \"https://web.pcc.gov.tw/tps/pss/tender.do?searchMode=common&searchType=advance\"\n",
    "        data={'method':'search','searchMethod':'true','searchTarget':'ATM','hid_1':1,'hid_2':1,'hid_3':1,'tenderStatus':'5,6,20,28,8,21,22,29,33,9,23','btnQuery':'查詢','awardAnnounceStartDate':current_date,'awardAnnounceEndDate':current_date}\n",
    "        while waitt:\n",
    "                print(url_type + f\"暫停休息 {60*waitt}秒 一下，有執行緒被鎖\")\n",
    "                time.sleep(60*waitt)\n",
    "        while True:\n",
    "            try:\n",
    "                r = s.post(url_first, headers = headers, cookies = cookies ,data = data)\n",
    "                if r.status_code<300:\n",
    "                    waitt = 0\n",
    "                    break\n",
    "            except Exception as ex: \n",
    "                waitt = waitt + 1\n",
    "                print(url_type + f\"暫停休息 {1200*waitt}秒 一下，本機 IP 被鎖\")\n",
    "                print(ex)\n",
    "                time.sleep(1200*waitt)\n",
    "        global progress_final\n",
    "    else:\n",
    "        raise Exception(\"url_type is wrong\") \n",
    "\n",
    "\n",
    "    print(\"外頁進入點存活\")  \n",
    "    refresh_proxies=[]\n",
    "    raw_datas=[]\n",
    "    for ii in range(start_page,2000):\n",
    "        #s = rq.session()\n",
    "        while waitt:\n",
    "                print(url_type + f\"暫停休息 {60*waitt}秒 一下，有執行緒被鎖_phase2\")\n",
    "                time.sleep(60*waitt)\n",
    "        while True:\n",
    "            try:\n",
    "                r = s.get(url + str(ii), headers = headers)\n",
    "                print(\"外頁列表頁存活\")  \n",
    "                if r.status_code == 200:\n",
    "                    waitt = 0\n",
    "                    break\n",
    "            except Exception as ex: \n",
    "                print(url_type + f\"暫停休息 {1200*waitt}秒 一下，本機 IP 被鎖_phase2\")\n",
    "                print(ex)\n",
    "                waitt = waitt + 1\n",
    "                time.sleep(1200*waitt)\n",
    "        print(\"頁面前往：\"+str(r.url))\n",
    "        html_doc = r.text\n",
    "        if re.search(r'無符合條件資料',html_doc):\n",
    "            print(\"無符合條件資料\")\n",
    "            break\n",
    "        soup = BeautifulSoup(html_doc, parser)\n",
    "        \n",
    "        time_start = datetime.datetime.now()\n",
    "        if url_type == \"tenderDeclaration\":\n",
    "            data = soup.select('div#print_area table tr')\n",
    "            all_data = int(data[datarow_of_end].select('span')[-1].text)\n",
    "            xxx = 0\n",
    "            for i in data[1:datarow_of_end]:\n",
    "                t_s = datetime.datetime.now()\n",
    "                xxx = xxx + 1\n",
    "                progress_tenderDeclaration = \"這是第 \" + str(ii) + \" 頁，的第 \"+ str(xxx) + \" 項，總共有 \" + str(all_data) + \" 項，目前進度是 \" + str(((ii-1)*count_per_page+xxx)/all_data*100) + \" %。\"\n",
    "                print(progress_tenderDeclaration)\n",
    "                # 取得資料日期\n",
    "\n",
    "                # 項次\n",
    "                no=i.select('td')[1].text.strip()\n",
    "                # 機關名稱\n",
    "                proposer_name=i.select('td')[1].text.strip()\n",
    "                # 標案案號\n",
    "                bid_no=re.match(r'(.*)\\r\\n\\t', i.select('td')[2].text.strip(), flags=0).group(1)\n",
    "                # 是否更正\n",
    "                if i.select('td')[2].select('font'):\n",
    "                    ischange=1\n",
    "                else:\n",
    "                    ischange=0  \n",
    "                # 標案名稱    \n",
    "                bid_name=i.select('td')[2].select('a')[0].text.strip()\n",
    "                # 傳輸次數\n",
    "                times=i.select('td')[3].text.strip()\n",
    "                # 招標方式\n",
    "                typ=i.select('td')[4].text.strip()\n",
    "                # 採購性質\n",
    "                clas=i.select('td')[5].text.strip()\n",
    "                # 公告日期\n",
    "                date=i.select('td')[6].text.strip()\n",
    "                # 截止投標\n",
    "                enddate=i.select('td')[7].text.strip()\n",
    "                # 預算金額\n",
    "                budget=i.select('td')[8].text.strip()\n",
    "                # 內文連結\n",
    "                detail_connect = \"https://web.pcc.gov.tw/tps/\" + i.select('td')[2].select('a')[0]['href'][3:]\n",
    "                detail_data ={}\n",
    "                detail_data_html=\"\"\n",
    "#                 while True:\n",
    "#                     if len(proxies)==0:\n",
    "#                         send_ip(refresh_proxies, db_settings,send_log=False)\n",
    "#                         refresh_proxies=[]\n",
    "#                         proxies = get_ip_from_db(\"yes\",db_settings,qual_ratio=0.3)\n",
    "#                         if proxies == []:\n",
    "#                             xx = get_proxy_ip(\"yes\")\n",
    "#                             send_ip(xx, db_settings)\n",
    "#                             proxies = get_ip_from_db(\"yes\",db_settings)\n",
    "#                         #(detail_data,detail_data_html,proxies,refresh_proxies) = get_tenderDeclaration_detail(detail_connect,proxies,refresh_proxies,headers) \n",
    "#                     (detail_data,detail_data_html,proxies,refresh_proxies) = get_tenderDeclaration_detail(detail_connect,proxies,refresh_proxies,headers) \n",
    "#                     str_detail_data_html=str(detail_data_html)\n",
    "#                     if len(detail_data)>0 and (re.search(bid_no,str_detail_data_html) or re.search(proposer_name,str_detail_data_html)):\n",
    "#                         print(detail_data['機關代碼'])\n",
    "#                         break\n",
    "                t_e = datetime.datetime.now()\n",
    "                time_se= t_e - t_s\n",
    "                print('此細項已抓取完成，總耗時：', time_se, 's')\n",
    "                print(\"------------------------------------\")\n",
    "                raw_datas.append({'no': no,'proposer_name': proposer_name,'bid_no': bid_no,'bid_name': bid_name,'times': times,'typ': typ,'clas': clas,'date': date,'enddate': enddate,'budget': budget,'ischange': ischange,'detail_connect':detail_connect,'detail_data':detail_data,'detail_data_html':detail_data_html,'get_data_date':get_data_date,'tenderType':\"tenderDeclaration\",'error_code':0})\n",
    "        elif url_type == \"searchAppeal\":\n",
    "            data = soup.select('center table:nth-child(1) tr')\n",
    "            all_data = int(data[datarow_of_end].select('span')[-1].text)\n",
    "            xxxx = 0\n",
    "            for i in data[17:datarow_of_end]:\n",
    "                xxxx = xxxx + 1\n",
    "                progress_searchAppeal = \"這是第 \" + str(ii) +\" 頁，的第 \"+ str(xxxx) + \" 項，總共有 \" + str(all_data) + \" 項，目前進度是 \" + str(((ii-1)*count_per_page+xxxx)/all_data*100) + \" %。\"\n",
    "                print(progress_searchAppeal)\n",
    "                # 取得資料日期\n",
    "\n",
    "                # 項次\n",
    "                no=i.select('td')[0].text.strip()\n",
    "                # 機關名稱\n",
    "                proposer_name=i.select('td')[1].text.strip()\n",
    "                # 標案案號\n",
    "                bid_no=i.select('td')[2].text.strip()\n",
    "                # 是否更正\n",
    "                ischange=0  \n",
    "                # 標案名稱    \n",
    "                bid_name=i.select('td')[3].text.strip()\n",
    "                # 傳輸次數 (公告次數)\n",
    "                times=i.select('td')[4].text.strip()\n",
    "                # 招標方式\n",
    "                typ=\"\"\n",
    "                # 採購性質\n",
    "                clas=\"\"\n",
    "                # 公告日期\n",
    "                date=re.match(r'(.*)\\s*－\\s*(.*)', i.select('td')[5].text.strip(), flags=0).group(1).strip()\n",
    "                # 截止投標\n",
    "                enddate=re.match(r'(.*)\\s*－\\s*(.*)', i.select('td')[5].text.strip(), flags=0).group(2).strip()\n",
    "                # 預算金額\n",
    "                budget=\"\"\n",
    "                # 內文連結\n",
    "                detail_connect = {}\n",
    "                detail_connects = i.select('td.T12 input')\n",
    "                for ij in detail_connects:\n",
    "                    try:\n",
    "                        detail_connect[ij['id']]=ij['value']\n",
    "                    except:\n",
    "                        break\n",
    "                detail_connect=urlencode(detail_connect)\n",
    "                (detail_data,detail_data_html) = get_searchAppeal_detail(headers,detail_connect)\n",
    "                raw_datas.append({'no': no,'proposer_name': proposer_name,'bid_no': bid_no,'bid_name': bid_name,'times': times,'typ': typ,'clas': clas,'date': date,'enddate': enddate,'budget': budget,'ischange': ischange,'detail_data':detail_data, 'detail_connect':detail_connect,'detail_data_html':detail_data_html,'get_data_date':get_data_date,'tenderType':\"searchAppeal\",'error_code':0})\n",
    "        elif url_type == \"publicRead\":\n",
    "            data = soup.select('td#page table:nth-child(3) tr')\n",
    "            all_data = int(data[datarow_of_end].select('span')[-1].text)\n",
    "            xxxxx=0\n",
    "            for i in data[15:datarow_of_end]:\n",
    "                xxxxx = xxxxx + 1\n",
    "                progress_publicRead = \"這是第 \" + str(ii) +\" 頁，的第 \"+ str(xxxxx) + \" 項，總共有 \" + str(all_data) + \" 項，目前進度是 \" +str(((ii-1)*count_per_page+xxxxx)/all_data*100) + \" %。\"\n",
    "                print(progress_publicRead)\n",
    "                # 取得資料日期\n",
    "\n",
    "                # 項次\n",
    "                no=i.select('td')[0].text.strip()\n",
    "                # 機關名稱\n",
    "                proposer_name=i.select('td')[1].text.strip()\n",
    "                # 標案案號\n",
    "                bid_no=i.select('td')[2].text.strip()\n",
    "                # 內文連結\n",
    "                detail_connect = \"https://web.pcc.gov.tw\" + i.select('td')[2].select('a')[0]['href']\n",
    "                # 是否更正\n",
    "                ischange=0  \n",
    "                # 標案名稱    \n",
    "                bid_name=i.select('td')[3].text.strip()\n",
    "                # 傳輸次數 (公告次數)\n",
    "                times=i.select('td')[4].text.strip()\n",
    "                # 招標方式\n",
    "                typ=\"\"\n",
    "                # 採購性質\n",
    "                clas=\"\"\n",
    "                # 公告日期\n",
    "                date=re.match(r'(.*)\\s*─\\s*(.*)', i.select('td')[5].text.strip(), flags=0).group(1).strip()\n",
    "                # 截止投標\n",
    "                enddate=re.match(r'(.*)\\s*─\\s*(.*)', i.select('td')[5].text.strip(), flags=0).group(2).strip()\n",
    "                # 預算金額\n",
    "                budget=\"\"\n",
    "                (detail_data,detail_data_html)=get_publicRead_detail(detail_connect)\n",
    "                detail_data=\"\"\n",
    "                raw_datas.append({'no': no,'proposer_name': proposer_name,'bid_no': bid_no,'bid_name': bid_name,'times': times,'typ': typ,'clas': clas,'date': date,'enddate': enddate,'budget': budget,'ischange': ischange,'detail_connect':detail_connect,'detail_data':detail_data,'detail_data_html':detail_data_html,'get_data_date':get_data_date,'tenderType':\"publicRead\",'error_code':0})\n",
    "        elif url_type == \"predict\" : \n",
    "            data = soup.select(\"div#print_area table tr\")\n",
    "            all_data = int(data[datarow_of_end].select('span')[-1].text)\n",
    "            xxxxx = 0\n",
    "            for i in data[1:datarow_of_end]:\n",
    "                xxxxx = xxxxx + 1\n",
    "                progress_predict = \"這是第 \" + str(ii) +\" 頁，的第 \"+ str(xxxxx) + \" 項，總共有 \" + str(all_data) + \" 項，目前進度是 \" + str(((ii-1)*count_per_page+xxxxx)/all_data*100) + \" %。\"\n",
    "                print(progress_predict)\n",
    "                # 取得資料日期\n",
    "\n",
    "                # 項次\n",
    "                no=i.select('td')[0].text.strip()\n",
    "                # 機關名稱\n",
    "                proposer_name=i.select('td')[3].text.strip()\n",
    "                # 標案案號\n",
    "                bid_no=i.select('td')[1].text.strip()\n",
    "                # 是否更正\n",
    "                ischange=0  \n",
    "                # 標案名稱    \n",
    "                bid_name=i.select('td')[2].select('a')[0].text.strip()\n",
    "                # 傳輸次數\n",
    "                times=\"\"\n",
    "                # 招標方式\n",
    "                typ=i.select('td')[4].text.strip()\n",
    "                # 採購性質\n",
    "                clas=\"\"\n",
    "                # 公告日期\n",
    "                date=i.select('td')[5].text.strip()\n",
    "                # 截止投標\n",
    "                enddate=\"\"\n",
    "                # 預算金額\n",
    "                budget=\"\"\n",
    "                # 內文連結\n",
    "                detail_connect = \"https://web.pcc.gov.tw/tps/\" + i.select('td')[6].select('a')[0]['href'][5:]\n",
    "                print(detail_connect)\n",
    "                while True:\n",
    "                    if len(proxies)==0:\n",
    "                        send_ip(refresh_proxies, db_settings,send_log=False)\n",
    "                        refresh_proxies=[]\n",
    "                        proxies = get_ip_from_db(\"yes\",db_settings,qual_ratio=0.3)\n",
    "                        if proxies == []:\n",
    "                            xx = get_proxy_ip(\"yes\",db_settings)\n",
    "                            send_ip(xx, db_settings)\n",
    "                            proxies = get_ip_from_db(\"yes\",db_settings)\n",
    "                        #(detail_data,detail_data_html,proxies,refresh_proxies) = get_tenderDeclaration_detail(detail_connect,proxies,refresh_proxies,headers) \n",
    "                    (detail_data,detail_data_html,proxies,refresh_proxies) = get_tenderDeclaration_detail(detail_connect,proxies,refresh_proxies,headers,typee=\"predict\") \n",
    "                    str_detail_data_html=str(detail_data_html)\n",
    "                    if len(proxies)>0 and (re.search(bid_no,str_detail_data_html) or re.search(proposer_name,str_detail_data_html)):\n",
    "                        print(\"predict detail 通過\")\n",
    "                        if re.search(r'勞務類',str_detail_data_html):\n",
    "                            clas=clas+'勞務類'\n",
    "                        if re.search(r'工程類',str_detail_data_html):\n",
    "                            clas=clas+'工程類'\n",
    "                        if re.search(r'財物類',str_detail_data_html):\n",
    "                            clas=clas+'財物類'\n",
    "                raw_datas.append({'no': no,'proposer_name': proposer_name,'bid_no': bid_no,'bid_name': bid_name,'times': times,'typ': typ,'clas': clas,'date': date,'enddate': enddate,'budget': budget,'ischange': ischange,'detail_connect':detail_connect,'detail_data':detail_data,'detail_data_html':detail_data_html,'get_data_date':get_data_date,'tenderType':\"predict\",'error_code':0})\n",
    "        elif url_type == \"final\" : \n",
    "            data = soup.select(\"div#print_area table tr\")\n",
    "            all_data = int(data[datarow_of_end].select('span')[-1].text)\n",
    "            xxxxxx = 0\n",
    "            for i in data[1:datarow_of_end]:\n",
    "                t_s = datetime.datetime.now()\n",
    "                xxxxxx = xxxxxx + 1\n",
    "                progress_final = \"這是第 \" + str(ii) +\" 頁，的第 \"+ str(xxxxxx) + \" 項，總共有 \" + str(all_data) + \" 項，目前進度是 \" + str(((ii-1)*count_per_page+xxxxxx)/all_data*100) + \" %。\"\n",
    "                print(progress_final)\n",
    "                # 取得資料日期\n",
    "\n",
    "                # 項次\n",
    "                no=i.select('td')[0].text.strip()\n",
    "                # 機關名稱\n",
    "                proposer_name=i.select('td')[1].text.strip()\n",
    "                # 標案案號\n",
    "                bid_no=re.match(r'(.*)\\r\\n\\t', i.select('td')[2].text.strip(), flags=0).group(1)\n",
    "                # 是否更正\n",
    "                if i.select('td')[2].select('font'):\n",
    "                    ischange=1\n",
    "                else:\n",
    "                    ischange=0   \n",
    "                # 標案名稱    \n",
    "                bid_name=i.select('td')[2].select('u')[0].text.strip()\n",
    "                # 傳輸次數\n",
    "                times=\"\"\n",
    "                # 這樣可以從內頁資料抓傳輸次數： 上面有定義 dicMemberCheck 可以吐出字典裡對應key的value\n",
    "#                 if detail_data['ischange'] ==0 and not re.search(r'限制性招標',detail_data['招標方式']) and not re.search(r'選擇性招標',detail_data['招標方式']):\n",
    "#                     print(detail_data['新增公告傳輸次數'])\n",
    "                \n",
    "                # 招標方式\n",
    "                typ=i.select('td')[3].text.strip()\n",
    "                # 採購性質\n",
    "                clas=i.select('td')[4].text.strip()\n",
    "                # 公告日期\n",
    "                date=i.select('td')[5].text.strip()\n",
    "                # 截止投標\n",
    "                enddate=\"\"\n",
    "                # 預算金額\n",
    "                budget=i.select('td')[6].text.strip()\n",
    "                # 內文連結\n",
    "                detail_connect = \"https://web.pcc.gov.tw/tps/\" + i.select('td')[2].select('a')[0]['href'][3:]\n",
    "                detail_data={}\n",
    "                detail_data_html=\"\"\n",
    "#                 while True:\n",
    "#                     if len(proxies)==0:\n",
    "#                         send_ip(refresh_proxies, db_settings,send_log=False)\n",
    "#                         refresh_proxies=[]\n",
    "#                         proxies = get_ip_from_db(\"yes\",db_settings,qual_ratio=0.3)\n",
    "#                         if proxies == []:\n",
    "#                             xx = get_proxy_ip(\"yes\",db_settings)\n",
    "#                             send_ip(xx, db_settings)\n",
    "#                             proxies = get_ip_from_db(\"yes\",db_settings)\n",
    "#                         #(detail_data,detail_data_html,proxies,refresh_proxies) = get_tenderDeclaration_detail(detail_connect,proxies,refresh_proxies,headers) \n",
    "#                     (detail_data,detail_data_html,proxies,refresh_proxies) = get_tenderDeclaration_detail(s, detail_connect,proxies,refresh_proxies,headers,typee=\"final\") \n",
    "#                     str_detail_data_html=str(detail_data_html)\n",
    "#                     if len(detail_data)>0 and (re.search(bid_no,str_detail_data_html) or re.search(proposer_name,str_detail_data_html)):\n",
    "#                         print(detail_data['機關代碼'])\n",
    "#                         break\n",
    "                t_e = datetime.datetime.now()\n",
    "                time_se= t_e - t_s\n",
    "                print('此細項已抓取完成，總耗時：', time_se, 's')\n",
    "                print(\"------------------------------------\")\n",
    "                raw_datas.append({'no': no,'proposer_name': proposer_name,'bid_no': bid_no,'bid_name': bid_name,'times': times,'typ': typ,'clas': clas,'date': date,'enddate': enddate,'budget': budget,'ischange': ischange,'detail_connect':detail_connect,'detail_data':detail_data,'detail_data_html':detail_data_html,'get_data_date':get_data_date,'tenderType':\"final\",'error_code':0})\n",
    "        time_end = datetime.datetime.now()\n",
    "        time_c= time_end - time_start\n",
    "        print('此分頁抓取總耗時：', time_c, 's')\n",
    "        #all_data = int(data[datarow_of_end].select('span')[-1].text)\n",
    "        print(all_data)\n",
    "        print(ii*count_per_page)\n",
    "        if ii*count_per_page >= all_data:\n",
    "            if url_type == \"tenderDeclaration\":\n",
    "                del progress_tenderDeclaration\n",
    "            elif url_type == \"searchAppeal\":\n",
    "                del progress_searchAppeal\n",
    "            elif url_type == \"publicRead\":    \n",
    "                del progress_publicRead\n",
    "            elif url_type == \"predict\":    \n",
    "                del progress_predict\n",
    "            elif url_type == \"final\":    \n",
    "                del progress_final\n",
    "            break\n",
    "    return raw_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd3ceb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_date(s,start_date, end_date,if_publish,headers):\n",
    "    r = s.get(\"https://web.pcc.gov.tw/prkms/prms-viewDailyTenderListClient.do?root=tps\",headers=headers)\n",
    "    html_doc = r.text\n",
    "    if r.status_code == 500 or re.search(r'The server encountered an internal error or misconfiguration and was unable to complete your request',html_doc):\n",
    "        raise Exception(\"打不進去header頁\") \n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(html_doc, parser)\n",
    "    publishs_href=[]\n",
    "    publishs_date=[]\n",
    "    not_publishs_href=[]\n",
    "    not_publishs_date=[]\n",
    "    left = soup.select('td#page table tr td:nth-child(2) li a')\n",
    "    right = soup.select('td#page table tr td:nth-child(4) li a')\n",
    "    for i in left:\n",
    "        publishs_href.append(i['href'])\n",
    "        y=str(int(re.match(r'(.*)年(.*)月(.*)日',i['title']).group(1))+1911)\n",
    "        m=re.match(r'(.*)年(.*)月(.*)日',i['title']).group(2)\n",
    "        d=re.match(r'(.*)年(.*)月(.*)日',i['title']).group(3)\n",
    "        ymd = datetime.datetime.strptime(\"-\".join([y,m,d]), \"%Y-%m-%d\")\n",
    "        publishs_date.append(ymd)\n",
    "    for i in right:\n",
    "        not_publishs_href.append(i['href'])\n",
    "        y=str(int(re.match(r'(.*)年(.*)月(.*)日',i['title']).group(1))+1911)\n",
    "        m=re.match(r'(.*)年(.*)月(.*)日',i['title']).group(2)\n",
    "        d=re.match(r'(.*)年(.*)月(.*)日',i['title']).group(3)\n",
    "        ymd = datetime.datetime.strptime(\"-\".join([y,m,d]), \"%Y-%m-%d\")\n",
    "        not_publishs_date.append(ymd)\n",
    "    for i in publishs_date:\n",
    "        publishs_start_date_index = publishs_date.index(i)\n",
    "        if i < start_date:\n",
    "            break\n",
    "    for i in not_publishs_date:\n",
    "        not_publishs_start_date_index = not_publishs_date.index(i)\n",
    "        if i < start_date:\n",
    "            break\n",
    "    for i in publishs_date:\n",
    "        publishs_end_date_index = publishs_date.index(i)\n",
    "        if i <= end_date:\n",
    "            break\n",
    "    for i in not_publishs_date:\n",
    "        not_publishs_end_date_index = not_publishs_date.index(i)\n",
    "        if i <= end_date:\n",
    "            break\n",
    "\n",
    "    if not_publishs_start_date_index<not_publishs_end_date_index:\n",
    "        raise Exception(\"開始日期不可大於結束日期\")\n",
    "    if publishs_start_date_index<publishs_end_date_index:\n",
    "        raise Exception(\"開始日期不可大於結束日期\")\n",
    "    if start_date > not_publishs_date[0] or start_date > publishs_date[0]:\n",
    "        raise Exception(\"無資料可爬，初始日期大於網站最新日期\")\n",
    "    \n",
    "    total_hrefs=[]\n",
    "    if if_publish==\"all\" or if_publish==\"yes\":\n",
    "        for i in range(0,len(publishs_href[publishs_end_date_index:publishs_start_date_index])):\n",
    "            total_href={'href':publishs_href[publishs_end_date_index+i],'date':publishs_date[publishs_end_date_index+i],'ispublish':1}\n",
    "            total_hrefs.append(total_href)\n",
    "    if if_publish==\"all\" or if_publish==\"no\":\n",
    "        for i in range(0,len(not_publishs_href[not_publishs_end_date_index:not_publishs_start_date_index])):\n",
    "            total_href={'href':not_publishs_href[not_publishs_end_date_index+i],'date':not_publishs_date[not_publishs_end_date_index+i],'ispublish':0}\n",
    "            total_hrefs.append(total_href)\n",
    "    return total_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d7f3488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_serach_crawler(s,search_date,headers):\n",
    "    url_head = \"https://web.pcc.gov.tw/prkms/\"\n",
    "    len_search_date=len(search_date)\n",
    "    global progress_date_serach\n",
    "    if len_search_date==0:\n",
    "        return\n",
    "    xxxxxx = 0\n",
    "    for i in search_date:\n",
    "        xxxxxx = xxxxxx + 1\n",
    "        progress_date_serach = \"這是第 \" + str(xxxxxx) + \" 個，總共有 \" + str(len_search_date) + \" 個要爬\"\n",
    "        print(search_date.index(i))\n",
    "        r = s.get(url_head + i['href'], headers=headers)\n",
    "        html_doc = r.text\n",
    "        soup = BeautifulSoup(html_doc, parser)\n",
    "        i['detail_data'] = soup.select('td#page table table table table tr td')[0]\n",
    "    del progress_date_serach\n",
    "    return search_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bc27f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_db_2(raw_datas,data_class, db_settings):\n",
    "    time_start = datetime.datetime.now() #開始計時\n",
    "    data_combine=[]\n",
    "    try:\n",
    "        # 建立Connection物件\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        # 建立Cursor物件\n",
    "        with conn.cursor() as cursor:\n",
    "          #資料表相關操作\n",
    "            get_max_log_id=\"select MAX(UID) FROM log\"\n",
    "            # % 操作符只能直接用於字串(‘123’)，列表([1,2,3])、元組\n",
    "            command = \"INSERT INTO date_search_by_date(log_UID, date, ispublish, upper, lower, href)VALUES(%s, %s, %s, %s, %s, %s) on DUPLICATE KEY UPDATE log_UID = values(log_UID),date=values(date),ispublish =values(ispublish),upper=values(upper),lower=values(lower),href = values(href)\"\n",
    "            command_log = \"INSERT INTO log(datetime, task, status, record_des, errmsg) VALUES(%s, %s, %s, %s, %s)\"\n",
    "            # 紀錄開始 (暫時取消)\n",
    "#             cursor.execute(command_log, (datetime.datetime.now(), \"date_search_by_date\", \"start\", data_class, \"\"))\n",
    "            # 取得 log 的 UID\n",
    "            cursor.execute(get_max_log_id)\n",
    "            log_UID = str(cursor.fetchone()[0])\n",
    "            # 組合數據\n",
    "            for raw_data in raw_datas:\n",
    "                data_combine.append((int(log_UID), raw_data[\"date\"], raw_data[\"ispublish\"], \"\", raw_data[\"detail_data\"], raw_data[\"href\"]))  # 注意要用两个括号扩起来\n",
    "\n",
    "            # 執行\n",
    "            try:\n",
    "                cursor.executemany(command, data_combine)\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "                detail = ex.args[0] #取得詳細內容\n",
    "                cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "                lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "                fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "                lineNum = lastCallStack[1] #取得發生的行號\n",
    "                funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "                errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "                print(errMsg)\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"date_search_by_date\", \"fail\", data_class, error_class + detail + errMsg))\n",
    "            # 紀錄結束\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"date_search_by_date\", \"success\", data_class, str(len(raw_datas)) + \" 筆資料已完成\"))\n",
    "\n",
    "    except Exception as ex:\n",
    "        print('send_db_2 出現錯誤：' + str(ex))\n",
    "\n",
    "    time_end = datetime.datetime.now()    #結束計時\n",
    "    time_c= time_end - time_start   #執行所花時間\n",
    "    print('time cost', time_c, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33c8b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global waitt\n",
    "# waitt = False\n",
    "# proxies = get_ip_from_db(\"yes\",db_settings)\n",
    "# x4 = gov_serach_crawler(\"searchAppeal\",proxies,db_settings, headers = headers,day_before=3)\n",
    "# send_db(x4,\"predict\", db_settings)\n",
    "# x5 = gov_serach_crawler(\"final\",proxies,db_settings, headers = headers)\n",
    "# send_db(x5,\"final\", db_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "360acc0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "請輸入主機ip ( 若要從本機啟動，可輸入127.0.0.1 )：\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [28/May/2022 01:18:07] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/May/2022 01:18:23] \"\u001b[37mGET /start_task_detail HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tenderDeclaration 一共抓出 0 筆還沒有內頁資料\n",
      "tenderDeclaration 資料皆已爬取完成\n",
      "final 一共抓出 0 筆還沒有內頁資料\n",
      "final 資料皆已爬取完成\n"
     ]
    }
   ],
   "source": [
    "#雙重 thread\n",
    "my_host=config_my_host\n",
    "app = Flask(__name__)\n",
    "headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "    \"Accept-Language\": \"zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6\", \n",
    "    \"Host\": \"web.pcc.gov.tw\",  #目標網站 \n",
    "    \"Sec-Fetch-Dest\": \"document\", \n",
    "    \"Sec-Fetch-Mode\": \"navigate\", \n",
    "    \"Sec-Fetch-Site\": \"same-origin\", \n",
    "    \"Upgrade-Insecure-Requests\": \"?1\", \n",
    "     #使用者代理\n",
    "}\n",
    "command_log = \"INSERT INTO log(datetime, task, status, record_des, errmsg) VALUES(%s, %s, %s, %s, %s)\"\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def test():\n",
    "#     ip = request.remote_addr\n",
    "#     print(ip)\n",
    "#     conn = pymysql.connect(**db_settings)\n",
    "#     # 建立Cursor物件\n",
    "#     with conn.cursor() as cursor:\n",
    "#         if_finish_command=\"SELECT status FROM log where task ='api_process_list' order by UID DESC\"\n",
    "#         cursor.execute(if_finish_command)\n",
    "#         if cursor.fetchone() is None:\n",
    "#             cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", \"initial\", \"\", \"\"))\n",
    "#         cursor.execute(if_finish_command)\n",
    "#         if_finish = str(cursor.fetchone()[0])\n",
    "#     if re.search(r'finish', if_finish, flags=0) or if_finish == \"initial\":\n",
    "#         wording ='<p>恭喜!網站建置成功!</p><p>api說明：</p><p>1.&nbsp;/thread：查看當前執行緒狀態，無須參數 (get請求)</p><p>2.&nbsp;/start_task_all：開始執行爬蟲，若要強制重啟，參數帶 restart = 1 (get請求)</p><p>3.&nbsp;/date_search_crawler：公告日期執行爬蟲，參數帶 start_date、end_date、if_publish (get請求)<br />start_date、end_date =&gt; 以字串型態帶日期，ex:20210125，if_publish =&gt; 以字串形式帶 all | yes | no </p><p>http://127.0.0.1:5000/date_search_crawler?start_date=20210128&end_date=20210204&if_publish=yes</p><p>4.&nbsp;/progress：查看當前執行緒進度，無須參數 (get請求)</p><p>5. /proxy : 從免費代理網站抓取proxy進資料庫，無須參數 (get請求)'\n",
    "#         status_code = 200\n",
    "#     else:\n",
    "#         wording =\"I am busy now, please wait\"\n",
    "#         status_code = 500\n",
    "        \n",
    "    wording ='<p>恭喜!網站建置成功!</p><p>api說明：</p><p>1.&nbsp;/thread：查看當前執行緒狀態，無須參數 (get請求)</p><p>2.&nbsp;/start_task_list：執行爬蟲：抓列表資料，若要強制重啟，參數帶 restart = 1；若要設定爬取的初始頁面，參數帶 start_page=[x,x,x,x] ([招標公告頁數、公開徵求頁數、公開閱覽頁數、決標公告頁數])；若想要設定爬取 N 天前的資料，參數要帶 day_before，以 int 的方式放值，ex: day_before=0  (get請求)</p><p>http://127.0.0.1:5000/start_task_list?restart=1&start_page=[1,1,1,1]&day_before=0</p><p><p>3.&nbsp;/start_task_detail：執行爬蟲：抓內頁資料，要晚於 /start_task_list 5分鐘，spilt 參數代表分段執行，spilt[0]是第幾段；spilt[1]是全部共幾段 (get請求)</p><p>http://127.0.0.1:5000/start_task_detail?spilt=(1,2)</p>4.&nbsp;/date_search_crawler：執行爬蟲：依公告日期查詢，參數帶 start_date、end_date、if_publish (get請求)<br />start_date、end_date =&gt; 以字串型態帶日期，ex:20210125，if_publish =&gt; 以字串形式帶 all | yes | no </p><p>http://127.0.0.1:5000/date_search_crawler?start_date=20210128&end_date=20210204&if_publish=yes</p><p>5.&nbsp;/progress：查看當前執行緒進度，無須參數 (get請求)</p><p>6. /proxy : 從免費代理網站抓取proxy進資料庫，無須參數 (get請求)' \n",
    "    status_code = 200\n",
    "\n",
    "    return wording,status_code\n",
    "\n",
    "@app.route('/thread', methods=['GET'])\n",
    "def thread_status():\n",
    "    wording =\"\"\n",
    "    wording2 =\"\"\n",
    "    wording3 =\"\"\n",
    "    wording4 =\"\"\n",
    "    wording5=\"\"\n",
    "    wording6=\"\"\n",
    "    code =200\n",
    "    print(str(threading.enumerate()))\n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_tenderDeclaration_searchAppeal\") >=0:\n",
    "            wording = \"當前正在執行 thread_tenderDeclaration_searchAppeal 執行緒\"\n",
    "            code = 500\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_date_search_crawler\") >=0:\n",
    "            wording2 = \"當前正在執行 thread_date_search_crawler 執行緒\"\n",
    "            code = 500\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_proxy_crawler\") >=0:\n",
    "            wording3 = \"當前正在執行 thread_proxy_crawler 執行緒\"\n",
    "            code = 500\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_final_datesearch_publicRead\") >=0:\n",
    "            wording4 = \"當前正在執行 thread_final_datesearch_publicRead 執行緒\"\n",
    "            code = 500\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_tenderDeclaration_detail\") >=0:\n",
    "            wording5 = \"當前正在執行 thread_tenderDeclaration_detail 執行緒\"\n",
    "            code = 500\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_final_detail\") >=0:\n",
    "            wording6 = \"當前正在執行 thread_final_detail 執行緒\"\n",
    "            code = 500\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "        \n",
    "    return wording + \" \" + wording2 + \" \" + wording3 + \" \" + wording4 + \" \" + wording5 + \" \" + wording6,code\n",
    "        \n",
    "@app.route('/proxy', methods=['GET'])\n",
    "def proxy():\n",
    "    def proxyy():\n",
    "        p_yes=get_proxy_ip(\"yes\",db_settings)\n",
    "        #p_no=get_proxy_ip(\"no\",db_settings)\n",
    "        send_ip(p_yes, db_settings ,\"only https\",new = True)\n",
    "        #send_ip(p_no, db_settings ,\"only http\",new = True)\n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_proxy_crawler\") >=0:\n",
    "            return \"已經在爬 proxy 了，別吵\",500\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass    \n",
    "    \n",
    "    thread_proxy_crawler = threading.Thread(target=proxyy, kwargs={},name=\"thread_proxy_crawler\")\n",
    "    thread_proxy_crawler.start()\n",
    "    return \"started\",200\n",
    "    \n",
    "@app.route('/start_task_list', methods=['GET'])\n",
    "def start_task_all():\n",
    "    if request.args:\n",
    "        dd= request.args.to_dict()\n",
    "        if set(['start_page','restart','day_before']) >= set(dd.keys()):\n",
    "            print(\"傳參成功\")\n",
    "        else:\n",
    "            return \"傳參失敗，參數只允許\"+' start_page '+'restart '+'day_before',500\n",
    "    def do_work(start_page,day_before,db_settings,headers):\n",
    "        get_data_date = (datetime.date.today() - datetime.timedelta(days=day_before))\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", str(ip) + \" start\", \"tenderDeclaration_searchAppeal\", f\"爬取日期：{get_data_date}，tenderDeclaration 從第 {start_page[0]} 頁開始爬，searchAppeal 從第 {start_page[1]} 頁開始爬\"))\n",
    "        time_start = time.time()\n",
    "        proxies = get_ip_from_db(\"yes\",db_settings)\n",
    "        print(\"=====1=======\")\n",
    "        \n",
    "        try:\n",
    "            x1 = gov_serach_crawler(\"tenderDeclaration\",proxies,db_settings, headers = headers,start_page = int(start_page[0]),day_before=day_before)\n",
    "            send_db(x1,\"tenderDeclaration\", db_settings)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "            detail = ex.args[0] #取得詳細內容\n",
    "            cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "            lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "            fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "            lineNum = lastCallStack[1] #取得發生的行號\n",
    "            funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "            errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "            print(errMsg)\n",
    "            conn = pymysql.connect(**db_settings)\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", \"wrong\", \"tenderDeclaration\", errMsg))\n",
    "        print(\"=====2=======\")\n",
    "        try:\n",
    "            time.sleep(90)\n",
    "            x2 = gov_serach_crawler(\"searchAppeal\",proxies,db_settings, headers = headers,start_page = int(start_page[1]),day_before=day_before)\n",
    "            send_db(x2,\"searchAppeal\", db_settings)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "            detail = ex.args[0] #取得詳細內容\n",
    "            cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "            lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "            fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "            lineNum = lastCallStack[1] #取得發生的行號\n",
    "            funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "            errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "            print(errMsg)\n",
    "            conn = pymysql.connect(**db_settings)\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", \"wrong\", \"searchAppeal\", errMsg))\n",
    "        \n",
    "#         try:\n",
    "#             x4 = gov_serach_crawler(\"predict\",proxies,db_settings, headers = headers, account=account,password=password)\n",
    "#             send_db(x4,\"predict\", db_settings)\n",
    "#         except Exception as ex:\n",
    "#             print(ex)\n",
    "#             error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "#             detail = ex.args[0] #取得詳細內容\n",
    "#             cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "#             lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "#             fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "#             lineNum = lastCallStack[1] #取得發生的行號\n",
    "#             funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "#             errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "#             print(errMsg)\n",
    "#             conn = pymysql.connect(**db_settings)\n",
    "#             with conn.cursor() as cursor:\n",
    "#                 cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", \"wrong\", \"predict\", errMsg))\n",
    "        print(\"=====4=======\")\n",
    "        time_d = time.time()\n",
    "        time_c = time_d - time_start\n",
    "        print(\"！！！！！  thread_tenderDeclaration_searchAppeal 總執行時間 \"+str(time_c) +\" 秒\")\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", str(ip) + \" finish\", \"tenderDeclaration_searchAppeal\", \"\"))\n",
    "            \n",
    "    def do_work_final(start_page,day_before,db_settings,headers):\n",
    "        get_data_date = (datetime.date.today() - datetime.timedelta(days=day_before))\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", str(ip) + \" start\", \"final_publicRead_datesearch\", f\"爬取日期：{get_data_date}，final 從第 {start_page[3]} 頁開始爬，publicRead 從第 {start_page[2]} 頁開始爬，datesearch爬當日\"))\n",
    "        time_start = time.time()\n",
    "        proxies = get_ip_from_db(\"yes\",db_settings)\n",
    "        try:\n",
    "            x5 = gov_serach_crawler(\"final\",proxies,db_settings, headers = headers,start_page = int(start_page[3]),day_before=day_before)\n",
    "            send_db(x5,\"final\", db_settings)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "            detail = ex.args[0] #取得詳細內容\n",
    "            cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "            lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "            fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "            lineNum = lastCallStack[1] #取得發生的行號\n",
    "            funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "            errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "            print(errMsg)\n",
    "            conn = pymysql.connect(**db_settings)\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", \"wrong\", \"final\", errMsg))\n",
    "                \n",
    "        try:\n",
    "            x3 = gov_serach_crawler(\"publicRead\",proxies,db_settings, headers = headers,start_page = int(start_page[3]),day_before=day_before)\n",
    "            send_db(x3,\"publicRead\", db_settings)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "            detail = ex.args[0] #取得詳細內容\n",
    "            cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "            lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "            fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "            lineNum = lastCallStack[1] #取得發生的行號\n",
    "            funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "            errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "            print(errMsg)\n",
    "            conn = pymysql.connect(**db_settings)\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", \"wrong\", \"publicRead\", errMsg))\n",
    "\n",
    "        try:\n",
    "            s = rq.session()\n",
    "            xx = search_by_date(s,datetime.datetime.today()- datetime.timedelta(days=1), datetime.datetime.today(),\"all\", headers = headers)\n",
    "            print(xx)\n",
    "            xxx = date_search_crawler(s,xx,headers)\n",
    "            send_db_2(xxx,\"date_search_by_date\", db_settings)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "            detail = ex.args[0] #取得詳細內容\n",
    "            cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "            lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "            fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "            lineNum = lastCallStack[1] #取得發生的行號\n",
    "            funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "            errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "            print(errMsg)\n",
    "            conn = pymysql.connect(**db_settings)\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", \"wrong\", \"date_search_crawler\", str(detail)))\n",
    "      \n",
    "        time_d = time.time()\n",
    "        time_c = time_d - time_start\n",
    "        print(\"！！！！！  thread_final_datesearch_publicRead 總執行時間 \"+str(time_c) +\" 秒\")\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", str(ip) + \" finish\", \"final_publicRead_datesearch\", \"\"))\n",
    "    \n",
    "    #驗證用圖片資料夾\n",
    "    path = './temp_validate_img'\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree (path)\n",
    "        os.mkdir(path)\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "    #資料庫log檢查\n",
    "    ip = request.remote_addr\n",
    "    conn = pymysql.connect(**db_settings)\n",
    "    if_finish_command=\"SELECT status FROM log where task ='api_process_list' order by UID DESC\"\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(if_finish_command)\n",
    "        if cursor.fetchone() is None:\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", \"initial\", \"\", \"\"))\n",
    "        cursor.execute(if_finish_command)\n",
    "        if_finish = str(cursor.fetchone()[0])\n",
    "    \n",
    "    # 不判斷資料庫log強制重啟\n",
    "    if re.search(r'finish', if_finish, flags=0) or re.search(r'wrong', if_finish, flags=0) or if_finish == \"initial\":\n",
    "        pass\n",
    "    elif \"restart\" in request.args:\n",
    "        try:\n",
    "            if request.args.get(\"restart\")==1:\n",
    "                pass\n",
    "        except:\n",
    "            return \"若想要重啟爬蟲，參數要帶restart，值請放 1 \"\n",
    "    else:\n",
    "        return \"資料庫未收到結束log，可帶參數 restart=1 忽略資料庫log判斷\",500\n",
    "    #執行序之間任一ip被鎖block全部\n",
    "    global waitt\n",
    "    waitt = 0\n",
    "    \n",
    "    day_before=0\n",
    "    if 'day_before' in request.args:\n",
    "        try:\n",
    "            day_before = eval(request.args.get(\"day_before\"))\n",
    "        except:\n",
    "            return \"若想要設定爬取 N 天前的資料，參數要帶 day_before，以 int 的方式放值，ex: day_before=0 \"\n",
    "\n",
    "    \n",
    "    #設定起始頁面\n",
    "    start_page=[1,1,1,1]\n",
    "    if 'start_page' in request.args:\n",
    "        try:\n",
    "            start_page = eval(request.args.get(\"start_page\"))\n",
    "            if len(start_page) !=4:\n",
    "                return \"start_page 參數長度不足 4 (須設定 4 個：tenderDeclaration、searchAppeal、publicRead、final)，無須爬取的項目可輸入1999！\"\n",
    "            else:\n",
    "                print(\"頁數參數接收成功\")\n",
    "        except:\n",
    "            return \"若想要設定爬取的初始頁數，參數要帶 start_page，以 list 的方式放值=>[招標公告頁數,公開徵求頁數,公開閱覽頁數,決標公告頁數]，ex: start_page=[1,1,1,1] \"\n",
    "    print(f\"tenderDeclaration 從第 {start_page[0]} 頁開始爬取\")\n",
    "    print(f\"searchAppeal 從第 {start_page[1]} 頁開始爬取\")\n",
    "    print(f\"publicRead 從第 {start_page[2]} 頁開始爬取\")\n",
    "    print(f\"final 從第 {start_page[3]} 頁開始爬取\")\n",
    "    \n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_tenderDeclaration_searchAppeal\") >= 0 :\n",
    "            wording0=\"thread_tenderDeclaration_searchAppeal 執行緒正在執行；\"\n",
    "        else:\n",
    "            thread_tenderDeclaration_searchAppeal = threading.Thread(target=do_work, kwargs={'start_page':start_page,'day_before':day_before,'db_settings': db_settings,'headers': headers},name=\"thread_tenderDeclaration_searchAppeal\")\n",
    "            thread_tenderDeclaration_searchAppeal.start()\n",
    "            wording0=\"啟動 thread_tenderDeclaration_searchAppeal 執行緒；\"\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_final_datesearch_publicRead\") >= 0 :\n",
    "            wording1=\"thread_final_datesearch_publicRead 執行緒正在執行\"\n",
    "        else:\n",
    "            thread_final_datesearch_publicRead = threading.Thread(target=do_work_final, kwargs={'start_page':start_page,'day_before':day_before,'db_settings': db_settings,'headers': headers},name=\"thread_final_datesearch_publicRead\")\n",
    "            thread_final_datesearch_publicRead.start()\n",
    "            wording1=\"啟動 thread_final_datesearch_publicRead 執行緒\"\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "\n",
    "    return wording0+wording1,200\n",
    "\n",
    "@app.route('/start_task_detail', methods=['GET'])\n",
    "def start_task_detail():\n",
    "    path = './temp_validate_img'\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree (path)\n",
    "        os.mkdir(path)\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "    ip = request.remote_addr\n",
    "\n",
    "    def do_work_detail(tenderType,spilt,diff_seconds_data,diff_seconds_conn,db_settings,headers,No_proxy,block_crawl_no):\n",
    "        if tenderType == \"final\":\n",
    "            global progress_final_detail\n",
    "            progress_final_detail =''\n",
    "        elif tenderType ==\"tenderDeclaration\":\n",
    "            global progress_tenderDeclaration_detail\n",
    "            progress_tenderDeclaration_detail =''\n",
    "        \n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        get_data=\"SELECT * FROM tender.gov_purchase where (detail_data_html ='' and detail_data like '%{}%') and error_code = 0  and tenderType='\"+tenderType+\"'\"\n",
    "        # 建立Cursor物件\n",
    "        with conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "          #資料表相關操作\n",
    "            cursor.execute(get_data)\n",
    "            raw_datas_all = cursor.fetchall()\n",
    "            print(f'{tenderType} 一共抓出 {len(raw_datas_all)} 筆還沒有內頁資料')\n",
    "            if spilt:\n",
    "                if spilt[1]>1:\n",
    "                    first_part = int(len(raw_datas_all)/spilt[0]*(spilt[1]-1))\n",
    "                    second_part = int(len(raw_datas_all)/spilt[0]*spilt[1])\n",
    "                    raw_datas_all = raw_datas_all[first_part:second_part]\n",
    "                    wording = f'{tenderType} 切分 {spilt[0]} 段，此為第 {spilt[1]} 段，取第 {first_part} 個到 {second_part} 個，共 {len(raw_datas_all)} 個，進行爬取'\n",
    "                else:\n",
    "                    wording = f'{tenderType} 不切分，共 {len(raw_datas_all)} 個，進行爬取'\n",
    "            elif len(raw_datas_all)>0:\n",
    "                wording = f'{tenderType} 不切分，共 {len(raw_datas_all)} 個，進行爬取'\n",
    "            else:\n",
    "                wording = f'{tenderType} 資料皆已爬取完成'\n",
    "            raw_datas=raw_datas_all[0:block_crawl_no]\n",
    "            print(wording)\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"api_process_detail\", str(ip) + \" start\", tenderType, wording))\n",
    "\n",
    "        try:\n",
    "            #限制嘗試抓取次數，避免死機\n",
    "            x=0\n",
    "            while len(raw_datas)>0:\n",
    "                \n",
    "                ll = get_only_detail(tenderType,raw_datas,diff_seconds_data,diff_seconds_conn,headers, db_settings,No_proxy=No_proxy)\n",
    "                # get_only_detail會回傳成功抓取的筆數 (ll)\n",
    "                if ll>=1:\n",
    "                    x = 0\n",
    "                else:\n",
    "                    x = x + 1 \n",
    "                    \n",
    "                #避免過長抓取失敗\n",
    "                del raw_datas_all[0:block_crawl_no]\n",
    "                if len(raw_datas_all)>=block_crawl_no:\n",
    "                    raw_datas=raw_datas_all[0:block_crawl_no]\n",
    "                else:\n",
    "                    raw_datas=raw_datas_all\n",
    "                    \n",
    "#                 conn = pymysql.connect(**db_settings)\n",
    "#                 # 建立Cursor物件\n",
    "#                 with conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "#                   #資料表相關操作\n",
    "#                     cursor.execute(get_data)\n",
    "#                     raw_datas = cursor.fetchall()\n",
    "#                 if len(raw_datas) ==0:\n",
    "#                     print('都抓完了！！！！')\n",
    "#                     break\n",
    "#                 else:\n",
    "                if x>=2:\n",
    "                    print(f'其他錯誤_8：{tenderType} 出現 {len(raw_datas)} 筆無法抓取內頁資料')\n",
    "                    break\n",
    "            conn = pymysql.connect(**db_settings)\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"api_process_detail\", str(ip)+\"finish\", tenderType, \"\"))\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "            detail = ex.args[0] #取得詳細內容\n",
    "            cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "            lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "            fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "            lineNum = lastCallStack[1] #取得發生的行號\n",
    "            funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "            errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "            print(errMsg)\n",
    "            conn = pymysql.connect(**db_settings)\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"api_process_detail\", \"wrong\", \"tenderDeclaration_final_detail\", errMsg))\n",
    "        \n",
    "        if tenderType=='tenderDeclaration':\n",
    "            del progress_tenderDeclaration_detail\n",
    "        elif tenderType=='final':\n",
    "            del progress_final_detail\n",
    "            \n",
    "            \n",
    "    #限制取得內頁資料的時間 (若成功抓取則等待)\n",
    "    diff_seconds_data = config_diff_seconds_data\n",
    "    #限制每一次嘗試取得內頁資料的時間 (每次嘗試連接都等待)\n",
    "    diff_seconds_conn = config_diff_seconds_conn\n",
    "    #設定用本機ip爬取內頁資料\n",
    "    No_proxy = config_No_proxy\n",
    "    #爬取幾筆就存進資料庫？\n",
    "    block_crawl_no = config_block_crawl_no\n",
    "    spilt = False\n",
    "    if request.args:\n",
    "        dd = request.args.to_dict()\n",
    "        print(dd)\n",
    "        if set(['spilt']) >= set(dd.keys()):\n",
    "            print(\"傳參成功\")\n",
    "        else:\n",
    "            return \"傳參失敗，參數只允許 spilt\",500\n",
    "        try:\n",
    "            #切分段數\n",
    "            spilt = dicMemberCheck('spilt',dd)\n",
    "            if spilt !=\"\" :\n",
    "                spilt=eval(spilt)\n",
    "                if len(spilt)==2 and spilt[0]>=spilt[1]:\n",
    "                    print(f\"spilt {spilt}\")\n",
    "            else:\n",
    "                spilt = False\n",
    "            print(f\"spilt {spilt}\")\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            return \"參數轉換錯誤\",500\n",
    "    \n",
    "    \n",
    "    if str(threading.enumerate()).find(\"thread_tenderDeclaration_detail\") >= 0 :\n",
    "        wording1=\"thread_tenderDeclaration_detail 執行緒正在執行\"\n",
    "    else:\n",
    "        thread_tenderDeclaration_detail = threading.Thread(target=do_work_detail, kwargs={'tenderType':'tenderDeclaration','diff_seconds_data':diff_seconds_data,'diff_seconds_conn':diff_seconds_conn,'headers':headers,'db_settings': db_settings,'No_proxy':No_proxy,'spilt':spilt,'block_crawl_no':block_crawl_no},name=\"thread_tenderDeclaration_detail\")\n",
    "        thread_tenderDeclaration_detail.start()\n",
    "        wording1=\"啟動 thread_tenderDeclaration_detail 執行緒\"\n",
    "        \n",
    "    if str(threading.enumerate()).find(\"thread_final_detail\") >= 0 :\n",
    "        wording2=\"thread_final_detail 執行緒正在執行\"\n",
    "    else:\n",
    "        thread_final_detail = threading.Thread(target=do_work_detail, kwargs={'tenderType':'final','diff_seconds_data':diff_seconds_data,'diff_seconds_conn':diff_seconds_conn,'headers':headers,'db_settings': db_settings,'No_proxy':No_proxy,'spilt':spilt,'block_crawl_no':block_crawl_no},name=\"thread_final_detail\")\n",
    "        thread_final_detail.start()\n",
    "        wording2=\"啟動 thread_final_detail 執行緒\"\n",
    "        \n",
    "    return wording1 +\" \"+ wording2,200\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/progress', methods=['GET'])\n",
    "def get_progress():\n",
    "    wording={}\n",
    "    try:\n",
    "        wording['tenderDeclaration'] = progress_tenderDeclaration\n",
    "    except:\n",
    "        wording['tenderDeclaration'] = \"tenderDeclaration 任務未啟動\"\n",
    "        \n",
    "    try:\n",
    "        wording['searchAppeal'] = progress_searchAppeal\n",
    "    except:\n",
    "        wording['searchAppeal'] = \"searchAppeal 任務未啟動\"\n",
    "        \n",
    "    try:\n",
    "        wording['publicRead'] = progress_publicRead\n",
    "    except:\n",
    "        wording['publicRead'] = \"publicRead 任務未啟動\"\n",
    "        \n",
    "    try:\n",
    "        wording['predict'] = progress_predict\n",
    "    except:\n",
    "        wording['predict'] = \"predict 任務未啟動\"\n",
    "    \n",
    "    try:\n",
    "        wording['final'] = progress_final\n",
    "    except:\n",
    "        wording['final'] = \"final 任務未啟動\"\n",
    "        \n",
    "    try:\n",
    "        wording['date_serach'] = progress_date_serach\n",
    "    except:\n",
    "        wording['date_serach'] = \"date_serach 任務未啟動\"\n",
    "\n",
    "    try:\n",
    "        wording['tenderDeclaration_detail'] = progress_tenderDeclaration_detail\n",
    "    except:\n",
    "        wording['tenderDeclaration_detail'] = \"tenderDeclaration_detail 任務未啟動\"\n",
    "        \n",
    "    try:\n",
    "        wording['final_detail'] = progress_final_detail\n",
    "    except:\n",
    "        wording['final'] = \"final_detail 任務未啟動\"\n",
    "        \n",
    "    return wording,200\n",
    "\n",
    "# @app.route('/test', methods=['GET'])\n",
    "# def tesddt():\n",
    "#     if request.args:\n",
    "#         dd= request.args.to_dict()\n",
    "#         if set(['start_date','end_date','']) >= set(dd.keys()):\n",
    "#             return \"傳參成功\",200\n",
    "#         else:\n",
    "#             return \"傳參失敗\",200\n",
    "\n",
    "\n",
    "@app.route('/date_search_crawler', methods=['GET'])\n",
    "def date_serach():\n",
    "    if request.args:\n",
    "        dd= request.args.to_dict()\n",
    "        if set(['start_date','end_date','if_publish']) >= set(dd.keys()):\n",
    "            print(\"傳參成功\")\n",
    "        else:\n",
    "            return \"傳參失敗，參數只允許\"+' start_date '+'end_date '+'if_publish',500\n",
    "    headers[\"User-Agent\"] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36'\n",
    "    def do_work_2(start_date,end_date,if_publish,db_settings,headers):\n",
    "        s = rq.session()\n",
    "        try:\n",
    "            xx = search_by_date(s,start_date, end_date,if_publish,headers)\n",
    "            print(xx)\n",
    "            xxx = date_search_crawler(s,xx,headers)\n",
    "            send_db_2(xxx,\"date_search_by_date\", db_settings)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "            detail = ex.args[0] #取得詳細內容\n",
    "            cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "            lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "            fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "            lineNum = lastCallStack[1] #取得發生的行號\n",
    "            funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "            errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "            print(errMsg)\n",
    "            conn = pymysql.connect(**db_settings)\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", \"wrong\", \"date_search_crawler\", error_class + detail + errMsg))\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", str(ip) + \" finish\", \"date_search_crawler\", \"\"))\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_start_date_search_crawler\") >=0:\n",
    "            return \"當前執行緒正在執行\",500\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        start_date = str(request.args.get(\"start_date\"))\n",
    "        start_date = datetime.datetime.strptime(start_date, \"%Y%m%d\")\n",
    "    except Exception as ex:    \n",
    "        return \"start_date 參數格式錯誤\",500\n",
    "    \n",
    "    try:\n",
    "        end_date = str(request.args.get(\"end_date\"))\n",
    "        end_date = datetime.datetime.strptime(end_date, \"%Y%m%d\")\n",
    "    except Exception as ex:    \n",
    "        return \"end_date 參數格式錯誤\",500\n",
    "    \n",
    "    try:\n",
    "        if_publish = str(request.args.get(\"if_publish\"))\n",
    "        if not (if_publish == \"yes\" or if_publish == \"no\" or if_publish == \"all\"):\n",
    "            return \"if_publish 參數格式錯誤\",500\n",
    "    except Exception as ex:\n",
    "        return \"if_publish 參數沒帶\",500\n",
    "        \n",
    "    \n",
    "    ip = request.remote_addr\n",
    "#     conn = pymysql.connect(**db_settings)\n",
    "#     with conn.cursor() as cursor:\n",
    "#         if_finish_command=\"SELECT status FROM log where task ='api_process_date_serach' order by UID DESC\"\n",
    "#         cursor.execute(if_finish_command)\n",
    "#         if cursor.fetchone() is None:\n",
    "#             cursor.execute(command_log, (datetime.datetime.now(), \"api_process_date_serach\", \"initial\", \"\", \"\"))\n",
    "#         cursor.execute(if_finish_command)\n",
    "#         if_finish = str(cursor.fetchone()[0])\n",
    "    \n",
    "#     print(if_finish)    \n",
    "#     if re.search(r'finish', if_finish, flags=0) or re.search(r'wrong', if_finish, flags=0) or if_finish == \"initial\":\n",
    "#         pass\n",
    "#     else:\n",
    "#         wording =\"資料庫未收到結束log\"\n",
    "#         status_code = 500\n",
    "#         return wording,status_code\n",
    "    \n",
    "    conn = pymysql.connect(**db_settings)\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", str(ip) + \" start\", \"date_search_crawler\", \"\"))\n",
    "    \n",
    "    thread_date_search_crawler = threading.Thread(target=do_work_2, kwargs={'start_date': start_date,'end_date': end_date,'if_publish':if_publish,'db_settings':db_settings,'headers':headers},name=\"thread_date_search_crawler\")\n",
    "    thread_date_search_crawler.start()    \n",
    "    return \"started\",200\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.debug = False\n",
    "    app.config['JSON_AS_ASCII'] = False\n",
    "    app.run(host=my_host, port=5000)\n",
    "    #serve(app, host=my_host, port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d151c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thread_list=[]\n",
    "# thread_list_index=[0]\n",
    "# thread = 4\n",
    "# for i in range(1,thread):\n",
    "#     thread_list_index.append(int(len(raw_datas)/thread*i))\n",
    "# thread_list_index.append(len(raw_datas))\n",
    "# print(thread_list_index)\n",
    "# ind=0\n",
    "# while ind<=thread:\n",
    "#     ind=ind+1\n",
    "#     thread_list.append(threading.Thread(target=do_work_detail, kwargs={'raw_datas':raw_datas[thread_list_index[ind]:thread_list_index[ind+1]],'diff_seconds_data':diff_seconds_data,'diff_seconds_conn':diff_seconds_conn,'headers':headers,'db_settings': db_settings},name=f\"thread_tenderDeclaration_final_detail_{thread_list_index[ind]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f96a6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests as rq\n",
    "# import ssl\n",
    "# s=rq.session()\n",
    "# URKK = \"https://web.pcc.gov.tw/pis/main/pis/client/index.do\"\n",
    "# URKK = \"https://www.youtube.com/watch?v=HMPVdcU9ZKg\"\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# headers = rq.utils.default_headers()\n",
    "# r = s.get(URKK,verify=False,timeout=(10,20))\n",
    "# r.close()\n",
    "# r.request.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5a77d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_settings = {\n",
    "    \"host\": \"60.250.109.71\",\n",
    "    \"port\": 23306,\n",
    "    \"user\": \"xuan\",\n",
    "    \"password\": \"Qaz123\",\n",
    "    \"db\": \"tender\",\n",
    "    \"charset\": \"utf8mb4\",\n",
    "    \"autocommit\":True\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "    \"Accept-Language\": \"zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6\", \n",
    "    \"Host\": \"web.pcc.gov.tw\",  #目標網站 \n",
    "    \"Sec-Fetch-Dest\": \"document\", \n",
    "    \"Sec-Fetch-Mode\": \"navigate\", \n",
    "    \"Sec-Fetch-Site\": \"same-origin\", \n",
    "    \"Upgrade-Insecure-Requests\": \"?1\", \n",
    "     #使用者代理\n",
    "}\n",
    "# global waitt\n",
    "# waitt =0\n",
    "# proxies = get_ip_from_db(\"yes\",db_settings)\n",
    "# x5 = gov_serach_crawler(\"searchAppeal\",proxies,db_settings, headers = headers)\n",
    "#send_db(x5,\"searchAppeal\", db_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01211f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_settings = {\n",
    "#     \"host\": \"60.250.109.71\",\n",
    "#     \"port\": 23306,\n",
    "#     \"user\": \"xuan\",\n",
    "#     \"password\": \"Qaz123\",\n",
    "#     \"db\": \"tender\",\n",
    "#     \"charset\": \"utf8mb4\",\n",
    "#     \"autocommit\":True\n",
    "# }\n",
    "\n",
    "# headers = {\n",
    "#     \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "#     \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "#     \"Accept-Language\": \"zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6\", \n",
    "#     \"Host\": \"web.pcc.gov.tw\",  #目標網站 \n",
    "#     \"Sec-Fetch-Dest\": \"document\", \n",
    "#     \"Sec-Fetch-Mode\": \"navigate\", \n",
    "#     \"Sec-Fetch-Site\": \"same-origin\", \n",
    "#     \"Upgrade-Insecure-Requests\": \"?1\", \n",
    "#      #使用者代理\n",
    "# }\n",
    "\n",
    "# time_start = time.time()\n",
    "# proxies = get_ip_from_db(\"yes\",db_settings)\n",
    "# try:\n",
    "#     x1 = gov_serach_crawler(\"tenderDeclaration\",proxies,db_settings, headers = headers)\n",
    "#     x2 = gov_serach_crawler(\"searchAppeal\",proxies,db_settings, headers = headers)\n",
    "#     x3 = gov_serach_crawler(\"publicRead\",proxies,db_settings, headers = headers)\n",
    "#     time_d = time.time()\n",
    "# except Exception as ex:\n",
    "#     print(ex)\n",
    "#     time_d = time.time()\n",
    "# time_c = time_d - time_start\n",
    "# print(time_c)\n",
    "# send_db(x1, db_settings)\n",
    "# send_db(x2, db_settings)\n",
    "# send_db(x3, db_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48a0db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = rq.session()\n",
    "# ss = datetime.datetime(2020, 1, 28)\n",
    "# e = datetime.datetime(2021, 6, 28)\n",
    "# xx = search_by_date(s,ss, e,'no')\n",
    "# xxx = date_serach_crawler(s,xx)\n",
    "\n",
    "# db_settings = {\n",
    "#     \"host\": \"60.250.109.71\",\n",
    "#     \"port\": 23306,\n",
    "#     \"user\": \"xuan\",\n",
    "#     \"password\": \"Qaz123\",\n",
    "#     \"db\": \"tender\",\n",
    "#     \"charset\": \"utf8mb4\",\n",
    "#     \"autocommit\":True\n",
    "# }\n",
    "# data_class=\"date_search_by_date\"\n",
    "# send_db_2(xxx,data_class, db_settings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

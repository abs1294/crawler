{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd2c3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\envs\\govpur\\lib\\site-packages\\cacert.pem\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as rq\n",
    "import re\n",
    "import datetime\n",
    "import pymysql\n",
    "import brotli\n",
    "from flask import Flask,jsonify, request\n",
    "from fake_useragent import UserAgent\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from urllib.parse import urlencode\n",
    "import threading\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import traceback\n",
    "import requests.packages.urllib3\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import base64\n",
    "os.environ['REQUESTS_CA_BUNDLE'] =  os.path.join(os.path.dirname(sys.argv[0]), 'cacert.pem')\n",
    "print(os.path.join(os.path.dirname(sys.argv[0]), 'cacert.pem'))\n",
    "import telnetlib\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f44854d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', '.ipynb_checkpoints', 'chromedriver.exe', 'gov_pur-NewSite-Copy1.ipynb', 'gov_pur-NewSite.ipynb', 'gov_pur.ipynb', 'gov_pur_pk.ipynb', 'gov_pur_settings.ini', 'gov_pur_settings_2.ini', 'potplayer.ipynb', 'ptt 爬蟲實戰.ipynb', 'teachable_api更換圖片.ipynb', 'teachable_selenium更換圖片.ipynb', 'temp_validate_image', 'temp_validate_img', 'thzu_crawler.ipynb', '公版', '圖像相似度.ipynb', '正則表達式練習.ipynb']\n",
      "---\n",
      "D:\\USER\\OneDrive\\jupyter notebook\\網路爬蟲\n",
      "---\n",
      "D:\\Program Files\\anaconda3\\envs\\govpur\\lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('.'))\n",
    "print(\"---\")\n",
    "print(os.getcwd())\n",
    "print(\"---\")\n",
    "print(os.path.dirname(sys.argv[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101d0f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "請輸入設定檔檔名(gov_pur_settings.ini)：gov_pur_settings_2.ini\n",
      "參數成功接收，嘗試連接 127.0.0.1:3306 tender_test 資料庫\n",
      "資料庫連線測試成功！(DB_VERSION：8.0.27)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # 建立 ConfigParser\n",
    "    config = configparser.ConfigParser()\n",
    "    read_file = input('請輸入設定檔檔名(gov_pur_settings.ini)：')\n",
    "    if read_file=='':\n",
    "        if 'gov_pur_settings.ini' in os.listdir('.'):\n",
    "            read_file='gov_pur_settings.ini'\n",
    "        else:\n",
    "            print(\"當前目錄未存在預設設定檔\")\n",
    "            raise\n",
    "    config.read(read_file)\n",
    "    config_db_settings = config['db_settings']    \n",
    "    db_settings = {\n",
    "        \"host\": config_db_settings['host'],\n",
    "        \"port\": config_db_settings.getint('port'),\n",
    "        \"user\": config_db_settings['user'],\n",
    "        \"password\": config_db_settings['password'],\n",
    "        \"db\": config_db_settings['db'],\n",
    "        \"charset\": config_db_settings['charset'],\n",
    "        \"autocommit\":config_db_settings.getboolean('autocommit')\n",
    "    }\n",
    "    \n",
    "    config_ip_time_use_interval = config['other_Settings']['ip_time_use_interval']\n",
    "    # 限制取得內頁資料的時間 (若成功抓取則等待)\n",
    "    config_diff_seconds_data = config['other_Settings'].getfloat('diff_seconds_data')\n",
    "    # 限制每一次嘗試取得內頁資料的時間 (每次嘗試連接都等待)\n",
    "    config_diff_seconds_conn = config['other_Settings'].getfloat('diff_seconds_conn')\n",
    "    # 設定是否用本機ip爬取內頁資料 (TRUE = 用本機ip)\n",
    "    config_No_proxy = config['other_Settings'].getboolean('No_proxy')\n",
    "    # 爬取幾筆就存進資料庫？\n",
    "    config_block_crawl_no = config['other_Settings'].getint('block_crawl_no')\n",
    "    # 請輸入主機ip ( 若要從本機啟動，可輸入127.0.0.1 )\n",
    "    config_my_host = config['other_Settings']['my_host']\n",
    "    # ip 允許 \"不成功連結\" 的時間\n",
    "    config_ip_time_notConnect = config['other_Settings'].getint('ip_time_notConnect')\n",
    "    # 創建一個資料夾存放驗證用撲克牌圖片\n",
    "    config_path = config['other_Settings']['path']\n",
    "    # 設定 ip qual_ratio\n",
    "    config_ip_qual_ratio = config['other_Settings']['ip_qual_ratio']\n",
    "    # 設定錯誤資料抓取期限\n",
    "    error_data_catch = config['other_Settings']['error_data_catch']\n",
    "    print(f\"參數成功接收，嘗試連接 {db_settings['host']}:{db_settings['port']} {db_settings['db']} 資料庫\")\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    print('參數填寫錯誤')\n",
    "    raise\n",
    "        \n",
    "try:\n",
    "    # 建立Connection物件\n",
    "    conn = pymysql.connect(**db_settings)\n",
    "    # 建立Cursor物件\n",
    "    with conn.cursor() as cursor:\n",
    "      #資料表相關操作\n",
    "        # % 操作符只能直接用於字串(‘123’)，列表([1,2,3])、元組\n",
    "        command = f\"SELECT VERSION()\"\n",
    "        # 執行\n",
    "        cursor.execute(command)\n",
    "        DB_VERSION = str(cursor.fetchone()[0])\n",
    "        print(F\"資料庫連線測試成功！(DB_VERSION：{DB_VERSION})\")\n",
    "except Exception as ex:\n",
    "    print(f\"資料庫連線測試失敗！，請檢查參數......{ex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14c88652",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "    \"Accept-Language\": \"zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6\", \n",
    "    \"Host\": \"web.pcc.gov.tw\",  #目標網站 \n",
    "    \"Connection\":\"close\",\n",
    "    \"Sec-Fetch-Dest\": \"document\", \n",
    "    \"Sec-Fetch-Mode\": \"navigate\", \n",
    "    \"Sec-Fetch-Site\": \"same-origin\", \n",
    "    \"Upgrade-Insecure-Requests\": \"?1\", \n",
    "    \"User-Agent\":UserAgent(use_cache_server=False).random\n",
    "     #使用者代理\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b15ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lxml pyinstaller編不動，nuitka不行\n",
    "#parser = \"lxml\"\n",
    "parser = \"html.parser\"\n",
    "parser_detail = 'html5lib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a61869e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#創建一個資料夾存放驗證用撲克牌圖片\n",
    "if os.path.exists(config_path):\n",
    "    shutil.rmtree (config_path)\n",
    "    os.mkdir(config_path)\n",
    "else:\n",
    "    os.mkdir(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f8be84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://web.pcc.gov.tw/tps/tpam/main/tps/tpam/tpam_tender_detail.do?searchMode=common&scope=F&primaryKey=53759591'\n",
    "# urll='https://icanhazip.com/'\n",
    "# s = rq.session()\n",
    "# r = s.get(urll, headers = headers,verify=False,proxies={'https':'http://170.155.5.235:8080'}, timeout=(4,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40471e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_ip(ip_time_notConnect,db_settings):\n",
    "    print(f\"ip_time_notConnect：{ip_time_notConnect}\")\n",
    "    try:\n",
    "        # 建立Connection物件\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        # 建立Cursor物件\n",
    "        with conn.cursor() as cursor:\n",
    "          #資料表相關操作\n",
    "            # % 操作符只能直接用於字串(‘123’)，列表([1,2,3])、元組\n",
    "            command = f\"DELETE FROM {db_settings['db']}.proxy_ip WHERE TIMESTAMPDIFF(HOUR, updated_time, now()) >= {ip_time_notConnect};\"\n",
    "            # 執行\n",
    "            try:\n",
    "                cursor.execute(command)\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"proxy_ip\", \"delete_wrong\", \"\", str(err)))\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02c33559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從免費代理網站取得代理ip\n",
    "def get_proxy_ip(isHttps,db_settings):\n",
    "    Now = datetime.datetime.now()\n",
    "    url = {\"usproxy\":\"https://www.us-proxy.org/\",\"freeproxylist_sslproxy\":\"https://www.sslproxies.org/\",\"freeproxylist_socksproxy\":\"https://www.socks-proxy.net/\"}\n",
    "    metas=[]\n",
    "    for i in url:\n",
    "        print(i)\n",
    "        r = rq.get(url[i],verify=False, timeout=(4,15))\n",
    "        html_doc = r.text\n",
    "        soup = BeautifulSoup(html_doc, parser)\n",
    "        trs = soup.select(\"table.table.table-striped.table-bordered tr\")  \n",
    "        #print(trs)\n",
    "        for tr in trs:\n",
    "            tds = tr.select(\"td\")\n",
    "            #print(tds)\n",
    "            #print(len(tds))\n",
    "            if len(tds) > 6:\n",
    "                ifScheme = tds[6].text.lower()\n",
    "                ip = tds[0].text\n",
    "                port = tds[1].text\n",
    "                # socks 代理會用到 version，sslproxy會抓錯，會抓成 anonymity\n",
    "                version = tds[4].text.lower()\n",
    "                proxy = \"%s:%s\"%(ip, port)\n",
    "                meta = {\n",
    "                    'proxyIp': proxy,\n",
    "                    'connect_times':0,\n",
    "                    'successful_connect_times':0,\n",
    "                    'qual_ratio':0,\n",
    "                    'isDelete':False,\n",
    "                    'fromm':i,\n",
    "                    'mine' : None,\n",
    "                    'isValidate':test_ip(ip, port),\n",
    "                    'updated_time':Now\n",
    "                  }\n",
    "                if ifScheme == isHttps:\n",
    "                    if \"sock\" in version:\n",
    "                        meta['ishttps'] = True\n",
    "                        meta['proxyIp'] = version + \"://\" + meta['proxyIp']\n",
    "                    elif ifScheme == \"yes\":\n",
    "                        meta['ishttps'] = True\n",
    "                        # https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#https-proxy-error-http-proxy\n",
    "                        meta['proxyIp'] = \"http://\" + meta['proxyIp']\n",
    "                    elif ifScheme == \"no\":\n",
    "                        meta['ishttps'] = False\n",
    "                        meta['proxyIp'] = \"http://\" + meta['proxyIp']\n",
    "                    \n",
    "                    #meta = test_proxy(meta,timeout_sec=3)\n",
    "                    metas.append(meta)\n",
    "                    continue                    \n",
    "    #------------------------------------\n",
    "    headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "    \"Accept-Language\": \"zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6\", \n",
    "    \"Host\": \"free-proxy.cz\",  #目標網站 \n",
    "    \"Sec-Fetch-Dest\": \"document\", \n",
    "    \"Sec-Fetch-Mode\": \"navigate\", \n",
    "    \"Sec-Fetch-Site\": \"same-origin\", \n",
    "    \"Upgrade-Insecure-Requests\": \"?1\", \n",
    "    \"User-Agent\":UserAgent(use_cache_server=False).random\n",
    "     #使用者代理\n",
    "}\n",
    "    \n",
    "    if isHttps ==\"yes\":\n",
    "        pp = 'https'\n",
    "    elif isHttps ==\"no\":\n",
    "        pp = 'http'\n",
    "    with rq.session() as s:\n",
    "        s.keep_alive = True\n",
    "        for j in range(1,4):\n",
    "            try:\n",
    "                r = s.get(f\"http://free-proxy.cz/zh/\",headers=headers,timeout=5)\n",
    "\n",
    "                headers['Content-Type']='application/x-www-form-urlencoded'\n",
    "                data=f\"country=all&protocol={pp}&anonymity=all&send=Filter+proxies\"\n",
    "                r = s.post(\"http://free-proxy.cz/en/?do=searchFilter-submit\",data=data,headers=headers,timeout=5)\n",
    "                #http://free-proxy.cz/en/proxylist/country/all/https/ping/all/2\n",
    "                if j >1:\n",
    "                    headers['Referer']=f'http://free-proxy.cz/en/proxylist/country/all/{pp}/ping/all'\n",
    "                    headers['Connection']=f'keep-alive'\n",
    "                    r = s.get(f\"http://free-proxy.cz/zh/proxylist/country/all/{pp}/ping/all/{j}\",headers=headers,timeout=5,verify=False)\n",
    "                print(\"free-proxy.cz抓取頁數： \"+str(j))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"free-proxy.cz 無法進入\")\n",
    "                break\n",
    "            print(r.url)\n",
    "            html_doc = r.text\n",
    "            soup = BeautifulSoup(html_doc, parser)\n",
    "            proxylist = soup.select('table#proxy_list tr')[1:]\n",
    "            print(len(proxylist))\n",
    "            for i in proxylist:\n",
    "                try:\n",
    "                    try:\n",
    "                        ip = base64.b64decode(re.search(r'Base64\\.decode\\(\\\"(.*)\\\"\\)',str(i.select('td')[0])).group(1)).decode(\"utf-8\")\n",
    "                    except AttributeError :\n",
    "                        continue\n",
    "                    port = i.select('td')[1].text\n",
    "                    Protocol  = i.select('td')[2].text.lower()\n",
    "                    place=i.select('td')[2].text.lower()\n",
    "                    meta = {\n",
    "                        'proxyIp': 'http://'+ str(ip) +':'+ str(port),\n",
    "                        'connect_times':0,\n",
    "                        'successful_connect_times':0,\n",
    "                        'qual_ratio':0,\n",
    "                        'isDelete':False,\n",
    "                        'fromm':'freeProxyCZ',\n",
    "                        'mine' : None,\n",
    "                        'isValidate':test_ip(ip, port),\n",
    "                        'updated_time':Now\n",
    "                      }\n",
    "                    if Protocol =='https' and isHttps ==\"yes\":\n",
    "                        meta['ishttps'] = True\n",
    "                        # 測了就抓不到下一頁了\n",
    "                        #meta = test_proxy(meta,timeout_sec=3)\n",
    "                        metas.append(meta)\n",
    "                    elif Protocol =='http' and isHttps ==\"no\":\n",
    "                        meta['ishttps'] = False\n",
    "                        #meta = test_proxy(meta,timeout_sec=3)\n",
    "                        metas.append(meta)\n",
    "                except:\n",
    "                    continue\n",
    "                if len(proxylist)<37:\n",
    "                    break\n",
    "    return metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e68bb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ip(ip,port):\n",
    "    try:\n",
    "        telnetlib.Telnet(ip, port, timeout=1)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "418ad0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 proxy 可用性\n",
    "def test_proxy(proxy,timeout_sec,headers={}):\n",
    "    #print(proxy['proxyIp'])\n",
    "    with rq.session() as s:\n",
    "        try:\n",
    "            if proxy['ishttps'] == True:\n",
    "                url = \"https://icanhazip.com/\"\n",
    "                r = s.get(url, headers = headers,proxies={'https':proxy['proxyIp'],},verify=False, timeout=timeout_sec)\n",
    "            elif proxy['ishttps'] == False:\n",
    "                url = \"http://icanhazip.com/\"\n",
    "                r = s.get(url, headers = headers,proxies={'http':proxy['proxyIp'],},verify=False, timeout=timeout_sec)\n",
    "            print(r.status_code)\n",
    "            print(proxy['proxyIp'])\n",
    "            if r.status_code==200:\n",
    "                proxy['isValidate']=True\n",
    "                proxy['successful_connect_times']=1\n",
    "                proxy['qual_ratio']=1\n",
    "            else:\n",
    "                proxy['isValidate']=False\n",
    "        except Exception as ex:\n",
    "            proxy['isValidate']=False\n",
    "            #print(ex)\n",
    "        #print(\"---------------\")\n",
    "    return proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8589419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將取得的代理ip寫入資料庫，此函數也可以更新已寫入資料庫的資料\n",
    "def send_ip(proxies_pool_https, db_settings,typee=\"\",send_log = True,new = False):\n",
    "    time_start = datetime.datetime.now() #開始計時\n",
    "    data_combine=[]\n",
    "    wrong = 0\n",
    "    try:\n",
    "        # 建立Connection物件\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        # 建立Cursor物件\n",
    "        with conn.cursor() as cursor:\n",
    "          #資料表相關操作\n",
    "            get_max_log_id=f\"select MAX(UID) FROM {db_settings['db']}.log\"\n",
    "            # % 操作符只能直接用於字串(‘123’)，列表([1,2,3])、元組\n",
    "            command = f\"INSERT INTO {db_settings['db']}.proxy_ip(log_UID, proxyIp, connect_times, successful_connect_times, qual_ratio, isValidate, ishttps,isDelete, fromm, updated_time)VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) on DUPLICATE KEY UPDATE log_UID = values(log_UID),connect_times =connect_times+values(connect_times),successful_connect_times=successful_connect_times+values(successful_connect_times),qual_ratio=((successful_connect_times+values(successful_connect_times))/(connect_times+values(connect_times))),isValidate = values(isValidate),ishttps=values(ishttps),isDelete=values(isDelete),updated_time=values(updated_time)\"\n",
    "            if new:\n",
    "                command = f\"INSERT INTO {db_settings['db']}.proxy_ip(log_UID, proxyIp, connect_times, successful_connect_times, qual_ratio, isValidate, ishttps,isDelete, fromm, updated_time)VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) on DUPLICATE KEY UPDATE log_UID = values(log_UID),connect_times = values(connect_times),successful_connect_times=values(successful_connect_times),qual_ratio=values(qual_ratio),isValidate = values(isValidate),ishttps=values(ishttps),isDelete=values(isDelete),updated_time=values(updated_time)\"\n",
    "            command_log = f\"INSERT INTO {db_settings['db']}.log(datetime, task, status, record_des, errmsg) VALUES(%s, %s, %s, %s, %s)\"\n",
    "            # 紀錄開始 (暫時取消)\n",
    "#             if send_log:\n",
    "#                 cursor.execute(command_log, (datetime.datetime.now(), \"proxy_ip\", \"start\", typee, \"\"))\n",
    "            # 取得 log 的 UID\n",
    "            cursor.execute(get_max_log_id)\n",
    "            log_UID = str(cursor.fetchone()[0])\n",
    "            # 組合數據\n",
    "            if type(proxies_pool_https) == list:\n",
    "                for proxy in proxies_pool_https:\n",
    "                    data_combine.append((log_UID, proxy[\"proxyIp\"], proxy[\"connect_times\"], proxy[\"successful_connect_times\"], proxy[\"qual_ratio\"], proxy[\"isValidate\"], proxy[\"ishttps\"], proxy[\"isDelete\"], proxy[\"fromm\"], proxy[\"updated_time\"]))  # 注意要用两个括号扩起来\n",
    "            else:\n",
    "                data_combine.append((log_UID, proxies_pool_https[\"proxyIp\"], proxies_pool_https[\"connect_times\"], proxies_pool_https[\"successful_connect_times\"], proxies_pool_https[\"qual_ratio\"], proxies_pool_https[\"isValidate\"], proxies_pool_https[\"ishttps\"], proxies_pool_https[\"isDelete\"], proxies_pool_https[\"fromm\"]))  # 注意要用两个括号扩起来\n",
    "            # 執行\n",
    "            try:\n",
    "                cursor.executemany(command, data_combine)\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "                wrong = wrong + 1\n",
    "                print(wrong)\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"proxy_ip\", \"wrong\" + str(wrong), \"\", str(err)))\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(data_combine)\n",
    "    time_end = datetime.datetime.now()    #結束計時\n",
    "    time_c= time_end - time_start   #執行所花時間\n",
    "    print('proxy ip寫入資料庫，time cost', time_c, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ac9a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_yes=get_proxy_ip(\"yes\",db_settings)\n",
    "# send_ip(p_yes, db_settings ,\"only https\",new = True)\n",
    "# x=get_ip_from_db(\"yes\",db_settings,config_ip_qual_ratio,config_ip_time_use_interval,False,connect_times=2,new=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfd11e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從資料庫取出ip，未來有自架proxy的話，可以把proxy放到資料庫內，並設定 mine = 1，這樣就會固定抓這個proxy來用，如果沒有自架proxy的話，就還需要多寫 proxy expire 的處置方式。\n",
    "def get_ip_from_db(isHttps,db_settings,ip_qual_ratio,ip_time_use_interval, isValidate, connect_times=1,new=True):\n",
    "    print(f'ip_qual_ratio：{ip_qual_ratio}')\n",
    "    print(f'ip_time_use_interval：{ip_time_use_interval}')\n",
    "    try:\n",
    "        # 建立Connection物件\n",
    "        data=[]\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        # 建立Cursor物件\n",
    "        with conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "          #資料表相關操作\n",
    "            if isHttps==\"yes\":\n",
    "                get_ip=f\"select * FROM {db_settings['db']}.proxy_ip where isHttps = 1 and isDelete is False and ((qual_ratio > {ip_qual_ratio} or connect_times < {connect_times}) or (mine = 1 and TIMESTAMPDIFF(second, updated_time, now())>{ip_time_use_interval}))\"\n",
    "            elif isHttps==\"no\":\n",
    "                get_ip=f\"select * FROM {db_settings['db']}.proxy_ip where isHttps = 0 and isDelete is False and ((qual_ratio > {ip_qual_ratio} or connect_times < {connect_times}) or (mine = 1 and TIMESTAMPDIFF(second, updated_time, now())>{ip_time_use_interval}))\"\n",
    "            if isValidate:\n",
    "                get_ip = get_ip + ' and isValidate=True'\n",
    "            cursor.execute(get_ip)\n",
    "            datas = cursor.fetchall()\n",
    "        if new:\n",
    "            for data in datas:\n",
    "                data['successful_connect_times']=0\n",
    "                data['connect_times']=0\n",
    "                data['qual_ratio']=0\n",
    "        return datas\n",
    "    except Exception as ex:\n",
    "        print(f'ip抓取失敗：{ex}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21f7c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#更新 mine proxy_ip 的可取用時間\n",
    "def update_mine_proxy_ip(proxy_ip,db_settings):\n",
    "    conn = pymysql.connect(**db_settings)\n",
    "    with conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "        update_proxy_ip = f\"update {db_settings['db']}.proxy_ip SET updated_time=now() where proxyIp = '{proxy_ip['proxyIp']}'\"\n",
    "        cursor.execute(update_proxy_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc4d9b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#撲克牌圖片驗證用算法\n",
    "def pHash(img):\n",
    "    # 感知哈希算法\n",
    "    # 缩放32*32\n",
    "    img = cv2.resize(img, (32, 32))   # , interpolation=cv2.INTER_CUBIC\n",
    " \n",
    "    # 转换为灰度图\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # 将灰度图转为浮点型，再进行dct变换\n",
    "    dct = cv2.dct(np.float32(gray))\n",
    "    # opencv实现的掩码操作\n",
    "    dct_roi = dct[0:8, 0:8]\n",
    " \n",
    "    hash = []\n",
    "    avreage = np.mean(dct_roi)\n",
    "    for i in range(dct_roi.shape[0]):\n",
    "        for j in range(dct_roi.shape[1]):\n",
    "            if dct_roi[i, j] > avreage:\n",
    "                hash.append(1)\n",
    "            else:\n",
    "                hash.append(0)\n",
    "    return hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ce8c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#撲克牌圖片驗證用算法 - 比較\n",
    "def cmpHash(hash1, hash2):\n",
    "    # Hash值对比\n",
    "    # 算法中1和0顺序组合起来的即是图片的指纹hash。顺序不固定，但是比较的时候必须是相同的顺序。\n",
    "    # 对比两幅图的指纹，计算汉明距离，即两个64位的hash值有多少是不一样的，不同的位数越小，图片越相似\n",
    "    # 汉明距离：一组二进制数据变成另一组数据所需要的步骤，可以衡量两图的差异，汉明距离越小，则相似度越高。汉明距离为0，即两张图片完全一样\n",
    "    n = 0\n",
    "    # hash长度不同则返回-1代表传参出错\n",
    "    if len(hash1) != len(hash2):\n",
    "        return -1\n",
    "    # 遍历判断\n",
    "    for i in range(len(hash1)):\n",
    "        # 不相等则n计数+1，n最终为相似度\n",
    "        if hash1[i] != hash2[i]:\n",
    "            n = n + 1\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3205f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#進行撲克牌驗證\n",
    "def validate_ip(s,url,headers,proxies,path):\n",
    "    response=\"\"\n",
    "    x=0\n",
    "    s.keep_alive = True\n",
    "    while x <= 5:\n",
    "        response = s.get(url, headers=headers, proxies=proxies)\n",
    "        html_doc = response.text\n",
    "        soup = BeautifulSoup(html_doc, parser)\n",
    "        idd= soup.select(\"form#validateForm input#id\")[0]['value']\n",
    "        csrf = soup.select(\"form#validateForm div input\")[-1]['value']\n",
    "        img_url_d = soup.select('form#validateForm img')\n",
    "        img_url_head = \"https://web.pcc.gov.tw\"\n",
    "        img_data = []\n",
    "        for j in img_url_d:\n",
    "            d = {'img_src':img_url_head +j['src'],'id':j['alt']}\n",
    "            img_data.append(d)\n",
    "        img_data[0]['id']='question'\n",
    "        right_hash_com =[]\n",
    "        left_hash_com =[]\n",
    "        \n",
    "        for k in img_data:\n",
    "            response = s.get(k['img_src'], headers=headers, proxies=proxies)\n",
    "            with open(path +'/'+ k['id']+'.png', 'wb') as file:\n",
    "                file.write(response.content)\n",
    "                file.close() \n",
    "            if k['id'] == 'question':\n",
    "                img = cv2.imread(path +'/'+\"question.png\")\n",
    "                crop_img = img[0:96, 6:77]\n",
    "                ret,crop_img = cv2.threshold(crop_img,127,255,cv2.THRESH_BINARY)\n",
    "                right_imageVar = pHash(crop_img)\n",
    "                crop_img = img[0:96, 89:160]\n",
    "                ret,crop_img = cv2.threshold(crop_img,127,255,cv2.THRESH_BINARY)\n",
    "                left_imageVar = pHash(crop_img)\n",
    "            else:\n",
    "                image = cv2.imread(path +'/'+k['id']+'.png')\n",
    "                ret,image = cv2.threshold(image,127,255,cv2.THRESH_BINARY)\n",
    "                right = cmpHash(pHash(image),right_imageVar)\n",
    "                left = cmpHash(pHash(image),left_imageVar)\n",
    "                right_hash_com.append(right)\n",
    "                left_hash_com.append(left)\n",
    "                \n",
    "        right_idx = np.argmin(right_hash_com)\n",
    "        left_idx = np.argmin(left_hash_com)\n",
    "        data=\"choose=\" +img_data[right_idx+1]['id']+\"&choose=\"+img_data[left_idx+1]['id']+\"&id=\"+str(idd)+\"&_csrf=\"+str(csrf)\n",
    "        headers['Content-Type']='application/x-www-form-urlencoded'\n",
    "        response = s.post(\"https://web.pcc.gov.tw/tps/validate/check\",headers = headers, proxies=proxies,data=data, timeout=(5,150))\n",
    "        \n",
    "        if re.search(\"為預防惡意程式針對本系統進行大量查詢\",response.text):\n",
    "            print(f\"再次檢核：{response.url}\")\n",
    "            url=response.url\n",
    "        elif not re.search(\"為預防惡意程式針對本系統進行大量查詢\",response.text) and response.status_code == 200:\n",
    "            print(\"通過驗證\")\n",
    "            #print(response.text)\n",
    "            break\n",
    "        else:\n",
    "            #print(response.text)\n",
    "            x=x+1\n",
    "            print(f\"驗證失敗，{response.status_code}\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8abbe5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#取出字典某 key 值對應 value，若找無此 key 值，則回傳空字串 \n",
    "def dicMemberCheck(key, dicObj):\n",
    "    if key in dicObj:\n",
    "        return dicObj[key]\n",
    "    else:\n",
    "        return \"\"\n",
    "# dicMemberCheck('標案案號', {'標案案號':123,'ji3':456})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "124fadb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# href = \"http://web.pcc.gov.tw/tps/main/pms/tps/atm/atmHistoryAction.do?method=review&searchMode=common&pkPmsMainHist=null&pkPmsMain=53735656&awardNoticeDateOrgn=111/03/16\"\n",
    "# r=rq.get(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d57803a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tenderType = 'tenderDeclaration'\n",
    "# conn = pymysql.connect(**db_settings)\n",
    "# get_data=\"SELECT * FROM gov_purchase where (detail_data_html ='' and detail_data like '%{}%') and tenderType='\"+tenderType+\"'\"\n",
    "# # 建立Cursor物件\n",
    "# with conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "#   #資料表相關操作\n",
    "#     cursor.execute(get_data)\n",
    "#     raw_datas_all = cursor.fetchall()\n",
    "# raw_datas = raw_datas_all[0:20]\n",
    "#get_only_detail(tenderType,raw_datas_all_3[99:],0,0,headers, db_settings,No_proxy=True,path=config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58102d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s=rq.session()\n",
    "# proxies=get_ip_from_db(\"yes\",db_settings,0.3)\n",
    "# refresh_proxies=[]\n",
    "# good=[]\n",
    "# ran=None\n",
    "# url = 'https://t2.pcc.gov.tw/prkms/urlSelector/common/atm?pk=NjAwMDUwNjA='\n",
    "# ll = get_only_detail(tenderType,raw_datas,0,0,headers, db_settings,No_proxy=True)\n",
    "# # get_only_detail會回傳成功抓取的筆數 (ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2127ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx=json.loads(raw_datas_all[0]['detail_data'])\n",
    "# raw_datas_all[0]['detail_data']=json.dumps(xx,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52c9b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# href=\"https://web.pcc.gov.tw/tps/tpam/main/tps/tpam/tpam_tender_detail.do?searchMode=common&scope=F&primaryKey=53789621\"\n",
    "# href2=\"https://www.youtube.com/watch?v=uV2rUMcGBYI\"\n",
    "# proxies=get_ip_from_db(\"yes\",db_settings,0.3)\n",
    "# proxies=proxies[:30]\n",
    "# good=[]\n",
    "# for i in proxies:\n",
    "#         try:\n",
    "#         r = rq.get(href, headers = headers,verify=False,proxies={'https':i['proxyIp']}, timeout=(6,15))\n",
    "#         print(str(i['proxyIp'])+\" !!!!!!!!!!!!!\")\n",
    "#         good.append(i)\n",
    "#     except:\n",
    "#         print(str(i['proxyIp'])+\" .............\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f25da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抓招標、決標內頁資料\n",
    "def get_tenderDeclaration_detail(s,href,ran,proxies,refresh_proxies,db_settings,headers,typee,path,diff_seconds_conn=1.2,No_proxy=False):\n",
    "    detail_data_html=\"\"\n",
    "    detail_data={}\n",
    "    global detail_waitt\n",
    "    detail_waitt = 0\n",
    "    #with requests.Session() as s:\n",
    "    headers[\"User-Agent\"] = UserAgent(use_cache_server=False).random\n",
    "    breakdown = \"no\"\n",
    "    time.sleep(diff_seconds_conn)\n",
    "    if No_proxy:\n",
    "        while detail_waitt:\n",
    "            localtime = time.localtime()\n",
    "            result = time.strftime(\"%Y%m%d_%H%M%S\", localtime)\n",
    "            print(typee + f\"暫停休息 {60*detail_waitt}秒 一下，有執行緒被鎖，目前時間：{result}\")\n",
    "            time.sleep(60*detail_waitt)\n",
    "        while True:\n",
    "            try:\n",
    "                r = s.get(href, headers = headers,verify=False, timeout=(4,15))\n",
    "                html_doc = r.text\n",
    "                soup = BeautifulSoup(html_doc, parser_detail)\n",
    "                if re.match(r'.*validate.*',r.url, flags=0):\n",
    "                    print(f\"本機 IP 準備執行驗證：{r.url}\")\n",
    "                    r = validate_ip(s,r.url,headers,{},path)\n",
    "                if r.status_code<300 and (re.search(r'公告日',html_doc) or re.search(r'標案案號',html_doc)):\n",
    "                    detail_waitt = 0\n",
    "                    break\n",
    "                elif re.search(r'系統錯誤訊息',html_doc):\n",
    "                    detail_waitt = 0\n",
    "                    error_section = soup.select(\"table.g_tb_01 tr\")\n",
    "                    for i in error_section:\n",
    "                        error_1 = i.select(\"td\")[0].text.strip()\n",
    "                        error_2 = i.select(\"td\")[1].text.strip()\n",
    "                        detail_data[error_1]=error_2\n",
    "                    breakdown = \"yes\"\n",
    "                    break\n",
    "            except rq.exceptions.RequestException as ex:\n",
    "                print(\"本機 ip 遭到封鎖\")\n",
    "                detail_waitt = detail_waitt + 1\n",
    "                localtime = time.localtime()\n",
    "                result = time.strftime(\"%Y%m%d_%H%M%S\", localtime)\n",
    "                print(typee + f\"暫停休息 {800*detail_waitt}秒 一下，本機 IP 被鎖，目前時間：{result}\")\n",
    "                print(ex)\n",
    "                time.sleep(800*detail_waitt)\n",
    "            except Exception as ex:\n",
    "                print(f'其他錯誤_1：{ex}')\n",
    "                break\n",
    "    else:\n",
    "        while True:\n",
    "            if not ran:\n",
    "                ran = np.random.randint(len(proxies))\n",
    "            print(f\"抽取第 {ran} 個，目前共有 {len(proxies)} 個 proxy，當前連接次數：{proxies[ran]['connect_times']}\")\n",
    "            try:\n",
    "                proxies[ran]['connect_times'] =  proxies[ran]['connect_times'] + 1\n",
    "                proxies[ran]['qual_ratio'] = proxies[ran]['successful_connect_times'] / proxies[ran]['connect_times']\n",
    "                #print(f\"{proxies[ran]['proxyIp']} 準備進入，{href}\")                 \n",
    "                r = s.get(href, headers = headers,verify=False,proxies={'https':proxies[ran]['proxyIp']}, timeout=(8,15))\n",
    "                html_doc = r.text\n",
    "                soup = BeautifulSoup(html_doc, parser_detail)\n",
    "                #print(\"內頁取得點存活\")\n",
    "                if re.match(r'.*validate.*',r.url, flags=0) or re.search(\"為預防惡意程式針對本系統進行大量查詢\",html_doc):\n",
    "                    print(f\"{proxies[ran]['proxyIp']} 準備執行驗證：{r.url}\")\n",
    "                    r = validate_ip(s,r.url,headers,{'https':proxies[ran]['proxyIp']},path)\n",
    "                    html_doc = r.text\n",
    "                    soup = BeautifulSoup(html_doc, parser_detail)\n",
    "                    \n",
    "                if re.search(r'公告日',html_doc) or (re.search(r'機關代碼',html_doc) and re.search(r'標案案號',html_doc)):\n",
    "                    proxies[ran]['successful_connect_times'] =  proxies[ran]['successful_connect_times'] + 1\n",
    "                    proxies[ran]['qual_ratio'] = proxies[ran]['successful_connect_times'] / proxies[ran]['connect_times']\n",
    "                    proxies[ran]['updated_time'] = datetime.datetime.now()\n",
    "                    #避免鎖 ip 的情況，所以限定每個 ip 爬取 50 筆資料\n",
    "                    if proxies[ran]['successful_connect_times']>=50:\n",
    "                        refresh_proxies.append(proxies.pop(ran))\n",
    "                        ran=None\n",
    "                        print(\"此 ip 已爬取 50 個\")\n",
    "                        if len(proxies) == 0:\n",
    "                            print(\"proxy 不夠用了，重新獲取0\")\n",
    "                    break\n",
    "                elif re.search(r'系統錯誤訊息',html_doc):\n",
    "                    error_section = soup.select(\"table.g_tb_01 tr\")\n",
    "                    for i in error_section:\n",
    "                        error_1 = i.select(\"td\")[0].text.strip()\n",
    "                        error_2 = i.select(\"td\")[1].text.strip()\n",
    "                        detail_data[error_1]=error_2\n",
    "                    breakdown = \"yes\"\n",
    "                    break\n",
    "                else:\n",
    "                    refresh_proxies.append(proxies.pop(ran))\n",
    "                    ran=None\n",
    "                    # 被chrome擋住 or validate_ip 執行太多次\n",
    "                    print(str(refresh_proxies[-1]['proxyIp']) + \" 內頁資料錯誤，還剩餘 \" + str(len(proxies)) +\" 個proxy\")\n",
    "                    \n",
    "                    localtime = time.localtime()\n",
    "                    result = time.strftime(\"%Y%m%d_%H%M%S\", localtime)\n",
    "                    with open(result+'.html', 'a+', encoding='utf-8') as file:\n",
    "                        file.write(str(html_doc))\n",
    "                        file.close()\n",
    "                        \n",
    "                    if len(proxies) == 0:\n",
    "                        breakdown = \"yes\"\n",
    "                        print(\"proxy 不夠用了，重新獲取\")\n",
    "                        break\n",
    "            except rq.exceptions.RequestException as ex:\n",
    "                if proxies[ran]['mine']==1:\n",
    "                    update_mine_proxy_ip(proxies[ran],db_settings)\n",
    "                #print(ex)\n",
    "                error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "                detail = ex.args[0] #取得詳細內容\n",
    "                cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "                lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "                fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "                lineNum = lastCallStack[1] #取得發生的行號\n",
    "                funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "                errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "                #print(errMsg)\n",
    "                refresh_proxies.append(proxies.pop(ran))\n",
    "                print(str(refresh_proxies[-1]['proxyIp']) + \" 連接失敗，還剩餘 \" + str(len(proxies)) +\" 個proxy\")\n",
    "                ran=None\n",
    "                if len(proxies) == 0:\n",
    "                    breakdown = \"yes\"\n",
    "                    print(\"proxy 不夠用了，重新獲取\")\n",
    "                    break\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "                detail = ex.args[0] #取得詳細內容\n",
    "                cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "                lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "                fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "                lineNum = lastCallStack[1] #取得發生的行號\n",
    "                funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "                errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "                print(\"其他錯誤_2：\" + errMsg)\n",
    "                ran=None\n",
    "    if breakdown == \"yes\":\n",
    "        return detail_data,detail_data_html,proxies,refresh_proxies,None\n",
    "\n",
    "    if typee==\"predict\":\n",
    "        #政府採購預告\n",
    "        detail_data_html = soup.select('div#printRange')[0]\n",
    "        for i in detail_data_html.select('table'):\n",
    "            for j in i.select('tr'):\n",
    "                try:\n",
    "                    col = j.select('th')[-1].text.strip()\n",
    "                except:\n",
    "                    col = j.select('td')[-2].text.strip()\n",
    "                val = j.select('td')[-1].text.strip()\n",
    "                detail_data[col]=val\n",
    "\n",
    "    elif typee ==\"final\":\n",
    "\n",
    "        # 決標公告：無法決標、撤銷公告\n",
    "        if soup.select('div#printArea.main div#hidden_message_id'):\n",
    "            detail_data_html = soup.select('div#printArea.main table')[0]\n",
    "            detail_data['really_final']=0\n",
    "            block_data = detail_data_html.select('tr')\n",
    "            if not block_data[0].text.strip():\n",
    "                del block_data[0]\n",
    "            if re.search(r'紅色字體表示此次更正公',block_data[0].text):\n",
    "                del block_data[0]\n",
    "            for j in block_data:\n",
    "                try:\n",
    "                    try:\n",
    "                        col = j.select('th')[-1].text.strip()\n",
    "                    except:\n",
    "                        col = j.select('td')[-2].text.strip()\n",
    "                    val = j.select('td')[-1].text.strip()\n",
    "                    detail_data[col]=val\n",
    "                except:\n",
    "                    #https://web.pcc.gov.tw/tps/main/pms/tps/atm/atmNonAwardAction.do?searchMode=common&method=nonAwardContentForPublic&pkAtmMain=53705535\n",
    "                    #https://web.pcc.gov.tw/tps/main/pms/tps/atm/atmNonAwardAction.do?searchMode=common&method=nonAwardContentForPublic&pkAtmMain=53705381\n",
    "                    print(\"其他錯誤_3\")\n",
    "                    print(r.url)\n",
    "                    print(j)\n",
    "                    pass\n",
    "                \n",
    "        else:\n",
    "            #決標公告：正常決標公告 (NewSite down)，未處理更正決標公告的部分\n",
    "            detail_data_html = soup.select('div#printRange')[0]\n",
    "            detail_data['really_final']=1\n",
    "            for idx_1,i in enumerate(detail_data_html.select('div#friendlyPrintRange table.tableShow')):\n",
    "                for idx_2,j in enumerate(i.select('tr')):\n",
    "                    if idx_2 == 0:\n",
    "                        class_name=j.select(\"td\")[0].text\n",
    "                        continue\n",
    "            #                 if idx == 0 and re.search(r'紅色字體表示此次更正公',block_data[0].text):\n",
    "            #                     del block_data[0]\n",
    "            #                 if len(block_data) == 0 :\n",
    "            #                     continue\n",
    "            #                 else:\n",
    "            #                     del block_data[0]\n",
    "                    script_tag = j.find('script')\n",
    "                    if script_tag:\n",
    "                        script_tag.decompose()\n",
    "                    try:\n",
    "                        col = j.select('td')[0].text.strip()\n",
    "                        val = j.select('td')[1].text.strip()\n",
    "                        detail_data[col]=val\n",
    "                    except:\n",
    "#                         print(f\"決標公告 內頁資料 解析失敗，出現在 {r.url} ，第{idx_1}個 table，第{idx_2}個 tr\")   \n",
    "#                         print(j)  \n",
    "                        pass\n",
    "                        \n",
    "    else:\n",
    "        #招標公告：電子競價公告 (NewSite down) 沒有公告日期...\n",
    "        if re.search(\"電子競價公告\",html_doc):\n",
    "            detail_data_html = soup.select('div.middle_1_cen_big_full')[0]\n",
    "            block_data = detail_data_html.select('table.tb_01')\n",
    "            for idx, j in enumerate(block_data[0].select('tr')):\n",
    "                if idx ==0:\n",
    "                    class_name=j.select(\"td\")[0].text\n",
    "                    continue\n",
    "                try:\n",
    "                    col = j.select('td')[0].text.strip()\n",
    "                    try:\n",
    "                        val = j.select('td')[1].text.strip()\n",
    "                    except:\n",
    "                        continue\n",
    "                    detail_data[col]=val\n",
    "                except:\n",
    "                    print(f\"招標電子競價公告 內頁資料 解析失敗，出現在 {r.url}，第{i}個 table，第{j}個 tr\")   \n",
    "        else:\n",
    "            #招標公告 (NewSite down)\n",
    "            try:\n",
    "                detail_data_html = soup.select('div#printRange')[0]\n",
    "            except:\n",
    "                print(f\"其他錯誤_10：招標公告抓取失敗。。。{soup}\")   \n",
    "                localtime = time.localtime()\n",
    "                result = time.strftime(\"%Y%m%d_%H%M%S\", localtime)\n",
    "                with open('soup_' + result+'.txt', 'a+', encoding='utf-8') as file:\n",
    "                    file.write(str(soup))\n",
    "                    file.close() \n",
    "                raise\n",
    "            #加上公告日期\n",
    "            publish_date = re.match(r'([\\s\\S]*)var targetDate = dateTransMinGo\\(\\\"(.*)\\\"\\);',html_doc).group(2)\n",
    "            publish_date = publish_date.replace(publish_date[:4],str(int(publish_date[:4])-1911),1)\n",
    "            detail_data_html = str(detail_data_html).replace('<div id=\"printRange\">',f'<div id=\"printRange\"><div style=\"text-align: center; width: 100%\">公告日：{publish_date}</div>',1)\n",
    "            detail_data_html = BeautifulSoup(detail_data_html, parser_detail)\n",
    "            for i in range(1,10):\n",
    "                block_data = detail_data_html.select('table.tb_0'+str(i))\n",
    "                if len(block_data) == 0 :\n",
    "                    continue\n",
    "\n",
    "                for idx, j in enumerate(block_data[0].select('tr')):\n",
    "                    if idx ==0:\n",
    "                        class_name=j.select(\"td\")[0].text\n",
    "                        continue\n",
    "                    try:\n",
    "                        col = j.select('td')[0].text.strip()\n",
    "                        try:\n",
    "                            val = j.select('td')[1].text.strip()\n",
    "                        except:\n",
    "                            continue\n",
    "                        detail_data[col]=val\n",
    "                    except:\n",
    "                        print(f\"招標公告 內頁資料 解析失敗，出現在 {r.url}，第{i}個 table，第{j}個 tr\")         \n",
    "                         \n",
    "    return detail_data,detail_data_html,proxies,refresh_proxies,ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbc87519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_detail(tenderType,raw_datas,diff_seconds_data,diff_seconds_conn,headers, db_settings,path,No_proxy=False):\n",
    "    global progress_final_detail\n",
    "    global progress_tenderDeclaration_detail\n",
    "    proxies = get_ip_from_db(\"yes\",db_settings,config_ip_qual_ratio,config_ip_time_use_interval,True)\n",
    "    if len(proxies)==0:\n",
    "        proxies = get_proxy_ip(\"yes\",db_settings)\n",
    "        send_ip(proxies, db_settings,new = True)\n",
    "        proxies = get_ip_from_db(\"yes\",db_settings,config_ip_qual_ratio,config_ip_time_use_interval,True)\n",
    "        \n",
    "    refresh_proxies=[]\n",
    "    longg = len(raw_datas)\n",
    "    loses=[]\n",
    "    x=0\n",
    "    ran=None\n",
    "    with rq.session() as s:\n",
    "        s.mount('https://', requests.adapters.HTTPAdapter(pool_connections=25, pool_maxsize=50))\n",
    "        s.mount('http://', requests.adapters.HTTPAdapter(pool_connections=20, pool_maxsize=40))\n",
    "        s.mount('https://', requests.adapters.HTTPAdapter(max_retries=1))\n",
    "        s.mount('http://', requests.adapters.HTTPAdapter(max_retries=1))\n",
    "        #s.keep_alive = False\n",
    "        for raw_data in raw_datas:\n",
    "            time_start = datetime.datetime.now() #開始計時\n",
    "            print(\"--------------------\")\n",
    "            try:\n",
    "                while True:\n",
    "                    if tenderType =='final' or tenderType =='tenderDeclaration':\n",
    "                        (detail_data,detail_data_html,proxies,refresh_proxies,ran) = get_tenderDeclaration_detail(s, raw_data['detail_connect'],ran,proxies,refresh_proxies,db_settings,headers,raw_data['tenderType'],path,diff_seconds_conn,No_proxy=No_proxy) \n",
    "                    elif tenderType =='searchAppeal':\n",
    "                        (detail_data,detail_data_html) = get_searchAppeal_detail(headers,raw_data['detail_connect'])\n",
    "                    str_detail_data_html=str(detail_data_html)\n",
    "                    \n",
    "                    # 先處理 proxy 的部分\n",
    "                    if len(proxies)==0:\n",
    "                        send_ip(refresh_proxies, db_settings,send_log=False)\n",
    "                        refresh_proxies=[]\n",
    "                        loses.append(1)\n",
    "                        #print(f\"失敗總次數：{len(loses)}，危險！\")\n",
    "                        proxies = get_ip_from_db(\"yes\",db_settings,config_ip_qual_ratio,config_ip_time_use_interval, False)\n",
    "                        print(f\"成功獲取 proxy：{len(proxies)} 個\")\n",
    "                        # 避免既有的proxy在qual_ratio的情況下突然失效，要讓這種proxy的qual_ratio降到不會被抓取的水平要很久，所以設置如果 7次都用光proxy就重爬proxy一次\n",
    "                        if len(proxies)==0 or len(loses)>7:\n",
    "                            print(f\"失敗總次數：{len(loses)}，強制重抓proxy！！！！！！！！！！\")\n",
    "                            loses=[]\n",
    "                            proxies = get_proxy_ip(\"yes\",db_settings)\n",
    "                            send_ip(proxies, db_settings,new = True)\n",
    "                            proxies = get_ip_from_db(\"yes\",db_settings,config_ip_qual_ratio,config_ip_time_use_interval,True)\n",
    "                        if len(detail_data)==0 or str_detail_data_html==\"\":\n",
    "                            continue\n",
    "                    \n",
    "                    # 再處理資料的正確性\n",
    "                    if len(detail_data)>0 and (re.search(raw_data['bid_no'],str_detail_data_html) or re.search(raw_data['proposer_name'],str_detail_data_html)):\n",
    "                        raw_data['detail_data'] = detail_data\n",
    "                        raw_data['detail_data_html'] = detail_data_html\n",
    "                        raw_data['error_code'] = 0\n",
    "                        print(raw_data['detail_data']['機關代碼'])\n",
    "                        \n",
    "                        time_end = datetime.datetime.now()    #結束計時\n",
    "                        diff=time_end-time_start\n",
    "                        if diff.seconds<diff_seconds_data:\n",
    "                            time.sleep(diff_seconds_data-diff.total_seconds())\n",
    "                        time_end = datetime.datetime.now()    #結束計時\n",
    "                        time_c= time_end - time_start   #執行所花時間\n",
    "                        if tenderType=='tenderDeclaration':\n",
    "                            progress_tenderDeclaration_detail = f'progress_tenderDeclaration_detail 目前是第 {x+1} 項，抓取該細項內頁花費 {time_c} 秒，進度：{(x+1)*100/longg}%，總共有 {longg} 項'\n",
    "                            print(progress_tenderDeclaration_detail)\n",
    "                        elif tenderType=='final':\n",
    "                            progress_final_detail = f'progress_final_detail 目前是第 {x+1} 項，抓取該細項內頁花費 {time_c} 秒，進度：{(x+1)*100/longg}%，總共有 {longg} 項'\n",
    "                            print(progress_final_detail)\n",
    "                        loses=[]\n",
    "                        x=x+1\n",
    "                        break\n",
    "                    elif \"錯誤代碼\" in str(detail_data.keys()) or \"錯誤訊息\" in str(detail_data.keys()):\n",
    "                        raw_data['error_code'] = 1\n",
    "                        raw_data['detail_data'] = detail_data\n",
    "                        time_end = datetime.datetime.now()#結束計時\n",
    "                        diff=time_end-time_start\n",
    "                        if diff.seconds<diff_seconds_data:\n",
    "                            time.sleep(diff_seconds_data-diff.total_seconds())\n",
    "                        time_end = datetime.datetime.now()    #結束計時\n",
    "                        time_c= time_end - time_start   #執行所花時間\n",
    "                        if tenderType=='tenderDeclaration':\n",
    "                            progress_tenderDeclaration_detail = f'progress_tenderDeclaration_detail 目前是第 {x+1} 項，抓取該細項內頁花費 {time_c} 秒，系統發生錯誤，進度：{(x+1)*100/longg}%，總共有 {longg} 項'\n",
    "                            print(progress_tenderDeclaration_detail)\n",
    "                        elif tenderType=='final':\n",
    "                            progress_final_detail = f'progress_final_detail 目前是第 {x+1} 項，抓取該細項內頁花費 {time_c} 秒，系統發生錯誤，進度：{(x+1)*100/longg}%，總共有 {longg} 項'\n",
    "                            print(progress_final_detail)\n",
    "                        loses=[]\n",
    "                        x=x+1\n",
    "                        break\n",
    "                    else:\n",
    "                        raw_data['error_code'] = 2\n",
    "                        loses=[]\n",
    "                        localtime = time.localtime()\n",
    "                        result = time.strftime(\"%Y%m%d_%H%M%S\", localtime)\n",
    "                        with open(str(raw_data['bid_no'])+'_'+result+'.txt', 'a+', encoding='utf-8') as file:\n",
    "                            file.write(str(raw_data))\n",
    "                            file.close()\n",
    "                            \n",
    "            except Exception as ex:\n",
    "                error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "                detail = ex.args[0] #取得詳細內容\n",
    "                cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "                lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "                fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "                lineNum = lastCallStack[1] #取得發生的行號\n",
    "                funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "                errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "                print(errMsg)\n",
    "                print(\"其他錯誤_6：\"+str(raw_data))\n",
    "                pass\n",
    "\n",
    "    if x>=1:\n",
    "        try:\n",
    "            send_ip(refresh_proxies, db_settings,send_log=False)\n",
    "            # 寫入要更新 detail 資料，所以 Not_update_detail=False\n",
    "            send_db(raw_datas,\"detail_data_\"+tenderType, db_settings,Not_update_detail=False)\n",
    "        except Exception as ex:\n",
    "            print(\"其他錯誤_7：招標決標寫入DB 失敗_1\")\n",
    "            localtime = time.localtime()\n",
    "            result = time.strftime(\"%Y%m%d_%H%M%S\", localtime)\n",
    "            with open('send_db_2_' + result+'.txt', 'a+', encoding='utf-8') as file:\n",
    "                file.write(str(raw_datas))\n",
    "                file.close() \n",
    "            with open('send_db_1_' + result+'.txt', 'a+', encoding='utf-8') as file:\n",
    "                file.write(str(ex))\n",
    "                file.close() \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c23d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抓公開徵求內頁資料\n",
    "def get_searchAppeal_detail(headers,detail_connect):\n",
    "    detail_data={}\n",
    "    detail_data_html=\"\"\n",
    "    x=0\n",
    "    while True:\n",
    "        try:\n",
    "            x=x+1\n",
    "            r = rq.get(detail_connect, headers = headers)\n",
    "            html_doc = r.text\n",
    "            soup = BeautifulSoup(html_doc, parser)\n",
    "            if r.status_code == 200 and re.search(r'公告日',html_doc):\n",
    "                x=0\n",
    "                break\n",
    "            elif re.search(r'系統錯誤訊息',html_doc):\n",
    "                detail_waitt = 0\n",
    "                error_section = soup.select(\"table.g_tb_01 tr\")\n",
    "                for i in error_section:\n",
    "                    error_1 = i.select(\"td\")[0].text.strip()\n",
    "                    error_2 = i.select(\"td\")[1].text.strip()\n",
    "                    detail_data[error_1]=error_2\n",
    "                return detail_data,detail_data_html\n",
    "            else:\n",
    "                x=0\n",
    "        except Exception as ex:\n",
    "            time.sleep(1201*x)\n",
    "            print(ex)\n",
    "            print(f\"get_searchAppeal_detail出錯，等待 {1201*x} 秒\")\n",
    "\n",
    "\n",
    "    if re.search(r'無符合此筆公開徵求廠商條件資料',html_doc):\n",
    "        print(\"其他錯誤_5：無符合此筆公開徵求廠商條件資料，可能公告已撤銷\")\n",
    "        detail_data_html = soup.select('center table')[0]\n",
    "        detail_data=\"\"\n",
    "        return detail_data,detail_data_html\n",
    "    try:\n",
    "        detail_data_html = soup.select('div#printRange')[0]\n",
    "    except:\n",
    "        print(\"----------------\")\n",
    "        print(soup)\n",
    "\n",
    "    for i in detail_data_html.select('tr'):\n",
    "        try:\n",
    "            col = i.select('th')[-1].text.strip()\n",
    "        except:\n",
    "            col = i.select('td')[0].text.strip()\n",
    "        val = i.select('td')[-1].text.strip()\n",
    "        detail_data[col]=val\n",
    "    \n",
    "    return detail_data,detail_data_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b142aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抓公開閱覽內頁資料\n",
    "def get_publicRead_detail(headers, detail_connect):\n",
    "    x=0\n",
    "    while True:\n",
    "        try:\n",
    "            x=x+1\n",
    "            r = rq.get(detail_connect, headers = headers)\n",
    "            if r.status_code == 200:\n",
    "                x=0\n",
    "                break\n",
    "        except Exception as ex:\n",
    "            time.sleep(1201*x)\n",
    "            print(ex)\n",
    "            print(f\"get_publicRead_detail出錯，等待 {1201*x} 秒\")\n",
    "    \n",
    "    html_doc = r.text\n",
    "    soup = BeautifulSoup(html_doc, parser)\n",
    "    detail_data_html = soup.select('div div.middle_1_cen_big')[0]\n",
    "    \n",
    "    # 為了顯示公告日期\n",
    "    for prev_sibling in detail_data_html.select(\"div#printRange\")[0].find_previous_siblings()[1:]:\n",
    "        prev_sibling.decompose()\n",
    "        \n",
    "    for next_sibling in detail_data_html.select(\"div#printRange\")[0].find_next_siblings():\n",
    "        next_sibling.decompose()\n",
    "    \n",
    "    #拿掉上邊框\n",
    "    detail_data_html = str(detail_data_html).replace(\"margin-top: 30px;\",\"\",1)\n",
    "    #拿掉表格邊距\n",
    "    detail_data_html = str(detail_data_html).replace(\"margin-top:10px\",\"width: 100%;\",1)\n",
    "    \n",
    "    detail_data_html = BeautifulSoup(detail_data_html, parser)\n",
    "    \n",
    "    detail_data={}\n",
    "    for i in detail_data_html.select('tr'):\n",
    "        col = i.select('td.tbg_1')[0].text.strip()\n",
    "        val = i.select('td.tbg_2')[0].text.strip()\n",
    "        detail_data[col]=val\n",
    "    return detail_data,detail_data_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4aeb1f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_date = (datetime.date.today()- datetime.timedelta(days=30)).strftime(\"%Y/%m/%d\")\n",
    "# current_date = str(int(current_date[0:4])-1911) + current_date[4:]\n",
    "# print(current_date)\n",
    "# s = rq.session()\n",
    "\n",
    "# url_first = \"https://web.pcc.gov.tw/tps/pss/tender.do?searchMode=common&searchType=advance\"\n",
    "# data={'method':'search','searchMethod':'true','searchTarget':'ATM','hid_1':1,'hid_2':1,'hid_3':1,'tenderStatus':'5,6,20,28,8,21,22,29,33,9,23','btnQuery':'查詢','awardAnnounceStartDate':current_date,'awardAnnounceEndDate':current_date}\n",
    "# r = s.post(url_first,data = data)\n",
    "\n",
    "# url = \"https://web.pcc.gov.tw/tps/pss/tender.do?searchMode=common&searchType=advance&searchTarget=ATM&method=search&isSpdt=&execLocationArea=&pageIndex=1\"\n",
    "# r = s.get(url)\n",
    "# soup = BeautifulSoup(r.text, parser)\n",
    "# data = soup.select(\"div#print_area table tr\")\n",
    "\n",
    "# for i in data[1:3]:\n",
    "#     #機關名稱\n",
    "#     proposer_name=i.select('td')[1].text.strip()\n",
    "#     print(proposer_name)\n",
    "#     #標案案號\n",
    "#     bid_no=re.match(r'(.*)\\r\\n\\t', i.select('td')[2].text.strip(), flags=0).group(1)\n",
    "#     print(bid_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "984f7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#招標決標資料塞進DB\n",
    "def send_db(raw_datas,data_class, db_settings,Not_update_detail=True):\n",
    "    errMsg=None\n",
    "    time_start = datetime.datetime.now() #開始計時\n",
    "    data_combine=[]\n",
    "    # 建立Connection物件\n",
    "    conn = pymysql.connect(**db_settings)\n",
    "    # 建立Cursor物件\n",
    "    with conn.cursor() as cursor:\n",
    "      #資料表相關操作\n",
    "        get_max_log_id=f\"select MAX(UID) FROM {db_settings['db']}.log\"\n",
    "        # % 操作符只能直接用於字串(‘123’)，列表([1,2,3])、元組\n",
    "        if Not_update_detail and (data_class =='tenderDeclaration' or data_class =='final' or data_class =='searchAppeal'):\n",
    "            command = f\"INSERT INTO {db_settings['db']}.gov_purchase(log_UID, proposer_name, bid_no, bid_name, times, typ, clas, date, enddate, budget, ischange, detail_connect, detail_data, detail_data_html, get_data_date, tenderType,error_code)VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) on DUPLICATE KEY UPDATE log_UID = values(log_UID),get_data_date=values(get_data_date),times =values(times),typ=values(typ),clas=values(clas),date = values(date),enddate=values(enddate),budget=values(budget),ischange=values(ischange),detail_connect=values(detail_connect)\"\n",
    "            db_class = \"api_process_list\"\n",
    "        else:\n",
    "            command = f\"INSERT INTO {db_settings['db']}.gov_purchase(log_UID, proposer_name, bid_no, bid_name, times, typ, clas, date, enddate, budget, ischange, detail_connect, detail_data, detail_data_html, get_data_date, tenderType,error_code)VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) on DUPLICATE KEY UPDATE log_UID = values(log_UID),get_data_date=values(get_data_date),times =values(times),typ=values(typ),clas=values(clas),date = values(date),enddate=values(enddate),budget=values(budget),ischange=values(ischange),detail_connect=values(detail_connect),detail_data=values(detail_data),detail_data_html=values(detail_data_html),error_code=values(error_code)\"\n",
    "            db_class = \"api_process_detail\"\n",
    "        command_log = f\"INSERT INTO {db_settings['db']}.log(datetime, task, status, record_des, errmsg) VALUES(%s, %s, %s, %s, %s)\"\n",
    "        # 紀錄開始 (暫時取消)\n",
    "#         cursor.execute(command_log, (datetime.datetime.now(), \"gov_purchase\", \"start\", data_class, \"\"))\n",
    "        # 取得 log 的 UID\n",
    "        cursor.execute(get_max_log_id)\n",
    "        log_UID = str(cursor.fetchone()[0])\n",
    "        # 空資料處理\n",
    "        if len(raw_datas)==0:\n",
    "            #cursor.execute(command_log, (datetime.datetime.now(), db_class, \"wrong\", data_class, \"無資料可爬取\"))\n",
    "            pass\n",
    "        elif type(raw_datas) == dict:\n",
    "            data_combine.append((int(log_UID), raw_datas[\"proposer_name\"], raw_datas[\"bid_no\"], raw_datas[\"bid_name\"], raw_datas[\"times\"], raw_datas[\"typ\"], raw_datas[\"clas\"], raw_datas[\"date\"], raw_datas[\"enddate\"], raw_datas[\"budget\"], raw_datas[\"ischange\"], raw_datas[\"detail_connect\"], json.dumps(raw_datas[\"detail_data\"],ensure_ascii=False), raw_datas[\"detail_data_html\"],raw_datas[\"get_data_date\"], raw_datas[\"tenderType\"],raw_datas[\"error_code\"]))\n",
    "        # 組合數據\n",
    "        else:\n",
    "            #抓取到的資料是以公告日期最新到最舊，這樣寫進資料庫時最新的資料會被舊的資料覆蓋住，所以加一個reversed，讓最舊的先進去\n",
    "            for raw_data in reversed(raw_datas):\n",
    "                data_combine.append((int(log_UID), raw_data[\"proposer_name\"], raw_data[\"bid_no\"], raw_data[\"bid_name\"], raw_data[\"times\"], raw_data[\"typ\"], raw_data[\"clas\"], raw_data[\"date\"], raw_data[\"enddate\"], raw_data[\"budget\"], raw_data[\"ischange\"], raw_data[\"detail_connect\"], json.dumps(raw_data[\"detail_data\"],ensure_ascii=False), raw_data[\"detail_data_html\"],raw_data[\"get_data_date\"], raw_data[\"tenderType\"],raw_data[\"error_code\"]))  # 注意要用两个括号扩起来\n",
    "\n",
    "            # 執行\n",
    "            try:\n",
    "                cursor.executemany(command, data_combine)\n",
    "                time_end = datetime.datetime.now()    #結束計時\n",
    "                time_c= time_end - time_start   #執行所花時間\n",
    "                print('招標決標資料塞進DB，time cost', time_c, 's')\n",
    "            except Exception as ex:\n",
    "                error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "                detail = ex.args[0] #取得詳細內容\n",
    "                cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "                lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "                fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "                lineNum = lastCallStack[1] #取得發生的行號\n",
    "                funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "                errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "                print(\"send_db\" + errMsg)\n",
    "                raise\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"fail\", \"wrong\", data_class, str(error_class)+ \"  \" + str(detail) + errMsg))\n",
    "        # 紀錄結束\n",
    "        if not errMsg:\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), db_class, \"success\", data_class, str(len(raw_datas)) + \" 筆資料已完成\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe514f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter   #引入Counter\n",
    "# a = ll\n",
    "# b = dict(Counter(a))\n",
    "# print ([key for key,value in b.items()if value > 1])  #只展示重複元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bc037ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global waitt\n",
    "# waitt =0\n",
    "# x5 = gov_serach_crawler(\"searchAppeal\", headers = headers,day_before=0)\n",
    "# send_db(x5,\"searchAppeal\", db_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc11c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tenderDeclaration\n",
    "#searchAppeal\n",
    "#publicRead\n",
    "#final\n",
    "# global waitt\n",
    "# waitt =0\n",
    "# for i in range(0,1):\n",
    "#     x5 = gov_serach_crawler(\"final\", headers = headers,day_before=2)\n",
    "#     send_db(x5,\"final\", db_settings)\n",
    "#     x5 = gov_serach_crawler(\"tenderDeclaration\", headers = headers,day_before=i)\n",
    "#     send_db(x5,\"tenderDeclaration\", db_settings)\n",
    "#     x5 = gov_serach_crawler(\"searchAppeal\", headers = headers,day_before=i)\n",
    "#     send_db(x5,\"searchAppeal\", db_settings)\n",
    "#     x5 = gov_serach_crawler(\"publicRead\", headers = headers,day_before=i)\n",
    "#     send_db(x5,\"publicRead\", db_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "698ee573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = rq.session()\n",
    "# ii=1\n",
    "# s.keep_alive = True\n",
    "# headers[\"User-Agent\"] = UserAgent(use_cache_server=False).random\n",
    "# count_per_page = 10\n",
    "# get_data_date = (datetime.date.today() - datetime.timedelta(days=0))\n",
    "# current_date = get_data_date.strftime(\"%Y/%m/%d\")\n",
    "# current_date = str(int(current_date[0:4])) + current_date[4:]\n",
    "# url_first_init = 'https://t2.pcc.gov.tw/prkms/tpAppeal/common/readTpAppeal/basic/returnToBasic'\n",
    "# data={'pageSize':str(count_per_page),'firstSearch':'true','searchType':'basic','level_1':'on','tenderType':'SEARCH_APPEAL','dateType':'isDate','endDate':current_date,'startDate':current_date}\n",
    "# data = urlencode(data)\n",
    "# url_first = f\"{url_first_init}?{data}\"\n",
    "# url = f\"{url_first}&d-4025577-p=\"\n",
    "# global progress_searchAppeal\n",
    "# r = s.get(url_first, headers = headers)\n",
    "# raw_datas=[]\n",
    "# r = s.get(url + str(1), headers = headers)\n",
    "# html_doc = r.text\n",
    "# if re.search(r'無符合條件資料',html_doc):\n",
    "#     print(\"無符合條件資料\")\n",
    "# soup = BeautifulSoup(html_doc, parser)\n",
    "        \n",
    "# data = soup.select('table#tpAppeal')\n",
    "# all_data = int(soup.select('div#displaytagBannerDiv.page span#pagebanner span')[0].text.strip())\n",
    "# xxxx = 0\n",
    "# print(data)\n",
    "# for i in data[0].select('tr'):\n",
    "#     xxxx = xxxx + 1\n",
    "#     progress_searchAppeal = \"這是第 \" + str(ii) +\" 頁，的第 \"+ str(xxxx) + \" 項，總共有 \" + str(all_data) + \" 項，目前進度是 \" + str(((ii-1)*count_per_page+xxxx)/all_data*100) + \" %。\"\n",
    "#     print(progress_searchAppeal)\n",
    "#     # 取得資料日期\n",
    "\n",
    "#     # 項次\n",
    "#     no=i.select('td')[0].text.strip()\n",
    "#     # 機關名稱\n",
    "#     proposer_name=i.select('td')[1].text.strip()\n",
    "#     # 標案案號\n",
    "#     bid_no=re.match(r'<script>var hw = Geps3.CNS.pageCode2Img\\(\\\"(.*)\\\"\\);',str(i.select('td')[3].select('span script')[0]), flags=0).group(1)\n",
    "#     # 是否更正\n",
    "#     ischange=0  \n",
    "#     # 標案名稱    \n",
    "#     bid_name=i.select('td')[3].text.strip()\n",
    "#     # 傳輸次數 (公告次數)\n",
    "#     times=i.select('td')[4].text.strip()\n",
    "#     # 招標方式\n",
    "#     typ=\"\"\n",
    "#     # 採購性質\n",
    "#     clas=\"\"\n",
    "#     # 公告日期\n",
    "#     date=i.select('td')[5].text.strip()[0:9]\n",
    "#     # 截止投標\n",
    "#     enddate=i.select('td')[5].text.strip()[-9:]\n",
    "#     # 預算金額\n",
    "#     budget=\"\"\n",
    "#     # 內文連結\n",
    "#     detail_connect = \"https://t2.pcc.gov.tw\" + i.select('td')[6].select('a')[0]['href']\n",
    "#     (detail_data,detail_data_html) = get_searchAppeal_detail(headers,detail_connect)\n",
    "#     raw_datas.append({'no': no,'proposer_name': proposer_name,'bid_no': bid_no,'bid_name': bid_name,'times': times,'typ': typ,'clas': clas,'date': date,'enddate': enddate,'budget': budget,'ischange': ischange,'detail_data':detail_data, 'detail_connect':detail_connect,'detail_data_html':detail_data_html,'get_data_date':get_data_date,'tenderType':\"searchAppeal\",'error_code':0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11fcd964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gov_serach_crawler(url_type,headers,start_page=1,day_before=0):\n",
    "    global waitt\n",
    "    get_data_date = (datetime.date.today() - datetime.timedelta(days=day_before))\n",
    "    current_date = get_data_date.strftime(\"%Y/%m/%d\")\n",
    "    current_date = str(int(current_date[0:4])) + current_date[4:]\n",
    "    # 取得資料日期\n",
    "    print(\"！！！！！爬取日期：\"+current_date)\n",
    "    s = rq.session()\n",
    "    s.keep_alive = True\n",
    "    headers[\"User-Agent\"] = UserAgent(use_cache_server=False).random\n",
    "    \n",
    "    count_per_page = 100\n",
    "    host = 'web.pcc.gov.tw'\n",
    "    headers[\"Host\"]=host\n",
    "    domain = 'https://' + host\n",
    "    \n",
    "    if url_type == \"tenderDeclaration\":\n",
    "        url_first_init = f'{domain}/prkms/tender/common/basic/readTenderBasic'\n",
    "        data={'pageSize':str(count_per_page),'firstSearch':'true','searchType':'basic','level_1':'on','tenderType':'TENDER_DECLARATION','tenderWay':'TENDER_WAY_ALL_DECLARATION','dateType':'isDate','tenderEndDate':current_date,'tenderStartDate':current_date}\n",
    "        data = urlencode(data)\n",
    "        url_first = f\"{url_first_init}?{data}\"\n",
    "        url = f\"{url_first}&d-49738-p=\"\n",
    "        global progress_tenderDeclaration\n",
    "    elif url_type == \"searchAppeal\":  \n",
    "        url_first_init = f'{domain}/prkms/tpAppeal/common/readTpAppeal/basic/returnToBasic'\n",
    "        data={'pageSize':str(count_per_page),'firstSearch':'true','searchType':'basic','level_1':'on','tenderType':'SEARCH_APPEAL','dateType':'isDate','endDate':current_date,'startDate':current_date}\n",
    "        data = urlencode(data)\n",
    "        url_first = f\"{url_first_init}?{data}\"\n",
    "        url = f\"{url_first}&d-4025577-p=\"\n",
    "        global progress_searchAppeal\n",
    "    elif url_type == \"publicRead\":\n",
    "        url_first_init = f'{domain}/prkms/tpRead/common/readTpRead/basic/returnToBasic'\n",
    "        data={'pageSize':str(count_per_page),'firstSearch':'true','searchType':'basic','level_1':'on','tenderType':'PUBLIC_READ','dateType':'isDate','queryEndDate':current_date,'queryStartDate':current_date}\n",
    "        data = urlencode(data)\n",
    "        url_first = f\"{url_first_init}?{data}\"\n",
    "        url = f\"{url_first}&d-447556-p=\"\n",
    "        global progress_publicRead\n",
    "    elif url_type == \"predict\":\n",
    "        url_first_init = f'{domain}/prkms/gpaPredict/common/readGpaPredict/basic/returnToBasic'\n",
    "        data={'pageSize':str(count_per_page),'firstSearch':'true','searchType':'basic','level_1':'on','tenderType':'PREDICT','dateType':'isDate','predictNoticeDateEnd':current_date,'predictNoticeDateStart':current_date}\n",
    "        data = urlencode(data)\n",
    "        url_first = f\"{url_first_init}?{data}\"\n",
    "        url = f\"{url_first}&d-49738-p=\"\n",
    "        global progress_predict\n",
    "    elif url_type == \"final\":  \n",
    "        url_first_init = f'{domain}/prkms/tender/common/agent/readTenderAgent'\n",
    "        data={'pageSize':str(count_per_page),'firstSearch':'false','tenderStatus':'TENDER_STATUS_1','tenderWay':'TENDER_WAY_ALL_DECLARATION','tenderRange':'TENDER_RANGE_ALL','awardAnnounceEndDate':current_date,'awardAnnounceStartDate':current_date}\n",
    "        data = urlencode(data)\n",
    "        url_first = f\"{url_first_init}?{data}\"\n",
    "        url = f\"{url_first}&d-16396-p=\"\n",
    "        global progress_final\n",
    "    else:\n",
    "        raise Exception(\"url_type 輸入錯誤\") \n",
    "\n",
    "    while waitt:\n",
    "            print(url_type + f\"暫停休息 {60*waitt}秒 一下，有執行緒被鎖\")\n",
    "            time.sleep(60*waitt)\n",
    "            \n",
    "    while True:\n",
    "        try:\n",
    "            r = s.get(url_first, headers = headers)\n",
    "            if r.status_code<300:\n",
    "                waitt = 0\n",
    "                break\n",
    "        except Exception as ex: \n",
    "            waitt = waitt + 1\n",
    "            print(url_type + f\"暫停休息 {1200*waitt}秒 一下，本機 IP 被鎖\")\n",
    "            print(ex)\n",
    "            time.sleep(1200*waitt)\n",
    "\n",
    "\n",
    "    print(\"外頁進入點存活\")  \n",
    "    raw_datas=[]\n",
    "    for ii in range(start_page,2000):\n",
    "        while waitt:\n",
    "                print(url_type + f\"暫停休息 {60*waitt}秒 一下，有執行緒被鎖_phase2\")\n",
    "                time.sleep(60*waitt)\n",
    "        while True:\n",
    "            try:\n",
    "                r = s.get(url + str(ii), headers = headers)\n",
    "                print(\"外頁列表頁存活\")  \n",
    "                if r.status_code == 200:\n",
    "                    waitt = 0\n",
    "                    break\n",
    "            except Exception as ex: \n",
    "                print(url_type + f\"暫停休息 {1200*waitt}秒 一下，本機 IP 被鎖_phase2\")\n",
    "                print(ex)\n",
    "                waitt = waitt + 1\n",
    "                time.sleep(1200*waitt)\n",
    "        print(\"頁面前往：\"+str(r.url))\n",
    "        html_doc = r.text\n",
    "        if re.search(r'無符合條件資料',html_doc):\n",
    "            print(\"無符合條件資料\")\n",
    "            break\n",
    "        soup = BeautifulSoup(html_doc, parser)\n",
    "\n",
    "        time_start = datetime.datetime.now()\n",
    "        if url_type == \"tenderDeclaration\":\n",
    "            data = soup.select('div#printArea table#tpam')\n",
    "            thead_tag = data[0].find('thead')\n",
    "            if thead_tag:\n",
    "                thead_tag.decompose()\n",
    "            all_data = int(soup.select('div#displaytagBannerDiv.page span#pagebanner span')[0].text.strip().replace(',',''))\n",
    "            xxx = 0\n",
    "            for i in data[0].select('tr'):\n",
    "                t_s = datetime.datetime.now()\n",
    "                xxx = xxx + 1\n",
    "                progress_tenderDeclaration = \"這是第 \" + str(ii) + \" 頁，的第 \"+ str(xxx) + \" 項，總共有 \" + str(all_data) + \" 項，目前進度是 \" + str(((ii-1)*count_per_page+xxx)/all_data*100) + \" %。\"\n",
    "                print(progress_tenderDeclaration)\n",
    "                # 取得資料日期\n",
    "\n",
    "                # 項次\n",
    "                no=i.select('td')[0].text.strip()\n",
    "                # 機關名稱\n",
    "                proposer_name=i.select('td')[1].text.strip()\n",
    "                # 標案案號\n",
    "                bid_no=i.select('td')[2].text.replace('(更正公告)','').strip()\n",
    "                # 是否更正\n",
    "                if i.select('td')[2].select('font'):\n",
    "                    ischange=1\n",
    "                else:\n",
    "                    ischange=0  \n",
    "                # 標案名稱    \n",
    "                bid_name=re.match(r'.*\\(\\\"(<.*>)*(.*)\\\"\\);',str(i.select('td')[2].select('a script')[0]), flags=0).group(2)\n",
    "                # 傳輸次數\n",
    "                times=i.select('td')[3].text.strip()\n",
    "                # 招標方式\n",
    "                typ=i.select('td')[4].text.strip()\n",
    "                # 採購性質\n",
    "                clas=i.select('td')[5].text.strip()\n",
    "                # 公告日期\n",
    "                date=i.select('td')[6].text.strip()\n",
    "                # 截止投標\n",
    "                enddate=i.select('td')[7].text.strip()\n",
    "                # 預算金額\n",
    "                budget=i.select('td')[8].text.strip()\n",
    "                # 內文連結\n",
    "                detail_connect = domain + i.select('td')[2].select('a')[0]['href']\n",
    "                detail_data ={}\n",
    "                detail_data_html=\"\"\n",
    "                t_e = datetime.datetime.now()\n",
    "                time_se= t_e - t_s\n",
    "                print('此細項已抓取完成，總耗時：', time_se, 's')\n",
    "                print(\"------------------------------------\")\n",
    "                raw_datas.append({'no': no,'proposer_name': proposer_name,'bid_no': bid_no,'bid_name': bid_name,'times': times,'typ': typ,'clas': clas,'date': date,'enddate': enddate,'budget': budget,'ischange': ischange,'detail_connect':detail_connect,'detail_data':detail_data,'detail_data_html':detail_data_html,'get_data_date':get_data_date,'tenderType':\"tenderDeclaration\",'error_code':0})\n",
    "        elif url_type == \"searchAppeal\":\n",
    "            data = soup.select('table#tpAppeal')\n",
    "            thead_tag = data[0].find('thead')\n",
    "            if thead_tag:\n",
    "                thead_tag.decompose()\n",
    "            all_data = int(soup.select('div#displaytagBannerDiv.page span#pagebanner span')[0].text.strip().replace(',',''))\n",
    "            xxxx = 0\n",
    "            for i in data[0].select('tr'):\n",
    "                xxxx = xxxx + 1\n",
    "                progress_searchAppeal = \"這是第 \" + str(ii) +\" 頁，的第 \"+ str(xxxx) + \" 項，總共有 \" + str(all_data) + \" 項，目前進度是 \" + str(((ii-1)*count_per_page+xxxx)/all_data*100) + \" %。\"\n",
    "                print(progress_searchAppeal)\n",
    "                # 取得資料日期\n",
    "\n",
    "                # 項次\n",
    "                no=i.select('td')[0].text.strip()\n",
    "                # 機關名稱\n",
    "                proposer_name=i.select('td')[1].text.strip()\n",
    "                # 標案案號\n",
    "                bid_no=i.select('td')[2].text.replace('(更正公告)','').strip()\n",
    "                # 是否更正\n",
    "                ischange=0  \n",
    "                # 標案名稱    \n",
    "                bid_name=re.match(r'.*\\(\\\"(<.*>)*(.*)\\\"\\);',str(i.select('td')[3].select('span script')[0]), flags=0).group(2)\n",
    "                # 傳輸次數 (公告次數)\n",
    "                times=i.select('td')[4].text.strip()\n",
    "                # 招標方式\n",
    "                typ=\"\"\n",
    "                # 採購性質\n",
    "                clas=\"\"\n",
    "                # 公告日期\n",
    "                date=i.select('td')[5].text.strip()[0:9]\n",
    "                # 截止投標\n",
    "                enddate=i.select('td')[5].text.strip()[-9:]\n",
    "                # 預算金額\n",
    "                budget=\"\"\n",
    "                # 內文連結\n",
    "                detail_connect = domain + i.select('td')[6].select('a')[0]['href']\n",
    "                detail_data={}\n",
    "                detail_data_html=''\n",
    "                #(detail_data,detail_data_html) = get_searchAppeal_detail(headers,detail_connect)\n",
    "                raw_datas.append({'no': no,'proposer_name': proposer_name,'bid_no': bid_no,'bid_name': bid_name,'times': times,'typ': typ,'clas': clas,'date': date,'enddate': enddate,'budget': budget,'ischange': ischange,'detail_data':detail_data, 'detail_connect':detail_connect,'detail_data_html':detail_data_html,'get_data_date':get_data_date,'tenderType':\"searchAppeal\",'error_code':0})\n",
    "        elif url_type == \"publicRead\":\n",
    "            data = soup.select('table#tpRead')\n",
    "            thead_tag = data[0].find('thead')\n",
    "            if thead_tag:\n",
    "                thead_tag.decompose()\n",
    "            all_data = int(soup.select('div#displaytagBannerDiv.page span#pagebanner span')[0].text.strip().replace(',',''))\n",
    "            xxxxx=0\n",
    "            for i in data[0].select('tr'):\n",
    "                xxxxx = xxxxx + 1\n",
    "                progress_publicRead = \"這是第 \" + str(ii) +\" 頁，的第 \"+ str(xxxxx) + \" 項，總共有 \" + str(all_data) + \" 項，目前進度是 \" +str(((ii-1)*count_per_page+xxxxx)/all_data*100) + \" %。\"\n",
    "                print(progress_publicRead)\n",
    "                # 取得資料日期\n",
    "\n",
    "                # 項次\n",
    "                no=i.select('td')[0].text.strip()\n",
    "                # 機關名稱\n",
    "                proposer_name=i.select('td')[1].text.strip()\n",
    "                # 標案案號\n",
    "                bid_no=i.select('td')[2].text.replace('(更正公告)','').strip()\n",
    "                # 內文連結\n",
    "                detail_connect = domain + i.select('td')[6].select('a')[0]['href']\n",
    "                # 是否更正\n",
    "                ischange=0  \n",
    "                # 標案名稱    \n",
    "                bid_name=re.match(r'.*\\(\\\"(<.*>)*(.*)\\\"\\);',str(i.select('td')[3].select('span script')[0]), flags=0).group(2)\n",
    "                # 傳輸次數 (公告次數)\n",
    "                times=i.select('td')[4].text.strip()\n",
    "                # 招標方式\n",
    "                typ=\"\"\n",
    "                # 採購性質\n",
    "                clas=\"\"\n",
    "                # 公告日期\n",
    "                date=re.match(r'(.*)\\s*─\\s*(.*)', i.select('td')[5].text.strip(), flags=0).group(1).strip()\n",
    "                # 截止投標\n",
    "                enddate=re.match(r'(.*)\\s*─\\s*(.*)', i.select('td')[5].text.strip(), flags=0).group(2).strip()\n",
    "                # 預算金額\n",
    "                budget=\"\"\n",
    "                (detail_data,detail_data_html)=get_publicRead_detail(headers,detail_connect)\n",
    "                raw_datas.append({'no': no,'proposer_name': proposer_name,'bid_no': bid_no,'bid_name': bid_name,'times': times,'typ': typ,'clas': clas,'date': date,'enddate': enddate,'budget': budget,'ischange': ischange,'detail_connect':detail_connect,'detail_data':detail_data,'detail_data_html':detail_data_html,'get_data_date':get_data_date,'tenderType':\"publicRead\",'error_code':0})\n",
    "        elif url_type == \"predict\" : \n",
    "            data = soup.select('table#gpaPredict')\n",
    "            thead_tag = data[0].find('thead')\n",
    "            if thead_tag:\n",
    "                thead_tag.decompose()\n",
    "            all_data = int(soup.select('div#displaytagBannerDiv.page span#pagebanner span')[0].text.strip().replace(',',''))\n",
    "            xxxxx = 0\n",
    "            for i in data[0].select('tr'):\n",
    "                xxxxx = xxxxx + 1\n",
    "                progress_predict = \"這是第 \" + str(ii) +\" 頁，的第 \"+ str(xxxxx) + \" 項，總共有 \" + str(all_data) + \" 項，目前進度是 \" + str(((ii-1)*count_per_page+xxxxx)/all_data*100) + \" %。\"\n",
    "                print(progress_predict)\n",
    "                # 取得資料日期\n",
    "\n",
    "                # 項次\n",
    "                no=i.select('td')[0].text.strip()\n",
    "                # 機關名稱\n",
    "                proposer_name=i.select('td')[1].text.strip()\n",
    "                # 標案案號\n",
    "                bid_no=i.select('td')[1].text.replace('(更正公告)','').strip()\n",
    "                # 是否更正\n",
    "                ischange=0  \n",
    "                # 標案名稱    \n",
    "                bid_name=i.select('td')[2].select('a')[0].text.strip()\n",
    "                # 傳輸次數\n",
    "                times=\"\"\n",
    "                # 招標方式\n",
    "                typ=i.select('td')[4].text.strip()\n",
    "                # 採購性質\n",
    "                clas=\"\"\n",
    "                # 公告日期\n",
    "                date=i.select('td')[5].text.strip()\n",
    "                # 截止投標\n",
    "                enddate=\"\"\n",
    "                # 預算金額\n",
    "                budget=\"\"\n",
    "                # 內文連結\n",
    "                detail_connect = domain + i.select('td')[6].select('a')[0]['href'][5:]\n",
    "                print(detail_connect)\n",
    "                raw_datas.append({'no': no,'proposer_name': proposer_name,'bid_no': bid_no,'bid_name': bid_name,'times': times,'typ': typ,'clas': clas,'date': date,'enddate': enddate,'budget': budget,'ischange': ischange,'detail_connect':detail_connect,'detail_data':detail_data,'detail_data_html':detail_data_html,'get_data_date':get_data_date,'tenderType':\"predict\",'error_code':0})\n",
    "        elif url_type == \"final\" : \n",
    "            data = soup.select('table#atm')\n",
    "            thead_tag = data[0].find('thead')\n",
    "            if thead_tag:\n",
    "                thead_tag.decompose()\n",
    "            all_data = int(soup.select('div#displaytagBannerDiv.page span#pagebanner span')[0].text.strip().replace(',',''))\n",
    "            xxxxxx = 0\n",
    "            for i in data[0].select('tr'):\n",
    "                t_s = datetime.datetime.now()\n",
    "                xxxxxx = xxxxxx + 1\n",
    "                progress_final = \"這是第 \" + str(ii) +\" 頁，的第 \"+ str(xxxxxx) + \" 項，總共有 \" + str(all_data) + \" 項，目前進度是 \" + str(((ii-1)*count_per_page+xxxxxx)/all_data*100) + \" %。\"\n",
    "                print(progress_final)\n",
    "                # 取得資料日期\n",
    "\n",
    "                # 項次\n",
    "                no=i.select('td')[0].text.strip()\n",
    "                # 機關名稱\n",
    "                proposer_name=i.select('td')[1].text.strip()\n",
    "                # 標案案號\n",
    "                bid_no=i.select('td')[2].text.replace('(更正公告)','').strip()\n",
    "                # 是否更正\n",
    "                if i.select('td')[2].select('font'):\n",
    "                    ischange=1\n",
    "                else:\n",
    "                    ischange=0   \n",
    "                # 標案名稱    \n",
    "                bid_name=re.match(r'.*\\(\\\"(<.*>)*(.*)\\\"\\);',str(i.select('td')[2].select('a script')[0]), flags=0).group(2)\n",
    "                # 傳輸次數\n",
    "                times=i.select('td')[8].text.strip()\n",
    "                # 這樣可以從內頁資料抓傳輸次數： 上面有定義 dicMemberCheck 可以吐出字典裡對應key的value\n",
    "#                 if detail_data['ischange'] ==0 and not re.search(r'限制性招標',detail_data['招標方式']) and not re.search(r'選擇性招標',detail_data['招標方式']):\n",
    "#                     print(detail_data['新增公告傳輸次數'])\n",
    "                \n",
    "                # 招標方式\n",
    "                typ=i.select('td')[3].text.strip()\n",
    "                # 採購性質\n",
    "                clas=i.select('td')[4].text.strip()\n",
    "                # 公告日期\n",
    "                date=i.select('td')[5].text.strip()\n",
    "                # 截止投標\n",
    "                enddate=\"\"\n",
    "                # 預算金額\n",
    "                budget=i.select('td')[6].text.strip()\n",
    "                # 內文連結\n",
    "                detail_connect = domain + i.select('td')[2].select('a')[0]['href']\n",
    "                detail_data={}\n",
    "                detail_data_html=\"\"\n",
    "                t_e = datetime.datetime.now()\n",
    "                time_se= t_e - t_s\n",
    "                print('此細項已抓取完成，總耗時：', time_se, 's')\n",
    "                print(\"------------------------------------\")\n",
    "                raw_datas.append({'no': no,'proposer_name': proposer_name,'bid_no': bid_no,'bid_name': bid_name,'times': times,'typ': typ,'clas': clas,'date': date,'enddate': enddate,'budget': budget,'ischange': ischange,'detail_connect':detail_connect,'detail_data':detail_data,'detail_data_html':detail_data_html,'get_data_date':get_data_date,'tenderType':\"final\",'error_code':0})\n",
    "        time_end = datetime.datetime.now()\n",
    "        time_c= time_end - time_start\n",
    "        if ii*count_per_page >= all_data:\n",
    "            print(f'此分頁抓取總耗時：{time_c} s，已完成 {all_data} 筆，共有 {all_data} 筆')\n",
    "            if url_type == \"tenderDeclaration\":\n",
    "                del progress_tenderDeclaration\n",
    "            elif url_type == \"searchAppeal\":\n",
    "                del progress_searchAppeal\n",
    "            elif url_type == \"publicRead\":    \n",
    "                del progress_publicRead\n",
    "            elif url_type == \"predict\":    \n",
    "                del progress_predict\n",
    "            elif url_type == \"final\":    \n",
    "                del progress_final\n",
    "            break\n",
    "        else:\n",
    "            print(f'此分頁抓取總耗時：{time_c} s，已完成 {ii*count_per_page} 筆，共有 {all_data} 筆')\n",
    "    return raw_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "091d65b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_date(s,start_date, end_date,if_publish,headers):\n",
    "    r = s.get(\"https://web.pcc.gov.tw/prkms/prms-viewDailyTenderListClient.do?root=tps\",headers=headers)\n",
    "    html_doc = r.text\n",
    "    if r.status_code == 500 or re.search(r'The server encountered an internal error or misconfiguration and was unable to complete your request',html_doc):\n",
    "        raise Exception(\"打不進去header頁\") \n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(html_doc, parser)\n",
    "    publishs_href=[]\n",
    "    publishs_date=[]\n",
    "    not_publishs_href=[]\n",
    "    not_publishs_date=[]\n",
    "    left = soup.select('td#page table tr td:nth-child(2) li a')\n",
    "    right = soup.select('td#page table tr td:nth-child(4) li a')\n",
    "    for i in left:\n",
    "        publishs_href.append(i['href'])\n",
    "        y=str(int(re.match(r'(.*)年(.*)月(.*)日',i['title']).group(1))+1911)\n",
    "        m=re.match(r'(.*)年(.*)月(.*)日',i['title']).group(2)\n",
    "        d=re.match(r'(.*)年(.*)月(.*)日',i['title']).group(3)\n",
    "        ymd = datetime.datetime.strptime(\"-\".join([y,m,d]), \"%Y-%m-%d\")\n",
    "        publishs_date.append(ymd)\n",
    "    for i in right:\n",
    "        not_publishs_href.append(i['href'])\n",
    "        y=str(int(re.match(r'(.*)年(.*)月(.*)日',i['title']).group(1))+1911)\n",
    "        m=re.match(r'(.*)年(.*)月(.*)日',i['title']).group(2)\n",
    "        d=re.match(r'(.*)年(.*)月(.*)日',i['title']).group(3)\n",
    "        ymd = datetime.datetime.strptime(\"-\".join([y,m,d]), \"%Y-%m-%d\")\n",
    "        not_publishs_date.append(ymd)\n",
    "    for i in publishs_date:\n",
    "        publishs_start_date_index = publishs_date.index(i)\n",
    "        if i < start_date:\n",
    "            break\n",
    "    for i in not_publishs_date:\n",
    "        not_publishs_start_date_index = not_publishs_date.index(i)\n",
    "        if i < start_date:\n",
    "            break\n",
    "    for i in publishs_date:\n",
    "        publishs_end_date_index = publishs_date.index(i)\n",
    "        if i <= end_date:\n",
    "            break\n",
    "    for i in not_publishs_date:\n",
    "        not_publishs_end_date_index = not_publishs_date.index(i)\n",
    "        if i <= end_date:\n",
    "            break\n",
    "\n",
    "    if not_publishs_start_date_index<not_publishs_end_date_index:\n",
    "        raise Exception(\"開始日期不可大於結束日期\")\n",
    "    if publishs_start_date_index<publishs_end_date_index:\n",
    "        raise Exception(\"開始日期不可大於結束日期\")\n",
    "    if start_date > not_publishs_date[0] or start_date > publishs_date[0]:\n",
    "        raise Exception(\"無資料可爬，初始日期大於網站最新日期\")\n",
    "    \n",
    "    total_hrefs=[]\n",
    "    if if_publish==\"all\" or if_publish==\"yes\":\n",
    "        for i in range(0,len(publishs_href[publishs_end_date_index:publishs_start_date_index])):\n",
    "            total_href={'href':publishs_href[publishs_end_date_index+i],'date':publishs_date[publishs_end_date_index+i],'ispublish':1}\n",
    "            total_hrefs.append(total_href)\n",
    "    if if_publish==\"all\" or if_publish==\"no\":\n",
    "        for i in range(0,len(not_publishs_href[not_publishs_end_date_index:not_publishs_start_date_index])):\n",
    "            total_href={'href':not_publishs_href[not_publishs_end_date_index+i],'date':not_publishs_date[not_publishs_end_date_index+i],'ispublish':0}\n",
    "            total_hrefs.append(total_href)\n",
    "    return total_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adf35bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_serach_crawler(s,search_date,headers):\n",
    "    url_head = \"https://web.pcc.gov.tw/prkms/\"\n",
    "    len_search_date=len(search_date)\n",
    "    global progress_date_serach\n",
    "    if len_search_date==0:\n",
    "        return\n",
    "    xxxxxx = 0\n",
    "    for i in search_date:\n",
    "        xxxxxx = xxxxxx + 1\n",
    "        progress_date_serach = \"這是第 \" + str(xxxxxx) + \" 個，總共有 \" + str(len_search_date) + \" 個要爬\"\n",
    "        print(search_date.index(i))\n",
    "        r = s.get(url_head + i['href'], headers=headers)\n",
    "        html_doc = r.text\n",
    "        soup = BeautifulSoup(html_doc, parser)\n",
    "        i['detail_data'] = soup.select('td#page table table table table tr td')[0]\n",
    "    del progress_date_serach\n",
    "    return search_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2328b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_db_2(raw_datas,data_class, db_settings):\n",
    "    time_start = datetime.datetime.now() #開始計時\n",
    "    data_combine=[]\n",
    "    try:\n",
    "        # 建立Connection物件\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        # 建立Cursor物件\n",
    "        with conn.cursor() as cursor:\n",
    "          #資料表相關操作\n",
    "            get_max_log_id=f\"select MAX(UID) FROM {db_settings['db']}.log\"\n",
    "            # % 操作符只能直接用於字串(‘123’)，列表([1,2,3])、元組\n",
    "            command = f\"INSERT INTO {db_settings['db']}.date_search_by_date(log_UID, date, ispublish, upper, lower, href)VALUES(%s, %s, %s, %s, %s, %s) on DUPLICATE KEY UPDATE log_UID = values(log_UID),date=values(date),ispublish =values(ispublish),upper=values(upper),lower=values(lower),href = values(href)\"\n",
    "            command_log = f\"INSERT INTO {db_settings['db']}.log(datetime, task, status, record_des, errmsg) VALUES(%s, %s, %s, %s, %s)\"\n",
    "            # 紀錄開始 (暫時取消)\n",
    "#             cursor.execute(command_log, (datetime.datetime.now(), \"date_search_by_date\", \"start\", data_class, \"\"))\n",
    "            # 取得 log 的 UID\n",
    "            cursor.execute(get_max_log_id)\n",
    "            log_UID = str(cursor.fetchone()[0])\n",
    "            # 組合數據\n",
    "            for raw_data in raw_datas:\n",
    "                data_combine.append((int(log_UID), raw_data[\"date\"], raw_data[\"ispublish\"], \"\", raw_data[\"detail_data\"], raw_data[\"href\"]))  # 注意要用两个括号扩起来\n",
    "\n",
    "            # 執行\n",
    "            try:\n",
    "                cursor.executemany(command, data_combine)\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "                detail = ex.args[0] #取得詳細內容\n",
    "                cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "                lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "                fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "                lineNum = lastCallStack[1] #取得發生的行號\n",
    "                funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "                errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "                print(errMsg)\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"date_search_by_date\", \"fail\", data_class, error_class + detail + errMsg))\n",
    "            # 紀錄結束\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"date_search_by_date\", \"success\", data_class, str(len(raw_datas)) + \" 筆資料已完成\"))\n",
    "\n",
    "    except Exception as ex:\n",
    "        print('send_db_2 出現錯誤：' + str(ex))\n",
    "\n",
    "    time_end = datetime.datetime.now()    #結束計時\n",
    "    time_c= time_end - time_start   #執行所花時間\n",
    "    print('time cost', time_c, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "769a2278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_host：127.0.0.1\n"
     ]
    }
   ],
   "source": [
    "#雙重 thread\n",
    "my_host=config_my_host\n",
    "print(f\"my_host：{my_host}\")\n",
    "app = Flask(__name__)\n",
    "\n",
    "command_log = f\"INSERT INTO {db_settings['db']}.log(datetime, task, status, record_des, errmsg) VALUES(%s, %s, %s, %s, %s)\"\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def test():\n",
    "#     ip = request.remote_addr\n",
    "#     print(ip)\n",
    "#     conn = pymysql.connect(**db_settings)\n",
    "#     # 建立Cursor物件\n",
    "#     with conn.cursor() as cursor:\n",
    "#         if_finish_command=\"SELECT status FROM log where task ='api_process_list' order by UID DESC\"\n",
    "#         cursor.execute(if_finish_command)\n",
    "#         if cursor.fetchone() is None:\n",
    "#             cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", \"initial\", \"\", \"\"))\n",
    "#         cursor.execute(if_finish_command)\n",
    "#         if_finish = str(cursor.fetchone()[0])\n",
    "#     if re.search(r'finish', if_finish, flags=0) or if_finish == \"initial\":\n",
    "#         wording ='<p>恭喜!網站建置成功!</p><p>api說明：</p><p>1.&nbsp;/thread：查看當前執行緒狀態，無須參數 (get請求)</p><p>2.&nbsp;/start_task_all：開始執行爬蟲，若要強制重啟，參數帶 restart = 1 (get請求)</p><p>3.&nbsp;/date_search_crawler：公告日期執行爬蟲，參數帶 start_date、end_date、if_publish (get請求)<br />start_date、end_date =&gt; 以字串型態帶日期，ex:20210125，if_publish =&gt; 以字串形式帶 all | yes | no </p><p>http://127.0.0.1:5000/date_search_crawler?start_date=20210128&end_date=20210204&if_publish=yes</p><p>4.&nbsp;/progress：查看當前執行緒進度，無須參數 (get請求)</p><p>5. /proxy : 從免費代理網站抓取proxy進資料庫，無須參數 (get請求)'\n",
    "#         status_code = 200\n",
    "#     else:\n",
    "#         wording =\"I am busy now, please wait\"\n",
    "#         status_code = 500\n",
    "        \n",
    "    wording ='<p>恭喜!網站建置成功!</p><p>api說明：</p><p>1.&nbsp;/thread：查看當前執行緒狀態，無須參數 (get請求)</p><p>2.&nbsp;/start_task_list：執行爬蟲：抓列表資料，若要強制重啟，參數帶 restart = 1；若要設定爬取的初始頁面，參數帶 start_page=[x,x,x,x] ([招標公告頁數、公開徵求頁數、公開閱覽頁數、決標公告頁數])；若想要設定爬取 N 天前的資料，參數要帶 day_before，以 int 的方式放值，ex: day_before=0  (get請求)</p><p>http://127.0.0.1:5000/start_task_list?restart=1&start_page=[1,1,1,1]&day_before=0</p><p><p>3.&nbsp;/start_task_detail：執行爬蟲：抓內頁資料，要晚於 /start_task_list 5分鐘，spilt 參數代表分段執行，spilt[0]是第幾段；spilt[1]是全部共幾段；error_data代表是否要重新獲取當前資料庫內錯誤資料 (get請求)</p><p>http://127.0.0.1:5000/start_task_detail?spilt=(1,2)&error_data=0</p>4.&nbsp;/date_search_crawler：執行爬蟲：依公告日期查詢，參數帶 start_date、end_date、if_publish (get請求)<br />start_date、end_date =&gt; 以字串型態帶日期，ex:20210125，if_publish =&gt; 以字串形式帶 all | yes | no </p><p>http://127.0.0.1:5000/date_search_crawler?start_date=20210128&end_date=20210204&if_publish=yes</p><p>5.&nbsp;/progress：查看當前執行緒進度，無須參數 (get請求)</p><p>6. /proxy : 從免費代理網站抓取proxy進資料庫，無須參數 (get請求)' \n",
    "    status_code = 200\n",
    "\n",
    "    return wording,status_code\n",
    "\n",
    "@app.route('/thread', methods=['GET'])\n",
    "def thread_status():\n",
    "    wording =\"\"\n",
    "    wording2 =\"\"\n",
    "    wording3 =\"\"\n",
    "    wording4 =\"\"\n",
    "    wording5=\"\"\n",
    "    wording6=\"\"\n",
    "    code =200\n",
    "    print(str(threading.enumerate()))\n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_tenderDeclaration_searchAppeal\") >=0:\n",
    "            wording = \"當前正在執行 thread_tenderDeclaration_searchAppeal 執行緒\"\n",
    "            code = 500\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_date_search_crawler\") >=0:\n",
    "            wording2 = \"當前正在執行 thread_date_search_crawler 執行緒\"\n",
    "            code = 500\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_proxy_crawler\") >=0:\n",
    "            wording3 = \"當前正在執行 thread_proxy_crawler 執行緒\"\n",
    "            code = 500\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_final_datesearch_publicRead\") >=0:\n",
    "            wording4 = \"當前正在執行 thread_final_datesearch_publicRead 執行緒\"\n",
    "            code = 500\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_tenderDeclaration_detail\") >=0:\n",
    "            wording5 = \"當前正在執行 thread_tenderDeclaration_detail 執行緒\"\n",
    "            code = 500\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_final_detail\") >=0:\n",
    "            wording6 = \"當前正在執行 thread_final_detail 執行緒\"\n",
    "            code = 500\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "        \n",
    "    return wording + \" \" + wording2 + \" \" + wording3 + \" \" + wording4 + \" \" + wording5 + \" \" + wording6,code\n",
    "        \n",
    "@app.route('/proxy', methods=['GET'])\n",
    "def proxy():\n",
    "    def proxyy():\n",
    "        p_yes=get_proxy_ip(\"yes\",db_settings)\n",
    "        #p_no=get_proxy_ip(\"no\",db_settings)\n",
    "        send_ip(p_yes, db_settings ,\"only https\",new = True)\n",
    "        #send_ip(p_no, db_settings ,\"only http\",new = True)\n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_proxy_crawler\") >=0:\n",
    "            return \"已經在爬 proxy 了，別吵\",500\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass    \n",
    "    \n",
    "    thread_proxy_crawler = threading.Thread(target=proxyy, kwargs={},name=\"thread_proxy_crawler\")\n",
    "    thread_proxy_crawler.start()\n",
    "    return \"started\",200\n",
    "    \n",
    "@app.route('/start_task_list', methods=['GET'])\n",
    "def start_task_all():\n",
    "    if request.args:\n",
    "        dd= request.args.to_dict()\n",
    "        if set(['start_page','restart','day_before']) >= set(dd.keys()):\n",
    "            print(\"傳參成功\")\n",
    "        else:\n",
    "            return \"傳參失敗，參數只允許\"+' start_page '+'restart '+'day_before',500\n",
    "    def do_work(start_page,day_before,db_settings,headers):\n",
    "        get_data_date = (datetime.date.today() - datetime.timedelta(days=day_before))\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", str(ip) + \" start\", \"tenderDeclaration_searchAppeal\", f\"爬取日期：{get_data_date}，tenderDeclaration 從第 {start_page[0]} 頁開始爬，searchAppeal 從第 {start_page[1]} 頁開始爬\"))\n",
    "        time_start = time.time()\n",
    "        print(\"=====1=======\")\n",
    "        \n",
    "        try:\n",
    "            x1 = gov_serach_crawler(\"tenderDeclaration\", headers = headers,start_page = int(start_page[0]),day_before=day_before)\n",
    "            send_db(x1,\"tenderDeclaration\", db_settings)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "            detail = ex.args[0] #取得詳細內容\n",
    "            cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "            lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "            fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "            lineNum = lastCallStack[1] #取得發生的行號\n",
    "            funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "            errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "            print(errMsg)\n",
    "            conn = pymysql.connect(**db_settings)\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", \"wrong\", \"tenderDeclaration\", errMsg))\n",
    "        print(\"=====2=======\")\n",
    "        try:\n",
    "            x2 = gov_serach_crawler(\"searchAppeal\", headers = headers,start_page = int(start_page[1]),day_before=day_before)\n",
    "            send_db(x2,\"searchAppeal\", db_settings)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "            detail = ex.args[0] #取得詳細內容\n",
    "            cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "            lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "            fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "            lineNum = lastCallStack[1] #取得發生的行號\n",
    "            funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "            errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "            print(errMsg)\n",
    "            conn = pymysql.connect(**db_settings)\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", \"wrong\", \"searchAppeal\", errMsg))\n",
    "        \n",
    "#         try:\n",
    "#             x4 = gov_serach_crawler(\"predict\",proxies,db_settings, headers = headers, account=account,password=password)\n",
    "#             send_db(x4,\"predict\", db_settings)\n",
    "#         except Exception as ex:\n",
    "#             print(ex)\n",
    "#             error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "#             detail = ex.args[0] #取得詳細內容\n",
    "#             cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "#             lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "#             fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "#             lineNum = lastCallStack[1] #取得發生的行號\n",
    "#             funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "#             errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "#             print(errMsg)\n",
    "#             conn = pymysql.connect(**db_settings)\n",
    "#             with conn.cursor() as cursor:\n",
    "#                 cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", \"wrong\", \"predict\", errMsg))\n",
    "        print(\"=====4=======\")\n",
    "        time_d = time.time()\n",
    "        time_c = time_d - time_start\n",
    "        print(\"！！！！！  thread_tenderDeclaration_searchAppeal 總執行時間 \"+str(time_c) +\" 秒\")\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", str(ip) + \" finish\", \"tenderDeclaration_searchAppeal\", \"\"))\n",
    "            \n",
    "    def do_work_final(start_page,day_before,db_settings,headers):\n",
    "        get_data_date = (datetime.date.today() - datetime.timedelta(days=day_before))\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", str(ip) + \" start\", \"final_publicRead_datesearch\", f\"爬取日期：{get_data_date}，final 從第 {start_page[3]} 頁開始爬，publicRead 從第 {start_page[2]} 頁開始爬\"))\n",
    "        time_start = time.time()\n",
    "        try:\n",
    "            time.sleep(5)\n",
    "            x5 = gov_serach_crawler(\"final\", headers = headers,start_page = int(start_page[3]),day_before=day_before)\n",
    "            send_db(x5,\"final\", db_settings)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "            detail = ex.args[0] #取得詳細內容\n",
    "            cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "            lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "            fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "            lineNum = lastCallStack[1] #取得發生的行號\n",
    "            funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "            errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "            print(errMsg)\n",
    "            conn = pymysql.connect(**db_settings)\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", \"wrong\", \"final\", errMsg))\n",
    "        print(\"=====3=======\")        \n",
    "        try:\n",
    "            x3 = gov_serach_crawler(\"publicRead\", headers = headers,start_page = int(start_page[3]),day_before=day_before)\n",
    "            send_db(x3,\"publicRead\", db_settings)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "            detail = ex.args[0] #取得詳細內容\n",
    "            cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "            lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "            fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "            lineNum = lastCallStack[1] #取得發生的行號\n",
    "            funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "            errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "            print(errMsg)\n",
    "            conn = pymysql.connect(**db_settings)\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", \"wrong\", \"publicRead\", errMsg))\n",
    "#依公告日期招標\n",
    "#         try:\n",
    "#             s = rq.session()\n",
    "#             xx = search_by_date(s,datetime.datetime.today()- datetime.timedelta(days=1), datetime.datetime.today(),\"all\", headers = headers)\n",
    "#             print(xx)\n",
    "#             xxx = date_search_crawler(s,xx,headers)\n",
    "#             send_db_2(xxx,\"date_search_by_date\", db_settings)\n",
    "#         except Exception as ex:\n",
    "#             print(ex)\n",
    "#             error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "#             detail = ex.args[0] #取得詳細內容\n",
    "#             cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "#             lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "#             fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "#             lineNum = lastCallStack[1] #取得發生的行號\n",
    "#             funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "#             errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "#             print(errMsg)\n",
    "#             conn = pymysql.connect(**db_settings)\n",
    "#             with conn.cursor() as cursor:\n",
    "#                 cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", \"wrong\", \"date_search_crawler\", str(detail)))\n",
    "      \n",
    "        time_d = time.time()\n",
    "        time_c = time_d - time_start\n",
    "        print(\"！！！！！  thread_final_datesearch_publicRead 總執行時間 \"+str(time_c) +\" 秒\")\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", str(ip) + \" finish\", \"final_publicRead_datesearch\", \"\"))\n",
    "    \n",
    "    #資料庫log檢查\n",
    "    ip = request.remote_addr\n",
    "    conn = pymysql.connect(**db_settings)\n",
    "    if_finish_command=f\"SELECT status FROM {db_settings['db']}.log where task ='api_process_list' order by UID DESC\"\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(if_finish_command)\n",
    "        if cursor.fetchone() is None:\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", \"initial\", \"\", \"\"))\n",
    "        cursor.execute(if_finish_command)\n",
    "        if_finish = str(cursor.fetchone()[0])\n",
    "    \n",
    "    # 不判斷資料庫log強制重啟\n",
    "    if re.search(r'finish', if_finish, flags=0) or re.search(r'wrong', if_finish, flags=0) or if_finish == \"initial\":\n",
    "        pass\n",
    "    elif \"restart\" in request.args:\n",
    "        try:\n",
    "            if request.args.get(\"restart\")==1:\n",
    "                pass\n",
    "        except:\n",
    "            return \"若想要重啟爬蟲，參數要帶restart，值請放 1 \"\n",
    "    else:\n",
    "        return \"資料庫未收到結束log，可帶參數 restart=1 忽略資料庫log判斷\",500\n",
    "    #執行序之間任一ip被鎖block全部\n",
    "    global waitt\n",
    "    waitt = 0\n",
    "    \n",
    "    day_before=0\n",
    "    if 'day_before' in request.args:\n",
    "        try:\n",
    "            day_before = eval(request.args.get(\"day_before\"))\n",
    "        except:\n",
    "            return \"若想要設定爬取 N 天前的資料，參數要帶 day_before，以 int 的方式放值，ex: day_before=0 \"\n",
    "\n",
    "    \n",
    "    #設定起始頁面\n",
    "    start_page=[1,1,1,1]\n",
    "    if 'start_page' in request.args:\n",
    "        try:\n",
    "            start_page = eval(request.args.get(\"start_page\"))\n",
    "            if len(start_page) !=4:\n",
    "                return \"start_page 參數長度不足 4 (須設定 4 個：tenderDeclaration、searchAppeal、publicRead、final)，無須爬取的項目可輸入1999！\"\n",
    "            else:\n",
    "                print(\"頁數參數接收成功\")\n",
    "        except:\n",
    "            return \"若想要設定爬取的初始頁數，參數要帶 start_page，以 list 的方式放值=>[招標公告頁數,公開徵求頁數,公開閱覽頁數,決標公告頁數]，ex: start_page=[1,1,1,1] \"\n",
    "    print(f\"tenderDeclaration 從第 {start_page[0]} 頁開始爬取\")\n",
    "    print(f\"searchAppeal 從第 {start_page[1]} 頁開始爬取\")\n",
    "    print(f\"publicRead 從第 {start_page[2]} 頁開始爬取\")\n",
    "    print(f\"final 從第 {start_page[3]} 頁開始爬取\")\n",
    "    \n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_tenderDeclaration_searchAppeal\") >= 0 :\n",
    "            wording0=\"thread_tenderDeclaration_searchAppeal 執行緒正在執行；\"\n",
    "        else:\n",
    "            thread_tenderDeclaration_searchAppeal = threading.Thread(target=do_work, kwargs={'start_page':start_page,'day_before':day_before,'db_settings': db_settings,'headers': headers},name=\"thread_tenderDeclaration_searchAppeal\")\n",
    "            thread_tenderDeclaration_searchAppeal.start()\n",
    "            wording0=\"啟動 thread_tenderDeclaration_searchAppeal 執行緒；\"\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_final_datesearch_publicRead\") >= 0 :\n",
    "            wording1=\"thread_final_datesearch_publicRead 執行緒正在執行\"\n",
    "        else:\n",
    "            thread_final_datesearch_publicRead = threading.Thread(target=do_work_final, kwargs={'start_page':start_page,'day_before':day_before,'db_settings': db_settings,'headers': headers},name=\"thread_final_datesearch_publicRead\")\n",
    "            thread_final_datesearch_publicRead.start()\n",
    "            wording1=\"啟動 thread_final_datesearch_publicRead 執行緒\"\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "\n",
    "    return wording0+wording1,200\n",
    "\n",
    "@app.route('/start_task_detail', methods=['GET'])\n",
    "def start_task_detail():\n",
    "    path = config_path\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree (path)\n",
    "        os.mkdir(path)\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "    ip = request.remote_addr\n",
    "\n",
    "    def do_work_detail(tenderType, error_data,spilt,diff_seconds_data,diff_seconds_conn,db_settings,headers,No_proxy,block_crawl_no,path):\n",
    "        if tenderType == \"final\":\n",
    "            global progress_final_detail\n",
    "            progress_final_detail =''\n",
    "        elif tenderType ==\"tenderDeclaration\":\n",
    "            global progress_tenderDeclaration_detail\n",
    "            progress_tenderDeclaration_detail =''\n",
    "        \n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        if error_data:\n",
    "            get_data=f\"SELECT * FROM {db_settings['db']}.gov_purchase \" + \" where (error_code > 0 and TIMESTAMPDIFF(DAY, get_data_date, now()) <= \" + error_data_catch + \" ) and tenderType='\"+tenderType+\"'\"\n",
    "            error_log_word='(錯誤資料重新抓取)'\n",
    "        else:\n",
    "            get_data=f\"SELECT * FROM {db_settings['db']}.gov_purchase \" + \" where (detail_data_html ='' and detail_data like '%{}%' and error_code = 0) and tenderType='\"+tenderType+\"'\"\n",
    "            error_log_word = \"\"\n",
    "        print(get_data)\n",
    "        # 建立Cursor物件\n",
    "        with conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "          #資料表相關操作\n",
    "            cursor.execute(get_data)\n",
    "            raw_datas_all = cursor.fetchall()\n",
    "            print(f'{tenderType} 一共抓出 {len(raw_datas_all)} 筆還沒有內頁資料 {error_log_word}')\n",
    "            if spilt:\n",
    "                if spilt[1]>1:\n",
    "                    first_part = int(len(raw_datas_all)/spilt[1]*(spilt[0]-1))\n",
    "                    second_part = int(len(raw_datas_all)/spilt[1]*spilt[0])\n",
    "                    raw_datas_all = raw_datas_all[first_part:second_part]\n",
    "                    wording = f'{tenderType} 切分 {spilt[0]} 段，此為第 {spilt[1]} 段，取第 {first_part} 個到 {second_part} 個，共 {len(raw_datas_all)} 個，進行爬取 {error_log_word}'\n",
    "                else:\n",
    "                    wording = f'{tenderType} 不切分，共 {len(raw_datas_all)} 個，進行爬取 {error_log_word}'\n",
    "            elif len(raw_datas_all)>0:\n",
    "                wording = f'{tenderType} 不切分，共 {len(raw_datas_all)} 個，進行爬取 {error_log_word}'\n",
    "            else:\n",
    "                wording = f'{tenderType} 資料皆已爬取完成 {error_log_word}'\n",
    "            raw_datas=raw_datas_all[0:block_crawl_no]\n",
    "            print(wording)\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"api_process_detail\", str(ip) + \" start\", tenderType, wording))\n",
    "\n",
    "        try:\n",
    "            #限制嘗試抓取次數，避免死機\n",
    "            x=0\n",
    "            while len(raw_datas)>0:\n",
    "                ll = get_only_detail(tenderType,raw_datas,diff_seconds_data,diff_seconds_conn,headers, db_settings,path,No_proxy=False)\n",
    "                # get_only_detail會回傳成功抓取的筆數 (ll)\n",
    "                if ll>=1:\n",
    "                    x = 0\n",
    "                else:\n",
    "                    x = x + 1 \n",
    "                    \n",
    "                #避免過長抓取失敗\n",
    "                del raw_datas_all[0:block_crawl_no]\n",
    "                raw_datas=raw_datas_all[0:block_crawl_no]\n",
    "                    \n",
    "#                 conn = pymysql.connect(**db_settings)\n",
    "#                 # 建立Cursor物件\n",
    "#                 with conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "#                   #資料表相關操作\n",
    "#                     cursor.execute(get_data)\n",
    "#                     raw_datas = cursor.fetchall()\n",
    "#                 if len(raw_datas) ==0:\n",
    "#                     print('都抓完了！！！！')\n",
    "#                     break\n",
    "#                 else:\n",
    "                if x>=2:\n",
    "                    print(f'其他錯誤_8：{tenderType} 出現 {len(raw_datas)} 筆無法抓取內頁資料')\n",
    "                    break\n",
    "            conn = pymysql.connect(**db_settings)\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"api_process_detail\", str(ip)+\"finish\", tenderType, \"\"))\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "            detail = ex.args[0] #取得詳細內容\n",
    "            cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "            lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "            fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "            lineNum = lastCallStack[1] #取得發生的行號\n",
    "            funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "            errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "            print(errMsg)\n",
    "            conn = pymysql.connect(**db_settings)\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"api_process_detail\", \"wrong\", \"tenderDeclaration_final_detail\", errMsg))\n",
    "        \n",
    "        if tenderType=='tenderDeclaration':\n",
    "            del progress_tenderDeclaration_detail\n",
    "        elif tenderType=='final':\n",
    "            del progress_final_detail\n",
    "            \n",
    "            \n",
    "    #限制取得內頁資料的時間 (若成功抓取則等待)\n",
    "    diff_seconds_data = config_diff_seconds_data\n",
    "    print(f\"diff_seconds_data：{diff_seconds_data}\")\n",
    "    #限制每一次嘗試取得內頁資料的時間 (每次嘗試連接都等待)\n",
    "    diff_seconds_conn = config_diff_seconds_conn\n",
    "    print(f\"diff_seconds_conn：{diff_seconds_conn}\")\n",
    "    #設定用本機ip爬取內頁資料\n",
    "    No_proxy = config_No_proxy\n",
    "    print(f\"No_proxy：{No_proxy}\")\n",
    "    #爬取幾筆就存進資料庫？\n",
    "    block_crawl_no = config_block_crawl_no\n",
    "    print(f\"block_crawl_no：{block_crawl_no}\")\n",
    "    spilt = False\n",
    "    error_data = False\n",
    "    if request.args:\n",
    "        dd = request.args.to_dict()\n",
    "        print(dd)\n",
    "        if set(['spilt', 'error_data']) >= set(dd.keys()):\n",
    "            print(\"傳參成功\")\n",
    "        else:\n",
    "            return \"傳參失敗，參數只允許 spilt, error_data\",500\n",
    "        try:\n",
    "            #切分段數\n",
    "            spilt = dicMemberCheck('spilt',dd)\n",
    "            if spilt !=\"\" :\n",
    "                spilt=eval(spilt)\n",
    "                if len(spilt)==2 and spilt[0]>=spilt[1]:\n",
    "                    print(f\"spilt {spilt}\")\n",
    "            else:\n",
    "                spilt = False\n",
    "            print(f\"spilt {spilt}\")\n",
    "            \n",
    "            #是否要重新獲取錯誤資料\n",
    "            error_data = dicMemberCheck('error_data',dd)\n",
    "            if error_data !=\"\" :\n",
    "                error_data=eval(error_data)\n",
    "                if error_data:\n",
    "                    print(f\"error_data {error_data}\")\n",
    "            else:\n",
    "                error_data = False\n",
    "            print(f\"spilt {spilt}\")\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            return \"參數轉換錯誤\",500\n",
    "    \n",
    "    delete_ip(config_ip_time_notConnect,db_settings)\n",
    "    if str(threading.enumerate()).find(\"thread_tenderDeclaration_detail\") >= 0 :\n",
    "        wording1=\"thread_tenderDeclaration_detail 執行緒正在執行\"\n",
    "    else:\n",
    "        thread_tenderDeclaration_detail = threading.Thread(target=do_work_detail, kwargs={'tenderType':'tenderDeclaration','error_data':error_data,'diff_seconds_data':diff_seconds_data,'diff_seconds_conn':diff_seconds_conn,'headers':headers,'db_settings': db_settings,'No_proxy':No_proxy,'spilt':spilt,'block_crawl_no':block_crawl_no,'path':config_path},name=\"thread_tenderDeclaration_detail\")\n",
    "        thread_tenderDeclaration_detail.start()\n",
    "        wording1=\"啟動 thread_tenderDeclaration_detail 執行緒\"\n",
    "        \n",
    "    if str(threading.enumerate()).find(\"thread_final_detail\") >= 0 :\n",
    "        wording2=\"thread_final_detail 執行緒正在執行\"\n",
    "    else:\n",
    "        thread_final_detail = threading.Thread(target=do_work_detail, kwargs={'tenderType':'final','error_data':error_data,'diff_seconds_data':diff_seconds_data,'diff_seconds_conn':diff_seconds_conn,'headers':headers,'db_settings': db_settings,'No_proxy':No_proxy,'spilt':spilt,'block_crawl_no':block_crawl_no,'path':config_path},name=\"thread_final_detail\")\n",
    "        thread_final_detail.start()\n",
    "        wording2=\"啟動 thread_final_detail 執行緒\"\n",
    "        \n",
    "    if str(threading.enumerate()).find(\"thread_searchAppeal_detail\") >= 0 :\n",
    "        wording2=\"thread_searchAppeal_detail 執行緒正在執行\"\n",
    "    else:\n",
    "        thread_searchAppeal_detail = threading.Thread(target=do_work_detail, kwargs={'tenderType':'searchAppeal','error_data':error_data,'diff_seconds_data':diff_seconds_data,'diff_seconds_conn':diff_seconds_conn,'headers':headers,'db_settings': db_settings,'No_proxy':No_proxy,'spilt':spilt,'block_crawl_no':block_crawl_no,'path':config_path},name=\"thread_searchAppeal_detail\")\n",
    "        thread_searchAppeal_detail.start()\n",
    "        wording3=\"啟動 thread_searchAppeal_detail 執行緒\"\n",
    "        \n",
    "    return wording1 +\" \"+ wording2+\" \"+ wording3,200\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/progress', methods=['GET'])\n",
    "def get_progress():\n",
    "    wording={}\n",
    "    try:\n",
    "        wording['tenderDeclaration'] = progress_tenderDeclaration\n",
    "    except:\n",
    "        wording['tenderDeclaration'] = \"tenderDeclaration 任務未啟動\"\n",
    "        \n",
    "    try:\n",
    "        wording['searchAppeal'] = progress_searchAppeal\n",
    "    except:\n",
    "        wording['searchAppeal'] = \"searchAppeal 任務未啟動\"\n",
    "        \n",
    "    try:\n",
    "        wording['publicRead'] = progress_publicRead\n",
    "    except:\n",
    "        wording['publicRead'] = \"publicRead 任務未啟動\"\n",
    "        \n",
    "    try:\n",
    "        wording['predict'] = progress_predict\n",
    "    except:\n",
    "        wording['predict'] = \"predict 任務未啟動\"\n",
    "    \n",
    "    try:\n",
    "        wording['final'] = progress_final\n",
    "    except:\n",
    "        wording['final'] = \"final 任務未啟動\"\n",
    "        \n",
    "    try:\n",
    "        wording['date_serach'] = progress_date_serach\n",
    "    except:\n",
    "        wording['date_serach'] = \"date_serach 任務未啟動\"\n",
    "\n",
    "    try:\n",
    "        wording['tenderDeclaration_detail'] = progress_tenderDeclaration_detail\n",
    "    except:\n",
    "        wording['tenderDeclaration_detail'] = \"tenderDeclaration_detail 任務未啟動\"\n",
    "        \n",
    "    try:\n",
    "        wording['final_detail'] = progress_final_detail\n",
    "    except:\n",
    "        wording['final'] = \"final_detail 任務未啟動\"\n",
    "        \n",
    "    return wording,200\n",
    "\n",
    "# @app.route('/test', methods=['GET'])\n",
    "# def tesddt():\n",
    "#     if request.args:\n",
    "#         dd= request.args.to_dict()\n",
    "#         if set(['start_date','end_date','']) >= set(dd.keys()):\n",
    "#             return \"傳參成功\",200\n",
    "#         else:\n",
    "#             return \"傳參失敗\",200\n",
    "\n",
    "\n",
    "@app.route('/date_search_crawler', methods=['GET'])\n",
    "def date_serach():\n",
    "    if request.args:\n",
    "        dd= request.args.to_dict()\n",
    "        if set(['start_date','end_date','if_publish']) >= set(dd.keys()):\n",
    "            print(\"傳參成功\")\n",
    "        else:\n",
    "            return \"傳參失敗，參數只允許\"+' start_date '+'end_date '+'if_publish',500\n",
    "    headers[\"User-Agent\"] = UserAgent(use_cache_server=False).random\n",
    "    def do_work_2(start_date,end_date,if_publish,db_settings,headers):\n",
    "        s = rq.session()\n",
    "        try:\n",
    "            xx = search_by_date(s,start_date, end_date,if_publish,headers)\n",
    "            print(xx)\n",
    "            xxx = date_search_crawler(s,xx,headers)\n",
    "            send_db_2(xxx,\"date_search_by_date\", db_settings)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "            detail = ex.args[0] #取得詳細內容\n",
    "            cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "            lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "            fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "            lineNum = lastCallStack[1] #取得發生的行號\n",
    "            funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "            errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "            print(errMsg)\n",
    "            conn = pymysql.connect(**db_settings)\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", \"wrong\", \"date_search_crawler\", error_class + detail + errMsg))\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", str(ip) + \" finish\", \"date_search_crawler\", \"\"))\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        if str(threading.enumerate()).find(\"thread_start_date_search_crawler\") >=0:\n",
    "            return \"當前執行緒正在執行\",500\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        start_date = str(request.args.get(\"start_date\"))\n",
    "        start_date = datetime.datetime.strptime(start_date, \"%Y%m%d\")\n",
    "    except Exception as ex:    \n",
    "        return \"start_date 參數格式錯誤\",500\n",
    "    \n",
    "    try:\n",
    "        end_date = str(request.args.get(\"end_date\"))\n",
    "        end_date = datetime.datetime.strptime(end_date, \"%Y%m%d\")\n",
    "    except Exception as ex:    \n",
    "        return \"end_date 參數格式錯誤\",500\n",
    "    \n",
    "    try:\n",
    "        if_publish = str(request.args.get(\"if_publish\"))\n",
    "        if not (if_publish == \"yes\" or if_publish == \"no\" or if_publish == \"all\"):\n",
    "            return \"if_publish 參數格式錯誤\",500\n",
    "    except Exception as ex:\n",
    "        return \"if_publish 參數沒帶\",500\n",
    "        \n",
    "    \n",
    "    ip = request.remote_addr\n",
    "#     conn = pymysql.connect(**db_settings)\n",
    "#     with conn.cursor() as cursor:\n",
    "#         if_finish_command=\"SELECT status FROM log where task ='api_process_date_serach' order by UID DESC\"\n",
    "#         cursor.execute(if_finish_command)\n",
    "#         if cursor.fetchone() is None:\n",
    "#             cursor.execute(command_log, (datetime.datetime.now(), \"api_process_date_serach\", \"initial\", \"\", \"\"))\n",
    "#         cursor.execute(if_finish_command)\n",
    "#         if_finish = str(cursor.fetchone()[0])\n",
    "    \n",
    "#     print(if_finish)    \n",
    "#     if re.search(r'finish', if_finish, flags=0) or re.search(r'wrong', if_finish, flags=0) or if_finish == \"initial\":\n",
    "#         pass\n",
    "#     else:\n",
    "#         wording =\"資料庫未收到結束log\"\n",
    "#         status_code = 500\n",
    "#         return wording,status_code\n",
    "    \n",
    "    conn = pymysql.connect(**db_settings)\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(command_log, (datetime.datetime.now(), \"api_process_list\", str(ip) + \" start\", \"date_search_crawler\", \"\"))\n",
    "    \n",
    "    thread_date_search_crawler = threading.Thread(target=do_work_2, kwargs={'start_date': start_date,'end_date': end_date,'if_publish':if_publish,'db_settings':db_settings,'headers':headers},name=\"thread_date_search_crawler\")\n",
    "    thread_date_search_crawler.start()    \n",
    "    return \"started\",200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee14bc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000 (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [09/Jul/2022 00:05:56] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Jul/2022 00:06:00] \"GET /start_task_detail?spilt=(1,2)&error_data=1 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff_seconds_data：0.0\n",
      "diff_seconds_conn：0.0\n",
      "No_proxy：False\n",
      "block_crawl_no：1000\n",
      "{'spilt': '(1,2)', 'error_data': '1'}\n",
      "傳參成功\n",
      "spilt (1, 2)\n",
      "error_data 1\n",
      "spilt (1, 2)\n",
      "ip_time_notConnect：25\n",
      "SELECT * FROM tender_test.gov_purchase  where (error_code > 0 and TIMESTAMPDIFF(DAY, get_data_date, now()) <= 7 ) and tenderType='tenderDeclaration'\n",
      "SELECT * FROM tender_test.gov_purchase  where (error_code > 0 and TIMESTAMPDIFF(DAY, get_data_date, now()) <= 7 ) and tenderType='searchAppeal'\n",
      "SELECT * FROM tender_test.gov_purchase  where (error_code > 0 and TIMESTAMPDIFF(DAY, get_data_date, now()) <= 7 ) and tenderType='final'\n",
      "searchAppeal 一共抓出 0 筆還沒有內頁資料 (錯誤資料重新抓取)\n",
      "searchAppeal 切分 1 段，此為第 2 段，取第 0 個到 0 個，共 0 個，進行爬取 (錯誤資料重新抓取)\n",
      "tenderDeclaration 一共抓出 0 筆還沒有內頁資料 (錯誤資料重新抓取)\n",
      "tenderDeclaration 切分 1 段，此為第 2 段，取第 0 個到 0 個，共 0 個，進行爬取 (錯誤資料重新抓取)\n",
      "final 一共抓出 0 筆還沒有內頁資料 (錯誤資料重新抓取)\n",
      "final 切分 1 段，此為第 2 段，取第 0 個到 0 個，共 0 個，進行爬取 (錯誤資料重新抓取)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.debug = False\n",
    "    app.config['JSON_AS_ASCII'] = False\n",
    "    app.run(host=my_host, port=5000)\n",
    "    #serve(app, host=my_host, port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32ae02b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tenderType = 'tenderDeclaration'\n",
    "# conn = pymysql.connect(**db_settings)\n",
    "# get_data=\"SELECT detail_data_html FROM gov_purchase where get_data_date>='2022-06-04' and tenderType='\"+tenderType+\"'\"\n",
    "# # 建立Cursor物件\n",
    "# with conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "#   #資料表相關操作\n",
    "#     cursor.execute(get_data)\n",
    "#     raw_datas_all = cursor.fetchall()\n",
    "\n",
    "# pattern = re.compile(r'class=\\\"(.*?)\\\"')\n",
    "# ele=[]\n",
    "# for i in raw_datas_all:\n",
    "#     mystr=str(i['detail_data_html'])\n",
    "#     result = set(pattern.findall(mystr))\n",
    "#     print(result)\n",
    "#     result=list(result)\n",
    "#     ele=ele+result\n",
    "# res = []\n",
    "# [res.append(x) for x in ele if x not in res]\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934382d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GOVPUR",
   "language": "python",
   "name": "govpur"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

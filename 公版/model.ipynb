{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "debcd9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudscraper\n",
    "import pymysql\n",
    "from openpyxl import Workbook\n",
    "import requests as rq\n",
    "import brotli\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import configparser\n",
    "from urllib.parse import urlencode\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef28d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "    \"Accept-Language\": \"zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6\", \n",
    "    \"Connection\":\"keep-alive\",\n",
    "    \"Sec-Fetch-Dest\": \"document\", \n",
    "    \"Sec-Fetch-Mode\": \"navigate\", \n",
    "    \"Sec-Fetch-Site\": \"same-origin\", \n",
    "    \"Upgrade-Insecure-Requests\": \"?1\", \n",
    "    \"User-Agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'  # 偽裝使用者代理\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d8f7237",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2666586191.py, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    host = 127.0.0.1\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# file handle\n",
    "def save_setting_ini(file_name, ini):\n",
    "    config = configparser.ConfigParser()\n",
    "    config['crawler'] = ini\n",
    "    with open(file_name, 'w') as f:\n",
    "        config.write(f)\n",
    "        \n",
    "def load_setting_ini(file_name):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(file_name)\n",
    "    data = {}\n",
    "    data['former_part'] = config['crawler']['former_part']\n",
    "    data['initial_pages'] = config['crawler']['initial_pages']\n",
    "    data['latter_part'] = config['crawler']['latter_part']\n",
    "    data['end_pages'] = config['crawler']['end_pages']\n",
    "    data['next_url'] = config['crawler']['next_url']\n",
    "    data['next_tag'] = config['crawler']['next_tag']\n",
    "    data['domain'] = config['crawler']['domain']\n",
    "    data['name'] = config['crawler']['name'].replace('%%','%')\n",
    "    data['tag_section'] = config['crawler']['tag_section']\n",
    "    data['tag_detail'] = config['crawler']['tag_detail']\n",
    "    data['upper_limit'] = config['crawler']['upper_limit']\n",
    "    data['data_download'] = config['crawler']['data_download']\n",
    "    data_before ={}\n",
    "    data_before['before_url'] = config['crawler']['before_url']\n",
    "    data_before['before_data'] = config['crawler']['before_data']\n",
    "    data_before['before_domain'] = config['crawler']['before_domain']\n",
    "    data_before['before_method'] = config['crawler']['before_method']\n",
    "    data_before['before_content_type'] = config['crawler']['before_content_type']\n",
    "    db_settings={}\n",
    "    db_settings_others={}\n",
    "    db_settings['host'] = config['crawler']['host']\n",
    "    if db_settings['host'] != '':\n",
    "        db_settings['port'] = config['crawler'].getint('port')\n",
    "        db_settings['user'] = config['crawler']['user']\n",
    "        db_settings['password'] = config['crawler']['password']\n",
    "        db_settings['db'] = config['crawler']['db']\n",
    "        db_settings['charset'] = config['crawler']['charset']\n",
    "        db_settings['autocommit'] = config['crawler']['autocommit']\n",
    "        db_settings_others['table_name'] = config['crawler']['table_name']    \n",
    "        db_settings_others['is_datetime'] = config['crawler']['is_datetime']\n",
    "        db_settings_others['is_cover'] = config['crawler'].getboolean('is_cover')\n",
    "    return data,data_before,db_settings,db_settings_others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca82ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用網址來做換頁的爬蟲\n",
    "class site_url_Spider:\n",
    "    def __init__(self):\n",
    "        self.s = rq.session()\n",
    "        self.headers = headers\n",
    "        self.datas=[]\n",
    "        self.json=[]\n",
    "        self.use_cloudscraper = True\n",
    "        self.scraper = cloudscraper.CloudScraper(browser='chrome')\n",
    "    def update_attr(self, name, headers_attr, url_attr, next_tag, tag_section, tag_detail, data, method):\n",
    "        self.name = name\n",
    "        # url_attr 用來組合要 request 的網址，依順序放入：(網址前半部分, 初始頁數, 網址後半部分, 結束頁數)\n",
    "        self.former_part = url_attr['former_part']\n",
    "        if url_attr['initial_pages'] == '':\n",
    "            self.url = self.former_part\n",
    "        else:\n",
    "            self.initial_pages = int(url_attr['initial_pages'])\n",
    "            self.latter_part = url_attr['latter_part']\n",
    "            self.end_pages = int(url_attr['end_pages'])\n",
    "            self.url = self.former_part+str(self.initial_pages)+self.latter_part\n",
    "        # headers_attr 用來傳入 headers的相關參數，依順序放入：(headers['Host'], headers['Content-Type'])\n",
    "        for i in headers_attr:\n",
    "            if headers_attr[i] != None:\n",
    "                headers[i]=headers_attr[i]\n",
    "        self.tag_section = tag_section\n",
    "        self.tag_detail = tag_detail\n",
    "        self.next_tag = next_tag\n",
    "        if data != None:\n",
    "            if 'urlencoded' in headers_attr['Content-Type']:\n",
    "                self.data=urlencode(data)\n",
    "            elif 'json' in headers_attr['Content-Type']:\n",
    "                self.data=json.dumps(data)\n",
    "        if method != None:     \n",
    "            self.method = method\n",
    "        if next_tag != '':\n",
    "            self.tag_upper_limit = int(next_tag[2])\n",
    "    def before_craw(self):\n",
    "        if self.use_cloudscraper:\n",
    "            if self.method =='GET':\n",
    "                r = self.scraper.get(self.url,headers = self.headers)\n",
    "            if self.method =='POST':\n",
    "                r = self.scraper.post(self.url,headers = self.headers, data = self.data)\n",
    "            self.cookies = str(list(self.scraper.cookies))\n",
    "        else:\n",
    "            if self.method =='GET':\n",
    "                r = self.s.get(self.url, headers = self.headers, data = self.data)\n",
    "            if self.method =='POST':\n",
    "                r = self.s.post(self.url, headers = self.headers, data = self.data)\n",
    "            self.cookies = str(list(self.s.cookies))\n",
    "        self.status_code = r.status_code\n",
    "        self.html = r.text\n",
    "        soup = BeautifulSoup(self.html, \"html.parser\")\n",
    "        \n",
    "    def test_parse(self, data_export, next_url):\n",
    "        if data_export:\n",
    "            self.datas=[]\n",
    "            self.json=[]\n",
    "        try:\n",
    "            if self.use_cloudscraper:\n",
    "                r = self.scraper.get(self.url,headers = self.headers)\n",
    "            else:\n",
    "                r = self.s.get(self.url,headers = self.headers)\n",
    "        except rq.exceptions.RequestException:\n",
    "            raise rq.exceptions.RequestException\n",
    "        self.status_code = r.status_code\n",
    "        self.html = r.text\n",
    "        soup = BeautifulSoup(self.html, \"html.parser\")\n",
    "        if next_url:\n",
    "            try:\n",
    "                url_section_list = soup.select(self.next_tag[0])\n",
    "                url = url_section_list[self.next_tag[1]]['href']\n",
    "                if re.search(r'http',url):\n",
    "                    self.next_url = url\n",
    "                else:\n",
    "                    self.next_url = re.search(r'.*//.*?/',self.former_part).group(0)[:-1] + url\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                self.next_url = None\n",
    "                    \n",
    "        # tag_detail 預期要放這個：(tag_name, css_selector, 屬性(text href....))\n",
    "        blocks = soup.select(self.tag_section)\n",
    "        for (idx_block,block) in enumerate(blocks):\n",
    "            data = []\n",
    "            dic = {}\n",
    "            for (idx_j,j) in enumerate(self.tag_detail):\n",
    "                this = ''\n",
    "                try:\n",
    "                    obj = block.select(j[1])[j[2]]\n",
    "                    if j[3] == 'text':\n",
    "                        this = obj.text.strip()\n",
    "                    else:\n",
    "                        this = obj[j[3]].strip()\n",
    "                except Exception as ex:\n",
    "                    print(f\"第 {idx_block+1} 個區塊，第 {idx_j+1} 個欲爬取的標籤 ( {j[1]} )無法獲取，跳過\")\n",
    "                    print(ex)\n",
    "                data.append(this)\n",
    "                dic[j[0]]=this\n",
    "            self.datas.append(data)\n",
    "            self.json.append(dic)\n",
    "        if data_export:\n",
    "            self.json = json.dumps(self.json,ensure_ascii=False)\n",
    "    def url_loop_parse(self):\n",
    "        self.datas=[]\n",
    "        self.json=[]\n",
    "        self.str = []\n",
    "        self.html = \"<div>當前未抓取任何頁面<div>\"\n",
    "        if self.initial_pages >= self.end_pages:\n",
    "            start = self.end_pages\n",
    "            end = self.initial_pages\n",
    "        else:\n",
    "            start = self.initial_pages\n",
    "            end = self.end_pages\n",
    "        for i in range(start, end+1):    \n",
    "            self.url = self.former_part+str(i)+self.latter_part\n",
    "            try:\n",
    "                self.test_parse(data_export = False, next_url = False)\n",
    "                self.str.append(f\"第 {i} 頁爬取成功\")\n",
    "            except rq.exceptions.RequestException as e:\n",
    "                self.str.append(f\"第 {i} 頁爬取失敗，停止爬取\")\n",
    "                break\n",
    "        self.json = json.dumps(self.json,ensure_ascii=False)\n",
    "    def next_loop_parse(self):\n",
    "        count=0\n",
    "        self.datas=[]\n",
    "        self.json=[]\n",
    "        self.str = []\n",
    "        self.url = self.former_part\n",
    "        self.test_parse(data_export = False, next_url = True)\n",
    "        while self.next_url != None and count<=self.tag_upper_limit:\n",
    "            count=count+1\n",
    "            self.url = self.next_url\n",
    "            try:\n",
    "                self.test_parse(data_export = False, next_url = True)\n",
    "                self.str.append(f\"第 {count} 頁爬取成功\")\n",
    "            except rq.exceptions.RequestException as e:\n",
    "                self.str.append(f\"第 {count} 頁爬取失敗，停止爬取\")\n",
    "                break\n",
    "        self.json = json.dumps(self.json,ensure_ascii=False)\n",
    "    def export_excel(self, path):\n",
    "        wb = Workbook()\n",
    "        sheet = wb.create_sheet(self.name, 0)\n",
    "        sheet.append([i[0] for i in self.tag_detail])\n",
    "        for i in self.datas:\n",
    "            sheet.append(i)\n",
    "        wb.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf05317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #gov_pur_crawler\n",
    "# header_part={'Host':'web.pcc.gov.tw','Content-Type':None}\n",
    "# url_attr={'former_part':'https://web.pcc.gov.tw/prkms/tender/common/basic/readTenderBasic?tenderEndDate=2022%2F06%2F01&orgName=&tenderName=&searchType=basic&d-49738-p=','initial_pages':'1','latter_part':'&firstSearch=false&pageSize=50&radProctrgCate=&tenderId=&orgId=&tenderStartDate=2022%2F06%2F01&tenderType=TENDER_DECLARATION&dateType=isDate&tenderWay=TENDER_WAY_ALL_DECLARATION&level_1=on','end_pages':'2'}\n",
    "# ax= site_url_Spider()\n",
    "# ax.update_attr(\"ptt_crawler\",header_part,url_attr,'','div#printArea table.tb_01 tr',[('機關名稱','td',1,'text'),('標案案號','td',2,'text'),('連結','td a ',0,'href'),('傳輸次數','td',3,'text'),('招標方式','td',4,'text')],None,'')\n",
    "# ax.url_loop_parse()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683adf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ptt_crawler\n",
    "# header_part={'Host':'www.ptt.cc','Content-Type':None}\n",
    "# url_attr={'former_part':'https://www.ptt.cc/bbs/LoL/index','initial_pages':'14386','latter_part':'.html','end_pages':'14380'}\n",
    "# ax= site_url_Spider()\n",
    "# ax.update_attr(\"ptt_crawler\",header_part,url_attr, (\"div.btn-group.btn-group-paging a\",0,10),'div.r-ent',\"[('標題','div.title',0,'text'),('作者','div.author',0,'text'),('日期','div.date',0,'text'),('連結','div.title a',0,'href')]\",None, None)\n",
    "# ax.next_loop_parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb4df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inside_crawler\n",
    "# header_part={'Host':'www.inside.com.tw','Content-Type':None}\n",
    "# url_attr={'former_part':'https://www.inside.com.tw/tag/%E5%8F%B0%E7%A9%8D%E9%9B%BB?page=','initial_pages':'1','latter_part':'','end_pages':'4'}\n",
    "# ax= site_url_Spider(\"inside_crawler\",header_part,url_attr)\n",
    "# ax.url_loop_parse('div.post_list.post_list-list_style div.post_list_item div.post_list_item_content',\"[('標題','h3.post_title a','text'),('細項','p.post_description.js-auto_break_text','text'),('日期','ul.post_meta li.post_date','text'),('連結','h3.post_title a','href')]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4706aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.inside.com.tw/tag/Apple\"\n",
    "# r= rq.get(url)\n",
    "# next_tag = (\"div.container ul.pagination li.pagination_item-next-wrapper a a\", 0)\n",
    "# soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "# url = soup.select(next_tag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6176aa4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Qt_crawler",
   "language": "python",
   "name": "qtcrawler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

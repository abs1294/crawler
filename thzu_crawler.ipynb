{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\\n%s\" %\n"
     ]
    }
   ],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "from flask import Flask\n",
    "from flask import jsonify, request\n",
    "import pymysql\n",
    "import json\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import sys\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從免費代理網站取得代理ip\n",
    "def get_proxy_ip(isHttps):\n",
    "    r = rq.get(\"https://www.us-proxy.org/\",verify=False)\n",
    "    html_doc = r.text\n",
    "    soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "    trs = soup.select(\"table.table.table-striped.table-bordered tr\")\n",
    "    metas=[]\n",
    "    for tr in trs:\n",
    "        tds = tr.select(\"td\")\n",
    "        if len(tds) > 6:\n",
    "            ifScheme = tds[6].text\n",
    "            ip = tds[0].text\n",
    "            port = tds[1].text\n",
    "            anonymity = tds[4].text\n",
    "            proxy = \"%s:%s\"%(ip, port)\n",
    "            meta = {\n",
    "                'proxyIp': proxy,\n",
    "                'connect_times':0,\n",
    "                'successful_connect_times':0,\n",
    "                'qual_ratio':0,\n",
    "                'isDelete':False,\n",
    "              }\n",
    "            if ifScheme == isHttps:\n",
    "                if ifScheme == \"yes\":\n",
    "                    meta['ishttps'] = True\n",
    "                    meta['proxyIp'] = \"https://\" + meta['proxyIp']\n",
    "                elif ifScheme == \"no\":\n",
    "                    meta['ishttps'] = False\n",
    "                    meta['proxyIp'] = \"http://\" + meta['proxyIp']\n",
    "                metas.append(meta)\n",
    "                continue\n",
    "    return metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從資料庫取出ip\n",
    "def get_ip_from_db(isHttps,db_settings,qual_ratio=0,connect_times=4):\n",
    "    try:\n",
    "        # 建立Connection物件\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        # 建立Cursor物件\n",
    "        with conn.cursor() as cursor:\n",
    "          #資料表相關操作\n",
    "            if isHttps==\"yes\":\n",
    "                get_ip=f\"select * FROM proxy_ip where isHttps = 1 and isDelete is False and ( qual_ratio > {qual_ratio} or ( connect_times < {connect_times} ))\"\n",
    "            elif isHttps==\"no\":\n",
    "                get_ip=f\"select * FROM proxy_ip where isHttps = 0 and isDelete is False and ( qual_ratio > {qual_ratio} or ( connect_times < {connect_times} ))\"\n",
    "            cursor.execute(get_ip)\n",
    "            xx = cursor.fetchall()\n",
    "            des = cursor.description\n",
    "            data=[]\n",
    "            for i in xx:\n",
    "                d = {}\n",
    "                x=0\n",
    "                for j in des:\n",
    "                    d[j[0]]=i[x]\n",
    "                    x=x+1\n",
    "                data.append(d)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我要翻頁!!!!!!   by 更改url\n",
    "def thzucrawler(domain,forum_number,pages=1, detail = False):\n",
    "    time_start = time.time()\n",
    "    now = datetime.date.today()\n",
    "    articles = []\n",
    "    s = rq.session()\n",
    "    proxies_this={}\n",
    "    proxies=get_proxy_ip(\"no\")\n",
    "    err = 0\n",
    "    headers = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "        \"Accept-Language\": \"zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6\", \n",
    "        \"Host\": \"web.pcc.gov.tw\",  #目標網站 \n",
    "        \"Sec-Fetch-Dest\": \"document\", \n",
    "        \"Sec-Fetch-Mode\": \"navigate\", \n",
    "        \"Sec-Fetch-Site\": \"same-origin\", \n",
    "        \"Upgrade-Insecure-Requests\": \"?1\", \n",
    "        \"User-Agent\":UserAgent().random\n",
    "         #使用者代理\n",
    "    } \n",
    "    \n",
    "    for i in range(1,10000):\n",
    "        print(i-err)\n",
    "        url = domain + 'forum-'+ str(forum_number)+'-'+str(i-err)+'.html'  \n",
    "        try:\n",
    "            response = rq.get(url,cookies={'over18': '1'},proxies = proxies_this,timeout=3)\n",
    "            html_doc = response.text\n",
    "            soup = BeautifulSoup(html_doc, 'lxml')\n",
    "            if i == 1:\n",
    "                pages_actual_text=soup.select('div.pg a')[-2].text\n",
    "                #總頁數\n",
    "                pages_actual= int(re.search('\\d.*',pages_actual_text).group(0))\n",
    "            page = soup.select(\"div#pgt div strong\")[0].text\n",
    "            for s in soup.select(\"div.mn div#threadlist div.bm_c form#moderate table tbody\"):\n",
    "                if (response.status_code == 200) and (re.search('normalthread.*',s['id'])):\n",
    "                    #requests.get() 的結果是 request.Response 物件, 我們可以先透過該物件的 statu_code 屬性取得 server 回覆的狀態碼\n",
    "                    #(例如 200 表示正常, 404 表示找不到網頁等), 若狀態碼為 200, 代表正常回應\n",
    "                    board=s.select(\"th em a\")[0].text\n",
    "                    name=s.select(\"th a.s.xst\")[0].text\n",
    "                    url_articles= domain + s.select(\"th a.s.xst\")[0]['href']\n",
    "                    reply = s.select(\"a.xi2\")[0].text\n",
    "                    look = s.select(\"td.num em\")[0].text\n",
    "                    nos_of_d=-1\n",
    "                    date=s.select(\"td span\")[0].text\n",
    "                    if re.search('前',date) or re.search('天',date):\n",
    "                        if re.search('小时',date) or re.search('秒',date) or re.search('分钟',date):\n",
    "                            date=now\n",
    "                        elif re.search('(.*)天',date).group(1)=='昨':\n",
    "                            date=now+datetime.timedelta(-1)\n",
    "                        elif re.search('(.*)天',date).group(1)=='前':\n",
    "                            date=now+datetime.timedelta(-2)\n",
    "                        else:\n",
    "                            delta=re.search('(.*)天',date).group(1)\n",
    "                            date=now+datetime.timedelta(int(delta))\n",
    "                    if detail == True:\n",
    "                        responsee = rq.get(url_articles,cookies={'over18': '1'})\n",
    "                        html_docc = responsee.text\n",
    "                        soupp = BeautifulSoup(html_docc, 'lxml')\n",
    "                        try:\n",
    "                            nos_of_d_text = soupp.select('dd p:nth-child(3)')[0].text\n",
    "                            nos_of_d = re.search(r'下载次数:(.*)',nos_of_d_text)\n",
    "                            nos_of_d = int(nos_of_d.group(1).strip())\n",
    "                        except Exception as exx:\n",
    "                            print(\"沒有下載次數\")\n",
    "                            nos_of_d = -1\n",
    "                            pass\n",
    "                    articles.append({'page': page,'board': board,'title': name,'date': str(date),'url': url_articles,'nos_of_d':nos_of_d,'reply':reply,'look':look})\n",
    "            if i-err == pages or i-err == pages_actual:\n",
    "                break\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "            detail = ex.args[0] #取得詳細內容\n",
    "            cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "            lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "            fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "            lineNum = lastCallStack[1] #取得發生的行號\n",
    "            funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "            errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "            print(errMsg)\n",
    "            print(\"換一下 proxy\")\n",
    "            err = err + 1\n",
    "            proxies_this = proxies[err]\n",
    "            if err == len(proxies):\n",
    "                err = 0\n",
    "                proxies=get_proxy_ip(\"no\")\n",
    "            continue\n",
    "    print(\"done\")\n",
    "    time_end = time.time()    #結束計時\n",
    "    time_c= time_end - time_start   #執行所花時間\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_settings = {\n",
    "    \"host\": \"127.0.0.1\",\n",
    "    \"port\": 3306,\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"As123459362\",\n",
    "    \"db\": \"pttcrawler\",\n",
    "    \"charset\": \"utf8\",\n",
    "    \"autocommit\":True\n",
    "}\n",
    "def send_db(db_settings, raw_datas,typee):\n",
    "# import charts\n",
    "# 資料庫參數設定\n",
    "\n",
    "    time_start = time.time() #開始計時\n",
    "\n",
    "    try:\n",
    "        # 建立Connection物件\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        # 建立Cursor物件\n",
    "        with conn.cursor() as cursor:\n",
    "          #資料表相關操作\n",
    "            get_max_log_id=\"select MAX(UID) FROM log\"\n",
    "            # % 操作符只能直接用於字串(‘123’)，列表([1,2,3])、元組\n",
    "            command = \"INSERT INTO thzu_data(log_UID, page, title, board, date, url, nos_of_d, type, look ,reply)VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)  on DUPLICATE KEY UPDATE log_UID = values(log_UID), page = values(page), title = values(title), board = values(board), date = values(date), url = values(url), nos_of_d = values(nos_of_d), type = values(type), look = values(look), reply = values(reply)\"\n",
    "            command_log = \"INSERT INTO log(datetime, task, status, record_des, errmsg) VALUES(%s, %s, %s, %s, %s)\"\n",
    "            # 紀錄開始\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"thzucrawler\", \"start\", \"\", \"\"))\n",
    "            # 取得 log 的 UID\n",
    "            cursor.execute(get_max_log_id)\n",
    "            log_UID = str(cursor.fetchone()[0])\n",
    "            # 執行\n",
    "            for raw_data in raw_datas:\n",
    "                try:\n",
    "                    cursor.execute(command, (int(log_UID),int(raw_data[\"page\"]), raw_data[\"title\"], raw_data[\"board\"], raw_data[\"date\"], raw_data[\"url\"], raw_data[\"nos_of_d\"], typee, int(raw_data[\"look\"]), int(raw_data[\"reply\"])))\n",
    "                except Exception as ex:\n",
    "                    error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "                    detail = ex.args[0] #取得詳細內容\n",
    "                    cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "                    lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "                    fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "                    lineNum = lastCallStack[1] #取得發生的行號\n",
    "                    funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "                    errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "                    print(errMsg)\n",
    "                    cursor.execute(command_log, (datetime.datetime.now(), \"thzucrawler\", \"wrong\", str(raw_data), str(ex) + \" + \" + errMsg))\n",
    "                    continue\n",
    "            # 紀錄結束\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"thzucrawler\", \"finish\", \"\", \"\"))\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        error_class = ex.__class__.__name__ #取得錯誤類型\n",
    "        detail = ex.args[0] #取得詳細內容\n",
    "        cl, exc, tb = sys.exc_info() #取得Call Stack\n",
    "        lastCallStack = traceback.extract_tb(tb)[-1] #取得Call Stack的最後一筆資料\n",
    "        fileName = lastCallStack[0] #取得發生的檔案名稱\n",
    "        lineNum = lastCallStack[1] #取得發生的行號\n",
    "        funcName = lastCallStack[2] #取得發生的函數名稱\n",
    "        errMsg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(fileName, lineNum, funcName, error_class, detail)\n",
    "        print(errMsg)\n",
    "\n",
    "    time_end = time.time()    #結束計時\n",
    "    time_c= time_end - time_start   #執行所花時間\n",
    "    print('寫入資料庫，time cost', time_c, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:981: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.us-proxy.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# http://29th.cc/forum-220-5.html\n",
    "# domain : http://29th.cc/\n",
    "# forum_number : 220\n",
    "# pages : 5\n",
    "# raw_datas=thzucrawler(\"http://29th.cc/\",39,10000)\n",
    "# send_db(db_settings, raw_datas, \"日韩情色(BT)\")\n",
    "raw_datas=thzucrawler(\"http://29th.cc/\",181,10000)\n",
    "send_db(db_settings, raw_datas,\"亚洲無碼原創\")\n",
    "raw_datas=thzucrawler(\"http://29th.cc/\",220,10000)\n",
    "send_db(db_settings, raw_datas,\"亚洲有碼原創\")\n",
    "raw_datas=thzucrawler(\"http://29th.cc/\",182,10000)\n",
    "send_db(db_settings, raw_datas,\"欧美無碼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立Connection物件\n",
    "conn = pymysql.connect(**db_settings)\n",
    "# 建立Cursor物件\n",
    "with conn.cursor(pymysql.cursors.DictCursor) as cursor: # 創建一個字典游標\n",
    "    get_url_100=\"select UID, url FROM thzu_data where nos_of_d = 0 limit 0,100\"\n",
    "    cursor.execute(get_url_100)\n",
    "    urls = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

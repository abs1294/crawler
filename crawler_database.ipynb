{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "e09683f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#我要翻頁!!!!!!   by 更改url\n",
    "def pttcrawler(url,pages):\n",
    "    response = rq.get(url,cookies={'over18': '1'})\n",
    "    html_doc = response.text\n",
    "    soup = BeautifulSoup(html_doc, 'lxml')\n",
    "    \n",
    "    this_year = datetime.datetime.now().year\n",
    "    \n",
    "    articles = []\n",
    "    board=soup.find(\"a\",class_= \"board\")[\"href\"]\n",
    "    board = re.search('bbs\\/(.*)\\/', board)[1]\n",
    "    for s in soup.find_all(\"div\", class_= \"r-ent\"):\n",
    "        if (s.find('a')) and (response.status_code == 200):\n",
    "            #requests.get() 的結果是 request.Response 物件, 我們可以先透過該物件的 statu_code 屬性取得 server 回覆的狀態碼\n",
    "            #(例如 200 表示正常, 404 表示找不到網頁等), 若狀態碼為 200, 代表正常回應\n",
    "            date=s.find(\"div\",class_= \"date\").string\n",
    "            date=str(this_year)+'/'+date if int(date[0:2])<=2 else str(this_year-1)+'/'+date\n",
    "            date=date.replace(\" \",\"\")\n",
    "            name=s.find('a').string\n",
    "            push=s.find(\"div\",class_= \"nrec\").string\n",
    "            try:\n",
    "                push=100 if push=='爆' else push\n",
    "                push=-100 if push=='XX' else push \n",
    "                push=int(push.replace(\"X\",\"-\"))*10 if push.find(\"X\")==0 else push\n",
    "            except:\n",
    "                continue\n",
    "            author=s.find(\"div\",class_= \"author\").string\n",
    "            url=s.find(\"div\",class_= \"title\").a[\"href\"]\n",
    "            articles.append({'board': board,'title': name,'push': push,'date': date,'author': author,'url': \"https://www.ptt.cc\"+url}) \n",
    "            \n",
    "    soup2=\"https://www.ptt.cc\"+soup.find_all(class_=\"btn wide\")[1][\"href\"]#第二個([1])class是\"btn wide\"之下的href值\n",
    "    begin=soup2.find(\"index\")+5\n",
    "    end=soup2.find(\".html\")\n",
    "#     try:\n",
    "#         int(soup2[-9:-5])\n",
    "#     except:\n",
    "#         try:\n",
    "#             int(soup2[-8:-5])\n",
    "#         except:\n",
    "#             last_num=-7\n",
    "#         else:\n",
    "#             last_num=-8\n",
    "#     else:\n",
    "#         last_num=-9\n",
    "#     print(soup2)\n",
    "#     print(last_num)\n",
    "    \n",
    "    if pages<2:\n",
    "        return articles\n",
    "    \n",
    "    for p in range(1,pages):\n",
    "#         new_soup=soup2.replace(soup2[ last_num:-5],str(int(soup2[ last_num:-5])-p))#也可使用序列切片的方式。\n",
    "        new_soup=soup2.replace(soup2[begin:end],str(int(soup2[begin:end])-p+1))\n",
    "        print(new_soup)\n",
    "        #string = 'abcdafg'  newstr = string[:4] + 'e' + string[5:]         [-9:-5]=str(int(soup2[-9:-5])-p)\n",
    "        response2 = rq.get(new_soup,cookies={'over18': '1'})\n",
    "        html_doc2 = response2.text\n",
    "        soups = BeautifulSoup(html_doc2, 'lxml')\n",
    "        \n",
    "        for ns in soups.find_all(\"div\", class_= \"r-ent\"):\n",
    "            if (ns.find('a')) and (response.status_code == 200):\n",
    "                date=ns.find(\"div\",class_= \"date\").string\n",
    "                date=str(this_year)+'/'+date if int(date[0:2])<=2 else str(this_year-1)+'/'+date\n",
    "                date=date.replace(\" \",\"\")\n",
    "                name=ns.find('a').string\n",
    "                push=ns.find(\"div\",class_= \"nrec\").string\n",
    "                try:\n",
    "                    push=100 if push=='爆' else push\n",
    "                    push=-100 if push=='XX' else push \n",
    "                    push=int(push.replace(\"X\",\"-\"))*10 if push.find(\"X\")==0 else push\n",
    "                except:\n",
    "                    continue\n",
    "                author=ns.find(\"div\",class_= \"author\").string\n",
    "                url=ns.find(\"div\",class_= \"title\").a[\"href\"]\n",
    "                articles.append({'board': board,'title': name,'push': push,'date': date,'author': author,'url': \"https://www.ptt.cc\"+url}) \n",
    "    articles_pd=pd.DataFrame(articles)\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aca8599",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pttcrawler() takes 0 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-45590f8d8a03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"https://www.ptt.cc/bbs/sex/index.html\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mraw_datas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpttcrawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mraw_datas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: pttcrawler() takes 0 positional arguments but 2 were given"
     ]
    }
   ],
   "source": [
    "url=\"https://www.ptt.cc/bbs/sex/index.html\"\n",
    "pages=10\n",
    "raw_datas=pttcrawler(url,pages)\n",
    "raw_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "12ce615d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 0.09163832664489746 s\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "import time\n",
    "# import charts\n",
    "# 資料庫參數設定\n",
    "db_settings = {\n",
    "    \"host\": \"127.0.0.1\",\n",
    "    \"port\": 3306,\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"As123459362\",\n",
    "    \"db\": \"pttcrawler\",\n",
    "    \"charset\": \"utf8\",\n",
    "    \"autocommit\":True\n",
    "}\n",
    "time_start = time.time() #開始計時\n",
    "\n",
    "try:\n",
    "    # 建立Connection物件\n",
    "    conn = pymysql.connect(**db_settings)\n",
    "    # 建立Cursor物件\n",
    "    with conn.cursor() as cursor:\n",
    "      #資料表相關操作\n",
    "        command = \"INSERT INTO ptt_data(title, board, push_no, date, author, url)VALUES(%s, %s, %s, %s, %s, %s)\"\n",
    "        command_log = \"INSERT INTO log(datetime, task, status, record_des, errmsg) VALUES(%s, %s, %s, %s, %s)\"\n",
    "        # 紀錄開始\n",
    "        cursor.execute(command_log, (datetime.datetime.now(), \"pttcrawler\", \"start\", \"\", \"\"))\n",
    "        # 執行\n",
    "        for raw_data in raw_datas:\n",
    "            try:\n",
    "                cursor.execute(command, (raw_data[\"title\"], raw_data[\"board\"], raw_data[\"push\"], raw_data[\"date\"], raw_data[\"author\"], raw_data[\"url\"]))\n",
    "                conn.commit()\n",
    "            except Exception as err:\n",
    "                cursor.execute(command_log, (datetime.datetime.now(), \"pttcrawler\", \"wrong\", str(raw_data), str(err)))\n",
    "                continue\n",
    "        # 紀錄結束\n",
    "        cursor.execute(command_log, (datetime.datetime.now(), \"pttcrawler\", \"finish\", \"\", \"\"))\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "        \n",
    "time_end = time.time()    #結束計時\n",
    "time_c= time_end - time_start   #執行所花時間\n",
    "print('time cost', time_c, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8091af5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://localhost:5000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"id\":\"00000000-0000-0000-0000-000000000000\",\"docnoType\":\"OB\",\"docnoDate\":\"2021-11-07\",\"docnoSeq\":0,\"purchasingUnitId\":\"0011110\",\"nfsStationId\":\"10126\",\"type\":1,\"pickupDate\":\"2021-11-02\",\"total\":269,\"userId\":null,\"orderProducts\":[{\"orderId\":\"00000000-0000-0000-0000-000000000000\",\"docnoType\":\"OB\",\"docnoDate\":\"2021-11-07\",\"docnoSeq\":0,\"seqNo\":0,\"productId\":\"4710713631106\",\"isReplenishment\":true,\"vendorId\":\"11457\",\"productType\":\"\\xe5\\xa4\\xa7\\xe5\\xae\\x97\\xe9\\xa3\\x9f\\xe5\\x93\\x81\",\"productClass\":\"Y32\",\"productName\":\"\\xe7\\xb6\\x93\\xe5\\x85\\xb8\\xe9\\xa6\\x99\\xe6\\xbb\\xb7\\xe8\\xb1\\xac\\xe6\\x8e\\x92\",\"unitType\":\"Quantity\",\"unit\":\"\\xe5\\x8c\\x85\",\"price\":269,\"amount\":1,\"subtotal\":269,\"analyticsCodeId\":\"Y3206\",\"spec\":\"1.6kg.\\xe5\\x9b\\xba1000g\"}]}'\n",
      "*-----*\n",
      "{'id': '00000000-0000-0000-0000-000000000000', 'docnoType': 'OB', 'docnoDate': '2021-11-07', 'docnoSeq': 0, 'purchasingUnitId': '0011110', 'nfsStationId': '10126', 'type': 1, 'pickupDate': '2021-11-02', 'total': 269, 'userId': None, 'orderProducts': [{'orderId': '00000000-0000-0000-0000-000000000000', 'docnoType': 'OB', 'docnoDate': '2021-11-07', 'docnoSeq': 0, 'seqNo': 0, 'productId': '4710713631106', 'isReplenishment': True, 'vendorId': '11457', 'productType': '大宗食品', 'productClass': 'Y32', 'productName': '經典香滷豬排', 'unitType': 'Quantity', 'unit': '包', 'price': 269, 'amount': 1, 'subtotal': 269, 'analyticsCodeId': 'Y3206', 'spec': '1.6kg.固1000g'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Nov/2021 20:00:28] \"\u001b[37mPOST /pttcrawler?url=https://www.ptt.cc/bbs/sex/index.html&pages=10 HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'host': '127.0.0.1', 'port': 3306, 'user': 'root', 'password': 'As123459362', 'db': 'pttcrawler', 'charset': 'utf8', 'autocommit': True}\n",
      "name 'raw_data' is not defined\n"
     ]
    }
   ],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "from flask import Flask\n",
    "from flask import jsonify, request\n",
    "import pymysql\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "@app.route(\"/\", methods=['GET'])\n",
    "def HELLO():\n",
    "    return \"HELLO\"\n",
    "\n",
    "@app.route(\"/pttcrawler\", methods=['POST'])\n",
    "def pttcrawler():\n",
    "    # post資料 送在 body 的 raw data裡，可以用 json 轉\n",
    "    data=request.get_data()\n",
    "    print(json.loads(data))\n",
    "    \n",
    "    if 'url' in request.args:\n",
    "        url = str(request.args['url'])\n",
    "    else:\n",
    "        return \"NO_URL\",500\n",
    "    if 'pages' in request.args:\n",
    "        pages = int(request.args['pages'])\n",
    "    else:\n",
    "        pages = 1\n",
    "    response = rq.get(url,cookies={'over18': '1'})\n",
    "    html_doc = response.text\n",
    "    soup = BeautifulSoup(html_doc, 'lxml')\n",
    "    \n",
    "    this_year = datetime.datetime.now().year\n",
    "    \n",
    "    articles = []\n",
    "    board=soup.find(\"a\",class_= \"board\")[\"href\"]\n",
    "    board = re.search('bbs\\/(.*)\\/', board)[1]\n",
    "    wrong=0\n",
    "    for s in soup.find_all(\"div\", class_= \"r-ent\"):\n",
    "        if (s.find('a')) and (response.status_code == 200):\n",
    "            #requests.get() 的結果是 request.Response 物件, 我們可以先透過該物件的 statu_code 屬性取得 server 回覆的狀態碼\n",
    "            #(例如 200 表示正常, 404 表示找不到網頁等), 若狀態碼為 200, 代表正常回應\n",
    "            date=s.find(\"div\",class_= \"date\").string\n",
    "            date=str(this_year)+'/'+date if int(date[0:2])<=2 else str(this_year-1)+'/'+date\n",
    "            date=date.replace(\" \",\"\")\n",
    "            name=s.find('a').string\n",
    "            push=s.find(\"div\",class_= \"nrec\").string\n",
    "            try:\n",
    "                push=100 if push=='爆' else push\n",
    "                push=-100 if push=='XX' else push \n",
    "                push=int(push.replace(\"X\",\"-\"))*10 if push.find(\"X\")==0 else push\n",
    "            except:\n",
    "                continue\n",
    "            author=s.find(\"div\",class_= \"author\").string\n",
    "            url=s.find(\"div\",class_= \"title\").a[\"href\"]\n",
    "            articles.append({'board': board,'title': name,'push': push,'date': date,'author': author,'url': \"https://www.ptt.cc\"+url}) \n",
    "            \n",
    "    soup2=\"https://www.ptt.cc\"+soup.find_all(class_=\"btn wide\")[1][\"href\"]#第二個([1])class是\"btn wide\"之下的href值\n",
    "    #找出這是哪一個版\n",
    "    begin=soup2.find(\"index\")+5\n",
    "    end=soup2.find(\".html\")\n",
    "    \n",
    "    if pages<2:\n",
    "        return  \"success, there are \"+str(wrong)+\" mistakes\",200\n",
    "    \n",
    "    for p in range(1,pages):\n",
    "#         new_soup=soup2.replace(soup2[ last_num:-5],str(int(soup2[ last_num:-5])-p))#也可使用序列切片的方式。\n",
    "        new_soup=soup2.replace(soup2[begin:end],str(int(soup2[begin:end])-p+1))\n",
    "        #string = 'abcdafg'  newstr = string[:4] + 'e' + string[5:]         [-9:-5]=str(int(soup2[-9:-5])-p)\n",
    "        response2 = rq.get(new_soup,cookies={'over18': '1'})\n",
    "        html_doc2 = response2.text\n",
    "        soups = BeautifulSoup(html_doc2, 'lxml')\n",
    "        \n",
    "        for ns in soups.find_all(\"div\", class_= \"r-ent\"):\n",
    "            if (ns.find('a')) and (response.status_code == 200):\n",
    "                date=ns.find(\"div\",class_= \"date\").string\n",
    "                date=str(this_year)+'/'+date if int(date[0:2])<=2 else str(this_year-1)+'/'+date\n",
    "                date=date.replace(\" \",\"\")\n",
    "                name=ns.find('a').string\n",
    "                push=ns.find(\"div\",class_= \"nrec\").string\n",
    "                try:\n",
    "                    push=100 if push=='爆' else push\n",
    "                    push=-100 if push=='XX' else push \n",
    "                    push=int(push.replace(\"X\",\"-\"))*10 if push.find(\"X\")==0 else push\n",
    "                except:\n",
    "                    continue\n",
    "                author=ns.find(\"div\",class_= \"author\").string\n",
    "                url=ns.find(\"div\",class_= \"title\").a[\"href\"]\n",
    "                articles.append({'board': board,'title': name,'push': push,'date': date,'author': author,'url': \"https://www.ptt.cc\"+url}) \n",
    "        \n",
    "        # 資料庫參數設定\n",
    "    db_settings = {\n",
    "        \"host\": \"127.0.0.1\",\n",
    "        \"port\": 3306,\n",
    "        \"user\": \"root\",\n",
    "        \"password\": \"As123459362\",\n",
    "        \"db\": \"pttcrawler\",\n",
    "        \"charset\": \"utf8\",\n",
    "        \"autocommit\":True\n",
    "    }\n",
    "    print(db_settings)\n",
    "    try:\n",
    "        # 建立Connection物件\n",
    "        conn = pymysql.connect(**db_settings)\n",
    "        # 建立Cursor物件\n",
    "        with conn.cursor() as cursor:\n",
    "          #資料表相關操作\n",
    "            command = \"INSERT INTO ptt_data(title, board, push_no, date, author, url)VALUES(%s, %s, %s, %s, %s, %s)\"\n",
    "            command_log = \"INSERT INTO log(datetime, task, status, record_des, errmsg) VALUES(%s, %s, %s, %s, %s)\"\n",
    "            # 紀錄開始\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"pttcrawler\", \"start\", \"\", \"\"))\n",
    "            # 執行\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    cursor.execute(command, (article[\"title\"], article[\"board\"], article[\"push\"], article[\"date\"], article[\"author\"], article[\"url\"]))\n",
    "                    conn.commit()\n",
    "                except Exception as err:\n",
    "                    wrong=wrong+1\n",
    "                    cursor.execute(command_log, (datetime.datetime.now(), \"pttcrawler\", \"wrong\", str(article), str(err)))\n",
    "                    continue\n",
    "            # 紀錄結束\n",
    "            cursor.execute(command_log, (datetime.datetime.now(), \"pttcrawler\", \"finish\", \"\", \"\"))\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "    return  \"success, there are \"+str(wrong)+\" mistakes\",200\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.debug = False\n",
    "    app.config['JSON_AS_ASCII'] = False\n",
    "    app.run(host='localhost', port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22788a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
